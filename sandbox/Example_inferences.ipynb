{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Inference Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from utils.custom_models import load_models\n",
    "from utils.inference_scripts import classify_species, classify_order, flatbug, classify_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regional_category_map_path='./models/03_uk_data_category_map.json'\n",
    "\n",
    "# load the json\n",
    "with open(regional_category_map_path) as f:\n",
    "    regional_category_map = json.load(f)\n",
    "\n",
    "regional_category_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if they go in order\n",
    "list(regional_category_map.values()) == list(range(len(regional_category_map)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the labeling is being indexed correctly\n",
    "index_to_label = {index: label for label, index in regional_category_map.items()}\n",
    "\n",
    "index_to_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now check these match\n",
    "assert(list(index_to_label.keys()) == list(range(len(regional_category_map))))\n",
    "assert(list(index_to_label.values()) == list(regional_category_map.keys()))\n",
    "assert(list(index_to_label.keys()) == list(regional_category_map.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_species2(image_tensor, regional_model, regional_category_map, top_n=5):\n",
    "    \"\"\"\n",
    "    Classify the species of the moth using the regional model.\n",
    "    \"\"\"\n",
    "\n",
    "    # print('Inference for species...')\n",
    "    output = regional_model(image_tensor)\n",
    "    predictions = torch.nn.functional.softmax(output, dim=1).cpu().detach().numpy()[0]\n",
    "\n",
    "    # Sort predictions to get the indices of the top 5 scores\n",
    "    top_n_indices = predictions.argsort()[-top_n:][::-1]\n",
    "\n",
    "    # Map indices to labels and fetch their confidence scores\n",
    "    index_to_label = {index: label for label, index in regional_category_map.items()}\n",
    "    top_n_labels = [index_to_label[idx] for idx in top_n_indices]\n",
    "    top_n_scores = [predictions[idx] for idx in top_n_indices]\n",
    "\n",
    "    return top_n_labels, top_n_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path1 = '/home/users/katriona/amber-inferences/data/uk/downloaded_images/20240624232150-snapshot.jpg'\n",
    "\n",
    "# image with blur\n",
    "image_path2 = '/home/users/katriona/amber-inferences/data/uk/downloaded_images/20240626230030-snapshot.jpg'\n",
    "\n",
    "# nice obvious moth\n",
    "image_path3 = '/home/users/katriona/amber-inferences/data/uk/downloaded_images/20240803003019-snapshot.jpg'\n",
    "\n",
    "image_path = image_path2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_name = 'gbr'\n",
    "flatbug_model_path='./models/flat_bug_M.pt'\n",
    "binary_model_path='./models/moth-nonmoth-effv2b3_20220506_061527_30.pth'\n",
    "order_model_path='./models/dhc_best_128.pth'\n",
    "order_labels_path='./models/thresholdsTestTrain.csv'\n",
    "regional_model_path='./models/turing-uk_v03_resnet50_2024-05-13-10-03_state.pt'\n",
    "regional_category_map_path='./models/03_uk_data_category_map.json'\n",
    "localisation_model_path = '/home/users/katriona/amber-inferences/models/v1_localizmodel_2021-08-17-12-06.pt'\n",
    "order_data_thresholds=image_path,\n",
    "proc_device=torch.device(\"cuda:0\")\n",
    "csv_file ='./examples/gb_examples.csv'\n",
    "save_crops=True\n",
    "box_threshold=0.995\n",
    "top_n=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load_models(\n",
    "        proc_device,\n",
    "        binary_model_path,\n",
    "        order_model_path,\n",
    "        order_labels_path,\n",
    "        regional_model_path,\n",
    "        regional_category_map_path,\n",
    "        flatbug_model_path,\n",
    "        localisation_model_path,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_species = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((300, 300)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dt = os.path.basename(image_path)\n",
    "\n",
    "if image_dt.startswith('20'):\n",
    "    image_dt = image_dt.split(\"-\")[0]\n",
    "else:\n",
    "    image_dt = image_dt.split(\"-\")[1]\n",
    "image_dt = datetime.strptime(image_dt, \"%Y%m%d%H%M%S%f\")\n",
    "image_dt = datetime.strftime(image_dt, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "current_dt = datetime.now()\n",
    "current_dt = datetime.strftime(current_dt, \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "print(image_dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # print the image\n",
    "    plt.imshow(image)\n",
    "except Exception as e:\n",
    "    print(f\"Error opening image {image_path}: {e}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_image = image.copy()\n",
    "original_width, original_height = image.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the flatbug crops and the predictions of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example with Flatbug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatbug_outputs = flatbug(image_path, models['flatbug_model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each detection\n",
    "image2 = image.copy()\n",
    "\n",
    "for i in range(len(flatbug_outputs[\"boxes\"])):\n",
    "    crop_status = \"crop \" + str(i)\n",
    "\n",
    "    x_min, y_min, x_max, y_max = flatbug_outputs[\"boxes\"][i]\n",
    "\n",
    "    box_score = flatbug_outputs[\"scores\"][i]\n",
    "    box_label = flatbug_outputs[\"labels\"][i]\n",
    "\n",
    "    x_min = x_min #* original_width / 300)\n",
    "    y_min = y_min #* original_height / 300)\n",
    "    x_max = x_max #* original_width / 300)\n",
    "    y_max = y_max #* original_height / 300)\n",
    "\n",
    "    # Crop the detected region and perform classification\n",
    "    cropped_image = original_image.crop((x_min, y_min, x_max, y_max))\n",
    "    cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(proc_device)\n",
    "\n",
    "    class_name, class_confidence = classify_box(cropped_tensor, models['classification_model'])\n",
    "    order_name, order_confidence = classify_order(\n",
    "        cropped_tensor, models['order_model'], models['order_model_labels'], models['order_model_thresholds']\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "    if (class_name == \"moth\") and ('Lepidoptera' not in order_name):\n",
    "        col = 'orange'\n",
    "    elif (class_name != \"moth\") and ('Lepidoptera' in order_name):\n",
    "        col = 'purple'\n",
    "    elif (class_name == \"moth\") and ('Lepidoptera' in order_name):\n",
    "            col = 'green'\n",
    "    else:\n",
    "        col = 'red'\n",
    "\n",
    "\n",
    "    # annotate the image with bounding boxes\n",
    "    draw = ImageDraw.Draw(image2)\n",
    "    draw.rectangle([x_min, y_min, x_max, y_max], outline=col, width=4)\n",
    "\n",
    "        # Annotate image with bounding box and class\n",
    "    if (class_name == \"moth\") or (\"Lepidoptera\" in order_name):\n",
    "        species_names, species_confidences = classify_species(\n",
    "            cropped_tensor, models['species_model'],\n",
    "            models['species_model_labels'], top_n\n",
    "        )\n",
    "        # annotate the name on the image\n",
    "        draw.text((x_min, y_min-50),\n",
    "                f\"{species_names[0]}: {species_confidences[0]:.2f}\",\n",
    "                fill=col, font=ImageFont.truetype(\"DejaVuSans.ttf\", 50),\n",
    "                verticalalignment=\"bottom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "plt.imshow(image2)\n",
    "plt.axis('off')\n",
    "\n",
    "# add legend\n",
    "adj=100\n",
    "shift=-20\n",
    "plt.text(0, shift, 'red: non-moth, non-Lepidoptera', color='red', size=8)\n",
    "plt.text(0, -1*adj+shift, 'orange: moth but not Lepidoptera', color='orange', size=8)\n",
    "plt.text(0, -2*adj+shift, 'purple: Lepidoptera but not moth', color='purple', size=8)\n",
    "plt.text(0, -3*adj+shift, 'green: moth and Lepidoptera', color='green', size=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example with Localisation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_loc = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((300, 300)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = transform_loc(image).unsqueeze(0).to(proc_device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    localisation_outputs = models['localisation_model'](input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(localisation_outputs[0][\"boxes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(localisation_outputs[0][\"boxes\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "box_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each detection\n",
    "image3 = image.copy()\n",
    "\n",
    "for i in range(len(localisation_outputs[0][\"boxes\"])):\n",
    "    crop_status = \"crop \" + str(i)\n",
    "\n",
    "    x_min, y_min, x_max, y_max = localisation_outputs[0][\"boxes\"][i]\n",
    "    box_score = localisation_outputs[0][\"scores\"].tolist()[i]\n",
    "    box_label = localisation_outputs[0][\"labels\"].tolist()[i]\n",
    "\n",
    "    x_min = int(int(x_min) * original_width / 300)\n",
    "    y_min = int(int(y_min) * original_height / 300)\n",
    "    x_max = int(int(x_max) * original_width / 300)\n",
    "    y_max = int(int(y_max) * original_height / 300)\n",
    "\n",
    "    if box_score < box_threshold:\n",
    "                continue\n",
    "\n",
    "    # Crop the detected region and perform classification\n",
    "    cropped_image = original_image.crop((x_min, y_min, x_max, y_max))\n",
    "    cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(proc_device)\n",
    "\n",
    "    class_name, class_confidence = classify_box(cropped_tensor, models['classification_model'])\n",
    "    order_name, order_confidence = classify_order(\n",
    "        cropped_tensor, models['order_model'], models['order_model_labels'], models['order_model_thresholds']\n",
    "    )\n",
    "\n",
    "    if (class_name == \"moth\") and ('Lepidoptera' not in order_name):\n",
    "        col = 'orange'\n",
    "    elif (class_name != \"moth\") and ('Lepidoptera' in order_name):\n",
    "        col = 'purple'\n",
    "    elif (class_name == \"moth\") and ('Lepidoptera' in order_name):\n",
    "            col = 'green'\n",
    "    else:\n",
    "        col = 'red'\n",
    "\n",
    "\n",
    "    # annotate the image with bounding boxes\n",
    "    draw = ImageDraw.Draw(image3)\n",
    "    draw.rectangle([x_min, y_min, x_max, y_max], outline=col, width=4)\n",
    "\n",
    "        # Annotate image with bounding box and class\n",
    "    if (class_name == \"moth\") or (\"Lepidoptera\" in order_name):\n",
    "        species_names, species_confidences = classify_species(\n",
    "            cropped_tensor, models['species_model'],\n",
    "            models['species_model_labels'], top_n\n",
    "        )\n",
    "        # annotate the name on the image\n",
    "        draw.text((x_min, y_min-50),\n",
    "                f\"{species_names[0]}: {species_confidences[0]:.2f}\",\n",
    "                fill=col, font=ImageFont.truetype(\"DejaVuSans.ttf\", 50),\n",
    "                verticalalignment=\"bottom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 10))\n",
    "plt.imshow(image3)\n",
    "plt.axis('off')\n",
    "\n",
    "# add legend\n",
    "adj=100\n",
    "shift=-20\n",
    "plt.text(0, shift, 'red: non-moth, non-Lepidoptera', color='red', size=8)\n",
    "plt.text(0, -1*adj+shift, 'orange: moth but not Lepidoptera', color='orange', size=8)\n",
    "plt.text(0, -2*adj+shift, 'purple: Lepidoptera but not moth', color='purple', size=8)\n",
    "plt.text(0, -3*adj+shift, 'green: moth and Lepidoptera', color='green', size=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets look side by side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(10, 15))\n",
    "ax[1].imshow(image2)\n",
    "ax[1].axis('off')\n",
    "ax[0].imshow(image3)\n",
    "ax[0].axis('off')\n",
    "\n",
    "# add legend\n",
    "# adj=100\n",
    "# shift=-20\n",
    "# ax[0].text(0, shift, 'red: non-moth, non-Lepidoptera', color='red', size=8)\n",
    "# ax[0].text(0, -1*adj+shift, 'orange: moth but not Lepidoptera', color='orange', size=8)\n",
    "# ax[0].text(0, -2*adj+shift, 'purple: Lepidoptera but not moth', color='purple', size=8)\n",
    "# ax[0].text(0, -3*adj+shift, 'green: moth and Lepidoptera', color='green', size=8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
