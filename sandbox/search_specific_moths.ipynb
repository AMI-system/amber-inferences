{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for Specific Moths\n",
    "\n",
    "In this notebook, we will search for specific moths in a dataset. Specifically, those which are: \n",
    "- large\n",
    "- sloth moths\n",
    "- highest test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the working directory\n",
    "os.chdir(os.path.expanduser('~/amber-inferences'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region='aia'\n",
    "country='anguilla'\n",
    "download_dir=f'./data/qc_plots/{country}'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "inference_dir = os.path.abspath(f'/gws/nopw/j04/ceh_generic/kgoldmann/{country}_inferences_tracking/')\n",
    "\n",
    "#listdir recursively\n",
    "def listdir_recursive(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "\n",
    "# Get all csv files in the inference directory\n",
    "inference_csvs = list(listdir_recursive(inference_dir))\n",
    "inference_csvs = [c for c in inference_csvs if c.endswith('.csv')]\n",
    "inference_csvs = [c for c in inference_csvs if not 'compute' in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_csvs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deps = [os.path.basename(os.path.dirname(x)) for x in inference_csvs]\n",
    "\n",
    "# print the value counts\n",
    "print(pd.Series(deps).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inference_csvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting and Data Wrangling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(s3_client, config, key, download_dir, bucket_name):\n",
    "    download_path = os.path.join(download_dir, os.path.basename(key))\n",
    "    s3_client.download_file(bucket_name, key, download_path, Config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_session(credentials_file=\"credentials.json\"):\n",
    "    \"\"\"\n",
    "    Load AWS and API credentials from a configuration file and initialise an AWS session.\n",
    "\n",
    "    Args:\n",
    "        credentials_file (str): Path to the credentials JSON file.\n",
    "\n",
    "    Returns:\n",
    "        boto3.Client: Initialised S3 client.\n",
    "    \"\"\"\n",
    "    with open(credentials_file, encoding=\"utf-8\") as config_file:\n",
    "        aws_credentials = json.load(config_file)\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=aws_credentials[\"AWS_ACCESS_KEY_ID\"],\n",
    "        aws_secret_access_key=aws_credentials[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        region_name=aws_credentials[\"AWS_REGION\"],\n",
    "    )\n",
    "    client = session.client(\"s3\", endpoint_url=aws_credentials[\"AWS_URL_ENDPOINT\"])\n",
    "    return client\n",
    "\n",
    "client = initialise_session('./credentials.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer configuration for optimised S3 download\n",
    "transfer_config = TransferConfig(\n",
    "    max_concurrency=20,  # Increase the number of concurrent transfers\n",
    "    multipart_threshold=8 * 1024 * 1024,  # 8MB\n",
    "    max_io_queue=1000,\n",
    "    io_chunksize=262144,  # 256KB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_by_species(inference_csvs, species_names, top_n=1, confidence_threshold=0):\n",
    "    # Pre-allocate list for better performance than repeated concatenations\n",
    "    dataframes = []\n",
    "\n",
    "    # Convert species_names to set for O(1) lookup performance\n",
    "    species_set = set(species_names)\n",
    "\n",
    "    for c in tqdm(inference_csvs, desc='reading in the csvs'):\n",
    "        try:\n",
    "            input_df = pd.read_csv(c, low_memory=False)\n",
    "        except Exception as e:\n",
    "            print(f\" - Error reading {c}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Pre-allocate list for this file's subsets\n",
    "        file_subsets = []\n",
    "\n",
    "        for i in range(top_n):\n",
    "            col_name = f'top_{i+1}_species'\n",
    "            confidence_col = f'top_{i+1}_confidence'\n",
    "\n",
    "            if col_name not in input_df.columns:\n",
    "                print(f\" - Column {col_name} not found in {c}. Skipping this file.\")\n",
    "                break\n",
    "\n",
    "            if confidence_col not in input_df.columns:\n",
    "                print(f\" - Column {confidence_col} not found in {c}. Skipping this column.\")\n",
    "                continue\n",
    "\n",
    "            # Vectorized filtering with combined conditions\n",
    "            species_mask = input_df[col_name].isin(species_set)\n",
    "            confidence_mask = input_df[confidence_col] > confidence_threshold\n",
    "            combined_mask = species_mask & confidence_mask\n",
    "\n",
    "            temp = input_df[combined_mask]\n",
    "            if not temp.empty:\n",
    "                file_subsets.append(temp)\n",
    "\n",
    "        # Only proceed if we found any matching data\n",
    "        if file_subsets:\n",
    "            # Single concatenation for this file's data\n",
    "            if len(file_subsets) == 1:\n",
    "                subset_df = file_subsets[0].copy()\n",
    "            else:\n",
    "                subset_df = pd.concat(file_subsets, ignore_index=True)\n",
    "\n",
    "            # Vectorized key generation\n",
    "            prefix = os.path.basename(os.path.dirname(c)) + \"/snapshot_images/\"\n",
    "            subset_df['key'] = prefix + subset_df['image_path'].apply(os.path.basename)\n",
    "\n",
    "            dataframes.append(subset_df)\n",
    "\n",
    "    # Single concatenation at the end\n",
    "    if dataframes:\n",
    "        df_moths = pd.concat(dataframes, ignore_index=True)\n",
    "    else:\n",
    "        df_moths = pd.DataFrame()\n",
    "\n",
    "    return df_moths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_by_size(inference_csvs, keep_n=20, moth_only=True, size_cutoff=None, drop_duplicates=True):\n",
    "    # Pre-allocate list for better performance than repeated DataFrame concatenation\n",
    "    dataframes = []\n",
    "\n",
    "    for c in tqdm(inference_csvs, desc='reading in the csvs'):\n",
    "        try:\n",
    "            # Only read necessary columns if we know them, otherwise read all\n",
    "            input_df = pd.read_csv(c, low_memory=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" - Error reading {c}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Vectorized string operation with na=False for better performance\n",
    "        crop_mask = input_df['crop_status'].str.contains('crop_', na=False)\n",
    "        input_df = input_df[crop_mask]\n",
    "\n",
    "        if input_df.empty:\n",
    "            continue\n",
    "\n",
    "        # Apply filters early to reduce data size before sorting\n",
    "        if moth_only:\n",
    "            # Vectorized operations with na=False\n",
    "            lepidoptera_mask = input_df['order_name'].str.contains('Lepidoptera', na=False)\n",
    "            moth_mask = input_df['class_name'] == 'moth'\n",
    "            input_df = input_df[lepidoptera_mask | moth_mask]\n",
    "\n",
    "            if input_df.empty:\n",
    "                continue\n",
    "\n",
    "        if size_cutoff is not None:\n",
    "            input_df = input_df[input_df['crop_area'] > size_cutoff]\n",
    "\n",
    "            if input_df.empty:\n",
    "                continue\n",
    "\n",
    "        # Sort only after filtering to reduce computational cost\n",
    "        input_df = input_df.sort_values('crop_area', ascending=False)\n",
    "\n",
    "        # Drop duplicates before taking head for efficiency\n",
    "        if 'track_id' in input_df.columns and drop_duplicates:\n",
    "            input_df = input_df.drop_duplicates('track_id')\n",
    "\n",
    "        # Take only what we need early\n",
    "        input_df = input_df.head(keep_n)\n",
    "\n",
    "        # Vectorized string operations instead of apply\n",
    "        prefix = os.path.basename(os.path.dirname(c)) + \"/snapshot_images/\"\n",
    "        input_df = input_df.copy()  # Avoid SettingWithCopyWarning\n",
    "        input_df['key'] = prefix + input_df['image_path'].apply(os.path.basename)\n",
    "\n",
    "        dataframes.append(input_df)\n",
    "\n",
    "    # Single concatenation at the end is much faster than repeated concatenations\n",
    "    if dataframes:\n",
    "        df_moths = pd.concat(dataframes, ignore_index=True)\n",
    "    else:\n",
    "        df_moths = pd.DataFrame()\n",
    "\n",
    "    return df_moths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_crops(df, output_dir, region, buffer=5, group_by_species=True):\n",
    "\n",
    "    if group_by_species:\n",
    "        species = df['top_1_species'].unique()\n",
    "        for sp in species:\n",
    "            os.makedirs(os.path.join(output_dir, sp.replace(' ', '_')), exist_ok=True)\n",
    "\n",
    "    for i, row in tqdm(df.iterrows(), desc='downloading images', total=df.shape[0]):\n",
    "        try:\n",
    "            download_images(client, transfer_config, row['key'], output_dir, region)\n",
    "        except Exception as e:\n",
    "            print(f\" - Error downloading {row['key']}: {e}\")\n",
    "\n",
    "        dep = row['deployment_id']\n",
    "\n",
    "        # crop the image and save it\n",
    "        image_path = os.path.join(output_dir, str(os.path.basename(row['key'])))\n",
    "\n",
    "        if group_by_species:\n",
    "            cropped_image_path = os.path.join(output_dir, row['top_1_species'].replace(' ', '_'),  f\"{dep}_{row['crop_status']}_{os.path.basename(row['key'])}\")\n",
    "        else:\n",
    "            cropped_image_path = os.path.join(output_dir, f\"{dep}_{row['crop_status']}_{os.path.basename(row['key'])}\")\n",
    "\n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "                # Crop the image\n",
    "                x_min = float(row['x_min']) -buffer\n",
    "                y_min = float(row['y_min']) -buffer\n",
    "                x_max = float(row['x_max']) +buffer\n",
    "                y_max = float(row['y_max']) +buffer\n",
    "\n",
    "                img_cropped = img.crop((x_min, y_min, x_max, y_max))\n",
    "                # Save the cropped image\n",
    "                img_cropped.save(cropped_image_path)\n",
    "            os.remove(image_path)  # Remove the original image after cropping\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" - Error cropping {image_path}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Lists Provided "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the txt file\n",
    "def read_species_list(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        species_list = [line.strip() for line in file if line.strip()]\n",
    "    return species_list\n",
    "\n",
    "\n",
    "# if the file exists\n",
    "if os.path.exists(f'./sandbox/lists/{region}_interesting_moths.txt'):\n",
    "    moths = read_species_list(f'./sandbox/lists/{region}_interesting_moths.txt') # or './sandbox/lists/sloth_moths.txt\n",
    "\n",
    "    moths = [x.replace('nr. ', '') for x in moths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_moths = pd.read_csv(f'../gbif_download_standalone/species_checklists/{country}-moths-keys-nodup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get moths in all_moths['species_name']\n",
    "moths_in_all = all_moths[all_moths['species_name_provided'].isin(moths)]\n",
    "moths_missing = [x for x in moths if x not in moths_in_all['species_name_provided'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moths_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notable_moths = all_moths.loc[(all_moths['gbif_species_name'].isin(moths)) |\n",
    "                              (all_moths['search_species_name'].isin(moths)) |\n",
    "                              (all_moths['species_name_provided'].isin(moths)), ]\n",
    "\n",
    "notable_moths = notable_moths['gbif_species_name'].values.tolist()\n",
    "notable_moths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenced_moths = subset_by_species(inference_csvs, notable_moths, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenced_moths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenced_moths['top_1_species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenced_moths['deployment_name'].value_counts().plot(kind='bar',\n",
    "                            title='Deployment Name Counts',\n",
    "                            xlabel='Deployment Name',\n",
    "                            ylabel='Count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a historgram\n",
    "inferenced_moths['top_1_confidence'].plot(kind='hist',\n",
    "                            title='Confidence Distribution for Interesting Moths',\n",
    "                            xlabel='Confidence',\n",
    "                            bins=50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_df = inferenced_moths[(~inferenced_moths[['deployment_name', 'track_id']].duplicated()) | (inferenced_moths['track_id'].isnull())]\n",
    "\n",
    "plot_df['top_1_confidence'].plot(kind='hist',\n",
    "                            title='Confidence Distribution (accounting for tracking)',\n",
    "                            xlabel='Confidence',\n",
    "                            color='orange',\n",
    "                            bins=50)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenced_backup = inferenced_moths.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferenced_moths = inferenced_backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option to remove certain dates\n",
    "\n",
    "inferenced_moths['date'] = pd.to_datetime(inferenced_moths['image_datetime'])\n",
    "\n",
    "# subset to date only\n",
    "inferenced_moths['date'] = inferenced_moths['date'].dt.date\n",
    "\n",
    "\n",
    "\n",
    "# get the number of occurrences per date\n",
    "occurrences_per_date_by_deployment = inferenced_moths[['date', 'deployment_name']].value_counts().sort_index()\n",
    "occurrences_per_date_by_deployment\n",
    "\n",
    "occurrences_per_date = inferenced_moths['date'].value_counts().sort_index()\n",
    "occurrences_per_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot date vs count\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(occurrences_per_date.index, occurrences_per_date.values, marker='o')\n",
    "plt.title('Occurrences Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Occurrences')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# colour by deployment_name\n",
    "for dep in inferenced_moths['deployment_name'].astype('category').cat.categories:\n",
    "    dep_mask = inferenced_moths['deployment_name'] == dep\n",
    "    plt.scatter(inferenced_moths.loc[dep_mask, 'date'], inferenced_moths.loc[dep_mask, 'top_1_confidence'], label=dep)\n",
    "plt.legend()\n",
    "plt.title('Occurrences Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Prediction Confidence')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "for deployment_name, group in occurrences_per_date_by_deployment.groupby(level=1):\n",
    "    plt.plot(group.index.get_level_values(0), group.values, marker='o', label=deployment_name)\n",
    "plt.title('Occurrences Over Time by Deployment')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Occurrences')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid()\n",
    "plt.legend(title='Deployment Name')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = '/gws/nopw/j04/ceh_generic/kgoldmann/thai_interesting_moths/'\n",
    "\n",
    "os.makedirs(download_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "# for each top_1_species get the most confident images for that species\n",
    "confident_moths = inferenced_moths.sort_values('top_1_confidence', ascending=False)\n",
    "\n",
    "confident_moths = confident_moths.loc[confident_moths['top_1_confidence'] > 0.8, ]\n",
    "# confident_moths = confident_moths.groupby('top_1_species').head(1000)\n",
    "confident_moths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confident_moths['top_1_species'].value_counts()\n",
    "\n",
    "# subset to 100 of each top_1_species\n",
    "subset_confident_moths = confident_moths.groupby('top_1_species').head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_crops(subset_confident_moths, download_dir, region, buffer=5, group_by_species=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# High Test Accuracy Moths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the json files\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "region_list = {\n",
    "    'costarica':'03',\n",
    "    'singapore':'02',\n",
    "    'thailand':'02',\n",
    "}\n",
    "\n",
    "\n",
    "with open(f'/home/users/katriona/amber-inferences/sandbox/turing-{country}_v{region_list[country]}_taxon-accuracy.json') as f:\n",
    "    accuracy = json.load(f)\n",
    "\n",
    "info = accuracy['About']\n",
    "(k := next(iter(accuracy)), accuracy.pop(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accuracy['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_df_list = {}\n",
    "\n",
    "for tax in accuracy.keys():\n",
    "    print(tax)\n",
    "\n",
    "    tax_acc = accuracy[tax]\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame.from_dict(tax_acc, orient='index', columns=['Top1 Accuracy', 'Total Test Points'])\n",
    "\n",
    "    # Reset the index to turn the index into a column\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Rename the index column to 'Family'\n",
    "    df.rename(columns={'index': tax}, inplace=True)\n",
    "\n",
    "    df['Total Train Points'] = df['Total Test Points']/0.15 * 0.75\n",
    "\n",
    "    tax_df_list[tax] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aside: just checking something...\n",
    "all_species = tax_df_list['species']['species']\n",
    "\n",
    "# subset to where Timocratica is in all_species\n",
    "species_names = [s for s in all_species if 'Timocratica' in s]\n",
    "species_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_df_list['species']\n",
    "\n",
    "no_sig = tax_df_list['species']\n",
    "no_sig = no_sig.loc[(no_sig['Top1 Accuracy'] > 90) & (no_sig['Total Train Points'] > 200)]\n",
    "\n",
    "print(f'There are {no_sig.shape[0]} species with >90% accuracy and >200 training points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the inferences for these species\n",
    "df_moths = subset_by_species(inference_csvs, no_sig['species'])\n",
    "df_moths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops = pd.DataFrame(df_moths['top_1_species'].value_counts())\n",
    "crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_moths.head()\n",
    "\n",
    "# group by top_1_species, subset to 20 random rows with a mix of dep, and image_date\n",
    "df_moths_subset = df_moths.groupby('top_1_species').apply(lambda x: x.sample(n=min([20, crops['count'][-1]]), random_state=42)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = f'/gws/nopw/j04/ceh_generic/kgoldmann/{region}_confident_species'\n",
    "\n",
    "# save the csv\n",
    "df_moths_subset.to_csv(os.path.join(download_dir, '{region}_confident_species.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_crops(df_moths_subset, download_dir, region, buffer=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_moths_backup = subset_by_size(inference_csvs, keep_n=10, moth_only=False, size_cutoff=20000, drop_duplicates=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_moths = large_moths_backup.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_moths = large_moths.sort_values('crop_area', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_nonmoths = large_moths.loc[(~ large_moths['order_name'].str.contains('Lepidoptera')) & (large_moths['class_name'] != 'moth'), ]\n",
    "large_moths = large_moths.loc[large_moths['order_name'].str.contains('Lepidoptera') | (large_moths['class_name'] == 'moth'), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'deployment_id' not in large_moths.columns:\n",
    "    large_moths['deployment_id'] = [x.split('/')[0] for x in large_moths['key']]\n",
    "    large_nonmoths['deployment_id'] = [x.split('/')[0] for x in large_nonmoths['key']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# large_moths = large_moths[large_moths['crop_area'] > 20000]\n",
    "\n",
    "print(large_moths.shape)\n",
    "print(large_nonmoths.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = f'/gws/nopw/j04/ceh_generic/kgoldmann/interesting_crops/{region}/large_moths/'\n",
    "os.makedirs(os.path.join(download_dir), exist_ok=True)\n",
    "output_crops(large_moths, download_dir, region, buffer=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = f'/gws/nopw/j04/ceh_generic/kgoldmann/interesting_crops/{region}/large_nonmoths/'\n",
    "os.makedirs(os.path.join(download_dir), exist_ok=True)\n",
    "output_crops(large_nonmoths, download_dir, region, buffer=5, group_by_species=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_moths.loc[large_moths['image_path'].str.contains('20240914015359'), 'top_1_species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High enough species confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = large_moths.loc[large_moths['top_1_confidence'] > 0.8, ]\n",
    "\n",
    "print(species.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = f'/gws/nopw/j04/ceh_generic/kgoldmann/interesting_crops/{region}/large_moths/'\n",
    "os.makedirs(os.path.join(download_dir), exist_ok=True)\n",
    "output_crops(species, download_dir, region, buffer=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
