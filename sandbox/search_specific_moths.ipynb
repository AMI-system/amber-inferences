{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Searching for Specific Moths\n",
    "\n",
    "In this notebook, we will search for specific moths in a dataset. Specifically, those which are: \n",
    "- large\n",
    "- sloth moths\n",
    "- highest test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import boto3\n",
    "import json\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the working directory\n",
    "os.chdir(os.path.expanduser('~/amber-inferences'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region='cri'\n",
    "country='costarica'\n",
    "download_dir=f'./data/qc_plots/{country}'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "\n",
    "inference_dir = os.path.abspath(f'/gws/nopw/j04/ceh_generic/kgoldmann/{country}_inferences_tracking/')\n",
    "\n",
    "#listdir recursively\n",
    "def listdir_recursive(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "\n",
    "# Get all csv files in the inference directory\n",
    "inference_csvs = list(listdir_recursive(inference_dir))\n",
    "inference_csvs = [c for c in inference_csvs if c.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inference_csvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting and Data Wrangling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(s3_client, config, key, download_dir, bucket_name):\n",
    "    download_path = os.path.join(download_dir, os.path.basename(key))\n",
    "    s3_client.download_file(bucket_name, key, download_path, Config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_session(credentials_file=\"credentials.json\"):\n",
    "    \"\"\"\n",
    "    Load AWS and API credentials from a configuration file and initialise an AWS session.\n",
    "\n",
    "    Args:\n",
    "        credentials_file (str): Path to the credentials JSON file.\n",
    "\n",
    "    Returns:\n",
    "        boto3.Client: Initialised S3 client.\n",
    "    \"\"\"\n",
    "    with open(credentials_file, encoding=\"utf-8\") as config_file:\n",
    "        aws_credentials = json.load(config_file)\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=aws_credentials[\"AWS_ACCESS_KEY_ID\"],\n",
    "        aws_secret_access_key=aws_credentials[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        region_name=aws_credentials[\"AWS_REGION\"],\n",
    "    )\n",
    "    client = session.client(\"s3\", endpoint_url=aws_credentials[\"AWS_URL_ENDPOINT\"])\n",
    "    return client\n",
    "\n",
    "client = initialise_session('./credentials.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer configuration for optimised S3 download\n",
    "transfer_config = TransferConfig(\n",
    "    max_concurrency=20,  # Increase the number of concurrent transfers\n",
    "    multipart_threshold=8 * 1024 * 1024,  # 8MB\n",
    "    max_io_queue=1000,\n",
    "    io_chunksize=262144,  # 256KB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_by_species(inference_csvs, species_names, top_n=1, confidence_threshold=0):\n",
    "    df_moths = pd.DataFrame()\n",
    "    for c in tqdm(inference_csvs, desc='reading in the csvs'):\n",
    "        try:\n",
    "            input_df = pd.read_csv(c, low_memory=False)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\" - Error reading {c}: {e}\")\n",
    "            continue\n",
    "\n",
    "        subset_df = pd.DataFrame()\n",
    "        for i in range(top_n):\n",
    "            col_name = f'top_{i+1}_species'\n",
    "            if col_name not in input_df.columns:\n",
    "                print(f\" - Column {col_name} not found in {c}. Skipping this file.\")\n",
    "                continue\n",
    "            temp = input_df.loc[(input_df[col_name].isin(species_names)) &\n",
    "                                (input_df[col_name.replace('species', 'confidence')] > confidence_threshold), ]\n",
    "            subset_df = pd.concat([subset_df, temp], ignore_index=True)\n",
    "        prefix = os.path.basename(os.path.dirname(c)) + \"/snapshot_images/\"\n",
    "        subset_df['key'] =  subset_df['image_path'].apply(lambda x: f\"{prefix}{os.path.basename(x)}\")\n",
    "\n",
    "        df_moths = pd.concat([df_moths, subset_df], ignore_index=True)\n",
    "        del subset_df\n",
    "        del input_df\n",
    "    return df_moths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# High Test Accuracy Moths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the json files\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "region_list = {\n",
    "    'costarica':'03',\n",
    "}\n",
    "country = 'costarica'\n",
    "\n",
    "with open(f'/home/users/katriona/amber-inferences/sandbox/turing-{country}_v{region_list[country]}_taxon-accuracy.json') as f:\n",
    "    accuracy = json.load(f)\n",
    "\n",
    "info = accuracy['About']\n",
    "(k := next(iter(accuracy)), accuracy.pop(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(accuracy['species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_df_list = {}\n",
    "tax =\n",
    "\n",
    "for tax in accuracy.keys():\n",
    "    print(tax)\n",
    "\n",
    "    tax_acc = accuracy[tax]\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df = pd.DataFrame.from_dict(tax_acc, orient='index', columns=['Top1 Accuracy', 'Total Test Points'])\n",
    "\n",
    "    # Reset the index to turn the index into a column\n",
    "    df.reset_index(inplace=True)\n",
    "\n",
    "    # Rename the index column to 'Family'\n",
    "    df.rename(columns={'index': tax}, inplace=True)\n",
    "\n",
    "    df['Total Train Points'] = df['Total Test Points']/0.15 * 0.75\n",
    "\n",
    "    tax_df_list[tax] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aside: just checking something...\n",
    "all_species = tax_df_list['species']['species']\n",
    "\n",
    "# subset to where Timocratica is in all_species\n",
    "species_names = [s for s in all_species if 'Timocratica' in s]\n",
    "species_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tax_df_list['species']\n",
    "\n",
    "no_sig = tax_df_list['species']\n",
    "no_sig = no_sig.loc[(no_sig['Top1 Accuracy'] > 90) & (no_sig['Total Train Points'] > 200)]\n",
    "\n",
    "print(f'There are {no_sig.shape[0]} species with >90% accuracy and >200 training points')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the inferences for these species\n",
    "df_moths = subset_by_species(inference_csvs, no_sig['species'])\n",
    "df_moths.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crops = pd.DataFrame(df_moths['top_1_species'].value_counts())\n",
    "crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_moths.head()\n",
    "\n",
    "# group by top_1_species, subset to 20 random rows with a mix of dep, and image_date\n",
    "df_moths_subset = df_moths.groupby('top_1_species').apply(lambda x: x.sample(n=min([20, crops['count'][-1]]), random_state=42)).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = '/gws/nopw/j04/ceh_generic/kgoldmann/cr_confident_species'\n",
    "\n",
    "# save the csv\n",
    "df_moths_subset.to_csv(os.path.join(download_dir, 'cr_confident_species.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = 5\n",
    "\n",
    "for i, row in tqdm(df_moths_subset.iterrows(), desc='downloading images', total=df_moths_subset.shape[0]):\n",
    "    try:\n",
    "        download_images(client, transfer_config, row['key'], download_dir, 'cri')\n",
    "    except Exception as e:\n",
    "        print(f\" - Error downloading {k}: {e}\")\n",
    "\n",
    "    # crop the image and save it\n",
    "    image_path = os.path.join(download_dir, str(os.path.basename(row['key'])))\n",
    "    os.makedirs(os.path.join(download_dir, row['top_1_species'].replace(' ', '_')), exist_ok=True)\n",
    "    cropped_image_path = os.path.join(download_dir, row['top_1_species'].replace(' ', '_'), f\"{row['crop_status']}_{os.path.basename(row['key'])}\")\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            # Crop the image\n",
    "            x_min = float(row['x_min']) -buffer\n",
    "            y_min = float(row['y_min']) -buffer\n",
    "            x_max = float(row['x_max']) +buffer\n",
    "            y_max = float(row['y_max']) +buffer\n",
    "\n",
    "\n",
    "            img_cropped = img.crop((x_min, y_min, x_max, y_max))\n",
    "            # Save the cropped image\n",
    "            img_cropped.save(cropped_image_path)\n",
    "        os.remove(image_path)  # Remove the original image after cropping\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" - Error cropping {image_path}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sloth Moths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sloth_moths = pd.read_csv('../gbif_download_standalone/species_checklists/costarica-moths-keys-nodup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sms = ['Bradypodicola hahneli',\n",
    "               'Cryptoses choloepi',\n",
    "               'Cryptoses waagei',\n",
    "               'Cryptoses rufipictus',\n",
    "               'Bradypophila garbei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sloth_moths = sloth_moths.loc[sloth_moths['family_name'] == 'Pyralidae', ]\n",
    "# sloth_moths = sloth_moths.loc[(sloth_moths['gbif_species_name'].isin(example_sms)) |\n",
    "#                               (sloth_moths['search_species_name'].isin(example_sms)) |\n",
    "#                               (sloth_moths['species_name_provided'].isin(example_sms)), ]\n",
    "\n",
    "sloth_moths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm = subset_by_species(inference_csvs, example_sms, 5, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm['top_1_species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm['top_1_confidence'].plot(kind='hist', bins=50, title='Top 1 Confidence for Sloth Moths')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = '/gws/nopw/j04/ceh_generic/kgoldmann/sloth_moths'\n",
    "\n",
    "# save the csv\n",
    "df_sm.to_csv(os.path.join(download_dir, 'sloth_moths.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = 5\n",
    "\n",
    "for i, row in tqdm(df_sm.iterrows(), desc='downloading images', total=df_sm.shape[0]):\n",
    "    try:\n",
    "        download_images(client, transfer_config, row['key'], download_dir, 'cri')\n",
    "    except Exception as e:\n",
    "        print(f\" - Error downloading {k}: {e}\")\n",
    "\n",
    "    # crop the image and save it\n",
    "    image_path = os.path.join(download_dir, str(os.path.basename(row['key'])))\n",
    "    os.makedirs(os.path.join(download_dir, row['top_1_species'].replace(' ', '_')), exist_ok=True)\n",
    "    cropped_image_path = os.path.join(download_dir, row['top_1_species'].replace(' ', '_'), f\"{row['crop_status']}_{os.path.basename(row['key'])}\")\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            # Crop the image\n",
    "            x_min = float(row['x_min']) -buffer\n",
    "            y_min = float(row['y_min']) -buffer\n",
    "            x_max = float(row['x_max']) +buffer\n",
    "            y_max = float(row['y_max']) +buffer\n",
    "\n",
    "\n",
    "            img_cropped = img.crop((x_min, y_min, x_max, y_max))\n",
    "            # Save the cropped image\n",
    "            img_cropped.save(cropped_image_path)\n",
    "        os.remove(image_path)  # Remove the original image after cropping\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\" - Error cropping {image_path}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
