{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regional QC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook outputs some plots and QC results following inference for a given country. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region='cri'\n",
    "country='costarica'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General file and notebook organisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import boto3\n",
    "import json\n",
    "from boto3.s3.transfer import TransferConfig\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta, time\n",
    "import matplotlib.dates as mdates\n",
    "import numpy as np\n",
    "from matplotlib.colors import LogNorm, ListedColormap\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import matplotlib as mpl\n",
    "\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publication-ready plot settings\n",
    "mpl.rcParams.update({\n",
    "    'font.size': 16,           # Larger base font size\n",
    "    'axes.titlesize': 18,     # Title font size\n",
    "    'axes.labelsize': 16,     # Axis label font size\n",
    "    'xtick.labelsize': 14,    # Tick label font size\n",
    "    'ytick.labelsize': 14,    # Tick label font size\n",
    "    'legend.fontsize': 14,    # Legend font size\n",
    "    'figure.titlesize': 20,   # Figure title font size\n",
    "    'figure.dpi': 300,        # High-res output\n",
    "    'savefig.dpi': 300,       # High-res saved figures\n",
    "    'figure.autolayout': True # Tight layout\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the working directory\n",
    "os.chdir(os.path.expanduser('~/amber-inferences'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "download_dir=f'./data/qc_plots/{country}'\n",
    "plot_dir = f'./sandbox/plots/{country}'\n",
    "os.makedirs(download_dir, exist_ok=True)\n",
    "os.makedirs(plot_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "inference_dir = os.path.abspath(f'/gws/nopw/j04/ceh_generic/kgoldmann/{country}_inferences_tracking/')\n",
    "\n",
    "#listdir recursively\n",
    "def listdir_recursive(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for file in files:\n",
    "            yield os.path.join(root, file)\n",
    "\n",
    "# Get all csv files in the inference directory\n",
    "inference_csvs = list(listdir_recursive(inference_dir))\n",
    "inference_csvs = [c for c in inference_csvs if c.endswith('.csv')]\n",
    "inference_csvs = [c for c in inference_csvs if 'compute' not in c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inference_csvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting and Data Wrangling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_image(image_path, dir, df, ax, scaling_required=False, crop_to_highlight=None, buffer=5, subtitle=None):\n",
    "    df_image = df.loc[df['image_path'] == image_path, ]\n",
    "\n",
    "    img = plt.imread(f\"{dir}/{os.path.basename(image_path)}\")\n",
    "    image = Image.open(f\"{dir}/{os.path.basename(image_path)}\").convert(\"RGB\")\n",
    "    # subp = ax.imshow(img, origin='lower')\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    for j, row in df_image.iterrows():\n",
    "        x_min = float(row['x_min']) -buffer\n",
    "        y_min = float(row['y_min']) -buffer\n",
    "        x_max = float(row['x_max']) +buffer\n",
    "        y_max = float(row['y_max']) +buffer\n",
    "\n",
    "        if scaling_required:\n",
    "            original_width, original_height = image.size\n",
    "            x_min = x_min /300 * original_width\n",
    "            y_min = y_min /300 * original_height\n",
    "            x_max = x_max /300 * original_width\n",
    "            y_max = y_max /300 * original_height\n",
    "\n",
    "        x = float(x_min)\n",
    "        y = float(y_min)\n",
    "        w = float(x_max - x_min)\n",
    "        h = float(y_max - y_min)\n",
    "\n",
    "        if (row['class_name'] == \"moth\") and ('Lepidoptera' not in row['order_name']):\n",
    "            col = 'orange'\n",
    "        elif (row['class_name'] != \"moth\") and ('Lepidoptera' in row['order_name']):\n",
    "            col = 'purple'\n",
    "        elif (row['class_name'] == \"moth\") and ('Lepidoptera' in row['order_name']):\n",
    "                col = 'green'\n",
    "        else:\n",
    "            col = 'red'\n",
    "\n",
    "        alph = 1\n",
    "        if crop_to_highlight is not None:\n",
    "            if row['crop_status'] != crop_to_highlight:\n",
    "                alph = 0.2\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"):\n",
    "                font_path = \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"\n",
    "            else:\n",
    "                import cv2\n",
    "\n",
    "                font_path = os.path.join(\n",
    "                    cv2.__path__[0], \"qt\", \"fonts\", \"DejaVuSans.ttf\"\n",
    "                )\n",
    "            font = ImageFont.truetype(font_path, size=50)\n",
    "        except Exception as e:\n",
    "            print(f\"Loading default font, could not another: {e}\")\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "\n",
    "        if (row['class_name'] == \"moth\") or (\"Lepidoptera\" in row['order_name']):\n",
    "            draw.text(\n",
    "                (x_min, y_min),\n",
    "                f\"{row['top_1_species']}: {row['top_1_confidence']:.2f}\",\n",
    "                fill=col,\n",
    "                font=font,\n",
    "            )\n",
    "\n",
    "        draw.rectangle([x_min, y_min, x_max, y_max], outline=col, width=3)\n",
    "\n",
    "        # rect = plt.Rectangle((x, y), w, h, fill=False, edgecolor=col, linewidth=1, alpha=alph)\n",
    "        # ax.add_patch(rect)\n",
    "\n",
    "    ax = plt.axes()\n",
    "\n",
    "    ax.add_patch(img)\n",
    "    if not subtitle:\n",
    "        subtitle=f\"{os.path.basename(image_path)}\"\n",
    "    ax.set_title(subtitle)\n",
    "    ax.axis('off')\n",
    "    # return subp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images(s3_client, config, key, download_dir, bucket_name):\n",
    "    download_path = os.path.join(download_dir, os.path.basename(key))\n",
    "    s3_client.download_file(bucket_name, key, download_path, Config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise_session(credentials_file=\"credentials.json\"):\n",
    "    \"\"\"\n",
    "    Load AWS and API credentials from a configuration file and initialise an AWS session.\n",
    "\n",
    "    Args:\n",
    "        credentials_file (str): Path to the credentials JSON file.\n",
    "\n",
    "    Returns:\n",
    "        boto3.Client: Initialised S3 client.\n",
    "    \"\"\"\n",
    "    with open(credentials_file, encoding=\"utf-8\") as config_file:\n",
    "        aws_credentials = json.load(config_file)\n",
    "    session = boto3.Session(\n",
    "        aws_access_key_id=aws_credentials[\"AWS_ACCESS_KEY_ID\"],\n",
    "        aws_secret_access_key=aws_credentials[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        region_name=aws_credentials[\"AWS_REGION\"],\n",
    "    )\n",
    "    client = session.client(\"s3\", endpoint_url=aws_credentials[\"AWS_URL_ENDPOINT\"])\n",
    "    return client\n",
    "\n",
    "client = initialise_session('./credentials.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer configuration for optimised S3 download\n",
    "transfer_config = TransferConfig(\n",
    "    max_concurrency=20,  # Increase the number of concurrent transfers\n",
    "    multipart_threshold=8 * 1024 * 1024,  # 8MB\n",
    "    max_io_queue=1000,\n",
    "    io_chunksize=262144,  # 256KB\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_csvs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moth_only_df(inference_csvs):\n",
    "    df = pd.DataFrame()\n",
    "    for c in tqdm(inference_csvs, desc='Reading in the csvs'):\n",
    "        try:\n",
    "            input_df = pd.read_csv(c, low_memory=False)\n",
    "            input_df = input_df.loc[input_df['crop_status'] != 'NO DETECTIONS FOR IMAGE', ]\n",
    "            input_df = input_df.drop_duplicates(subset=[ 'x_min', 'x_max', 'y_min', 'y_max'])\n",
    "\n",
    "            input_df = input_df.loc[input_df['top_1_species'].isna() == False, ]\n",
    "            # if deployment_name in columns set dep=deployment_name, else split on basename\n",
    "            if 'deployment_name' in input_df.columns:\n",
    "                input_df['dep'] = input_df['deployment_name']\n",
    "            else:\n",
    "                input_df['dep'] = os.path.basename(c).split('.')[0].split('_')[0]\n",
    "            input_df['crop_area'] = (input_df['x_max'] - input_df['x_min']) * (input_df['y_max'] - input_df['y_min'])\n",
    "\n",
    "            # set new keys column as 'dep' and 'image_path' combined\n",
    "            input_df['keys'] = input_df['image_path'].apply(lambda x: f\"{input_df['deployment_id'].iloc[0]}/snapshot_images/{os.path.basename(x)}\")\n",
    "            df = pd.concat([df, input_df])\n",
    "            del input_df\n",
    "        except Exception as e:\n",
    "            print(f\" - Error reading {c}: {e}\")\n",
    "            continue\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cat_summary(inference_csvs, category='order_name', macro_only=False):\n",
    "    df = pd.DataFrame()\n",
    "    for c in inference_csvs:\n",
    "        try:\n",
    "            input_df = pd.read_csv(c, low_memory=False)\n",
    "        except Exception as e:\n",
    "            print(f\" - Error reading {c}: {e}\")\n",
    "            continue\n",
    "        input_df = input_df.loc[input_df['crop_status'] != 'NO DETECTIONS FOR IMAGE', ]\n",
    "        input_df = input_df.drop_duplicates(subset=[ 'x_min', 'x_max', 'y_min', 'y_max'])\n",
    "\n",
    "        # if deployment_name in columns set dep=deployment_name, else split on basename\n",
    "        if 'deployment_name' in input_df.columns:\n",
    "            input_df['dep'] = input_df['deployment_name']\n",
    "        else:\n",
    "            input_df['dep'] = os.path.basename(c).split('.')[0].split('_')[0]\n",
    "        input_df['crop_area'] = (input_df['x_max'] - input_df['x_min']) * (input_df['y_max'] - input_df['y_min'])\n",
    "\n",
    "        if macro_only:\n",
    "            input_df = input_df.loc[input_df['order_name'] == 'Lepidoptera Macros', ]\n",
    "\n",
    "        # summarise the order_name by deployment\n",
    "        summary = input_df[['dep', category]].value_counts()\n",
    "        summary = summary.reset_index()\n",
    "        summary.columns = ['deployment', category, 'count']\n",
    "        summary['file'] = os.path.basename(c)\n",
    "\n",
    "        df = pd.concat([df, summary], ignore_index=True)\n",
    "        del input_df\n",
    "\n",
    "    df = df[['deployment', category, 'count']].groupby(['deployment', category]).sum().reset_index()\n",
    "\n",
    "    df = df.sort_values(by=['deployment', 'count'], ascending=[True, False])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_date(file_name, format=\"%Y-%m-%d\"):\n",
    "    \"\"\"\n",
    "    Extracts the date from the file name based on the specified format.\n",
    "    Assuming the date is in the filename and formatted as YYYYMMDD or similar.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        file_name = os.path.splitext(file_name)[0]\n",
    "        file_raw = os.path.basename(file_name).replace(\"_\", \"-\").split(\"-\")\n",
    "        file = [x for x in file_raw if x.startswith(\"202\")][0]\n",
    "\n",
    "        # catch for delim between date and time in file name\n",
    "        if len(file) < 12:\n",
    "            i0 = [idx for idx in range(len(file_raw)) if file_raw[idx].startswith(\"202\")][0]\n",
    "            file = ('').join(file_raw[i0:i0+2])\n",
    "\n",
    "        image_dt = datetime.strptime(file, \"%Y%m%d%H%M%S%f\")\n",
    "        image_dt = datetime.strftime(image_dt, format)\n",
    "        return image_dt\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error parsing date from file name {file_name}: {e}\")\n",
    "        return np.nan\n",
    "\n",
    "def assign_night(ts, night_endpoint=12, night_startpoint=12):\n",
    "    \"\"\"\n",
    "    Defines the recording night from date and time.\n",
    "    The recording night cutoff is defined between night_startpoint on day 1\n",
    "    and night_endpoint day 2.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if ts.time() < time(night_endpoint, 0):  # before night_endpoint o'clock\n",
    "            night_start = ts.date() - timedelta(days=1)\n",
    "        elif ts.time() >= time(night_startpoint, 0):  # after night_startpoint or later\n",
    "            night_start = ts.date()\n",
    "        else:\n",
    "            # times not included in this overnight window\n",
    "            night_start = pd.NaT\n",
    "        if pd.isna(night_start):\n",
    "            return None\n",
    "        night_end = night_start + timedelta(days=1)\n",
    "        return night_start\n",
    "    except Exception as e:\n",
    "        return 'No known date'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activity_plot_data(df, drop_empty_days = True, min_date=pd.to_datetime('2024-01-01'), end_date=None, count_column='count'):\n",
    "    df['session'] = pd.to_datetime(df['session'])\n",
    "\n",
    "    if end_date is None:\n",
    "        end_date = df['session'].max()\n",
    "\n",
    "    all_dates = pd.date_range(start=min_date, end=end_date)\n",
    "    df = df.set_index('session').fillna(0).reindex(all_dates).rename_axis('session').reset_index()\n",
    "\n",
    "    # Add week, day of week, and year\n",
    "    df['week'] = df['session'].dt.isocalendar().week\n",
    "    df['weekday'] = df['session'].dt.weekday  # Monday=0\n",
    "    df['month'] = df['session'].dt.month\n",
    "    df['year'] = df['session'].dt.year\n",
    "    df['week_number'] = ((df['session'] - min_date).dt.days // 7).astype(int)\n",
    "    df['week_label'] = 'W' + df['week_number'].astype(str)\n",
    "\n",
    "    # Some dates in last week of December may belong to week 1 of next year\n",
    "    df.loc[df['week'] == 1, 'year'] = df['session'].dt.year\n",
    "\n",
    "    # Pivot for heatmap\n",
    "    heatmap_data = df.pivot_table(index='weekday', columns='week_number', values=count_column, aggfunc='sum')\n",
    "\n",
    "    # Reorder to GitHub style\n",
    "    day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "    heatmap_data.index = [day_names[i] for i in heatmap_data.index]\n",
    "    heatmap_data = heatmap_data.reindex(['Sun', 'Sat', 'Fri', 'Thu', 'Wed', 'Tue', 'Mon'])\n",
    "\n",
    "    # Determine month and year labels\n",
    "    unique_weeks = df.drop_duplicates('week_number')\n",
    "    month_labels = unique_weeks.set_index('week_number').loc[heatmap_data.columns, 'session'].dt.strftime('%b')\n",
    "    week_labels = unique_weeks.set_index('week_number').loc[heatmap_data.columns, 'week_label']\n",
    "    year_labels = unique_weeks.set_index('week_number').loc[heatmap_data.columns, 'year']\n",
    "\n",
    "\n",
    "    if drop_empty_days:\n",
    "        heatmap_data = heatmap_data.fillna(0)\n",
    "        heatmap_data = heatmap_data.loc[(heatmap_data != 0).any(axis=1)]\n",
    "        heatmap_data = heatmap_data.dropna(axis=0, how='all')\n",
    "\n",
    "    return [heatmap_data, month_labels, week_labels, year_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activity_plot(df,\n",
    "                  ax,\n",
    "                  dep,\n",
    "                  min_date=pd.to_datetime('2024-01-01'),\n",
    "                  vmin=0,\n",
    "                  vmax=1e6,\n",
    "                  custom_cmap='Greens',\n",
    "                  include_month_labels=True,\n",
    "                  end_date=None,\n",
    "                  label_buffer=1,\n",
    "                  show_colourbar=True,\n",
    "                  drop_empty_days=True,\n",
    "                  count_column='count',\n",
    "    ):\n",
    "\n",
    "    heatmap_data, month_labels, week_labels, year_labels = activity_plot_data(df, drop_empty_days=drop_empty_days, min_date=min_date, end_date=end_date, count_column=count_column)\n",
    "    masked_data = np.ma.masked_invalid(heatmap_data.values)\n",
    "\n",
    "    norm = LogNorm(vmin=vmin, vmax=vmax + 1)\n",
    "\n",
    "    c = ax.pcolor(masked_data, cmap=custom_cmap,\n",
    "                  edgecolors='white', linewidths=1, norm=norm)\n",
    "\n",
    "    ax.set_yticks(np.arange(0.5, len(heatmap_data.index), 1))\n",
    "    ax.set_yticklabels(heatmap_data.index)\n",
    "\n",
    "    ax.set_xticks(np.arange(0.5, len(week_labels), 1))\n",
    "    ax.set_xticklabels(week_labels, rotation=90)\n",
    "\n",
    "    if include_month_labels:\n",
    "        for i, label in enumerate(month_labels):\n",
    "            if i > 0 and month_labels[i] != month_labels[i - 1]:\n",
    "                ax.text(i + 0.5, -2*label_buffer, label, ha='center', va='center')\n",
    "            elif i == 0:\n",
    "                ax.text(i + 0.5, -2*label_buffer, label, ha='center', va='center')\n",
    "\n",
    "        for i, label in enumerate(year_labels):\n",
    "            if i > 0 and year_labels[i] != year_labels[i - 1]:\n",
    "                ax.text(i + 0.5, -3*label_buffer, label, ha='center', va='center')\n",
    "            elif i == 0:\n",
    "                ax.text(i + 0.5, -3*label_buffer, label, ha='center', va='center')\n",
    "\n",
    "    if show_colourbar:\n",
    "        fig.colorbar(c, ax=ax, orientation='vertical', label='Number of crops')\n",
    "    ax.set_title(dep)\n",
    "\n",
    "    return c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Activity Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ami_names = {'BAMK': 'Bishan-Ang Mo Kio Park',\n",
    "    'BTNR': 'Bukit Timah Nature Reserve',\n",
    "    'JLG': 'Jurong Lake Gardens',\n",
    "    'NTU': 'Nanyang Technological University',\n",
    "    'SBWR': 'Sungei Buloh Wetland Reserve',\n",
    "    'W02': 'Central Catchment Nature Reserve',\n",
    "    'SBG': 'Singapore Botanic Gardens',\n",
    "    'RCHG': 'Rail Corridor Holland Green'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to assign index\n",
    "def recording_index(hour):\n",
    "    if 13 <= hour <= 23:\n",
    "        return hour - 13 + 1\n",
    "    elif 0 <= hour <= 12:\n",
    "        return hour + 12\n",
    "    else:\n",
    "        return None           # Out of the range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df for deployment counts\n",
    "dep_session = pd.DataFrame()\n",
    "dep_session_moth = pd.DataFrame()\n",
    "dep_hour = pd.DataFrame()\n",
    "for c in tqdm(inference_csvs, desc='reading in the csvs'):\n",
    "    try:\n",
    "        input_df = pd.read_csv(c, low_memory=False)\n",
    "        input_df = input_df.loc[input_df['crop_status'] != 'No detections for this image.', ]\n",
    "        input_df = input_df.loc[input_df['crop_status'] != 'Image corrupt', ]\n",
    "    except Exception as e:\n",
    "        print(f\" - Error reading {c}: {e}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    if input_df.shape[0] == 0:\n",
    "        print(f\"  - No detections in {os.path.basename(c)}\")\n",
    "        continue\n",
    "    input_df = input_df.drop_duplicates(subset=[ 'x_min', 'x_max', 'y_min', 'y_max'])\n",
    "\n",
    "\n",
    "    # if deployment_name in columns set dep=deployment_name, else split on basename\n",
    "    if 'deployment_name' in input_df.columns:\n",
    "        input_df['dep'] = input_df['deployment_name']\n",
    "    else:\n",
    "        input_df['dep'] = os.path.basename(c).split('.')[0].split('_')[0]\n",
    "\n",
    "    # rename\n",
    "    for name, ami_name in ami_names.items():\n",
    "        input_df.loc[input_df['dep'] == name, 'dep'] = ami_name\n",
    "\n",
    "    input_df['crop_area'] = (input_df['x_max'] - input_df['x_min']) * (input_df['y_max'] - input_df['y_min'])\n",
    "    input_df['datetime'] = input_df['image_path'].apply(lambda x: get_date(x, format=\"%Y-%m-%d %H:%M:%S\"))\n",
    "    input_df['datetime'] = pd.to_datetime(input_df['datetime'])\n",
    "    input_df['hour'] = input_df['datetime'].dt.hour.astype(float)\n",
    "\n",
    "    # add an column with index for hour, in order of appearance\n",
    "    input_df['recording_hour'] = input_df['hour'].apply(recording_index)\n",
    "    input_df['recording_hour'] = input_df['recording_hour'] - input_df['recording_hour'].min() + 1 # rescale\n",
    "\n",
    "    input_df['hour_fraction'] = 1.0 # to represent whole hour covered by the time period\n",
    "\n",
    "    # for first hour, change to fraction of hour corresponding to first timeframe\n",
    "    start_time = 1 - input_df['datetime'].min().minute / 60\n",
    "    end_time = input_df['datetime'].max().minute / 60\n",
    "    if end_time == 0:\n",
    "        end_time = 10/60/60\n",
    "    input_df.loc[input_df['recording_hour'] == 1, 'hour_fraction'] = start_time\n",
    "    input_df.loc[input_df['recording_hour'] == input_df['recording_hour'].max(), 'hour_fraction'] = end_time\n",
    "\n",
    "    running_time = (input_df['datetime'].max() - input_df['datetime'].min()).total_seconds() / 3600\n",
    "\n",
    "    # get the session night\n",
    "    input_df['session'] = input_df['datetime'].apply(assign_night)\n",
    "\n",
    "    # summarise the order_name by deployment\n",
    "    summary = input_df[['dep', 'session']].value_counts()\n",
    "    summary = summary.reset_index()\n",
    "    summary['file'] = os.path.basename(c)\n",
    "    summary['hourly_count'] = summary['count'] / running_time\n",
    "    summary['hourly_count'] = summary['hourly_count'].replace([np.inf, -np.inf], 0)\n",
    "    dep_session = pd.concat([dep_session, summary], ignore_index=True)\n",
    "\n",
    "    summary = input_df[['dep', 'hour', 'hour_fraction', 'order_name']].value_counts()\n",
    "    summary = summary.reset_index()\n",
    "    summary['file'] = os.path.basename(c)\n",
    "    summary['adjusted_count'] = summary['count'] / summary['hour_fraction']\n",
    "    summary['adjusted_count'] = summary['adjusted_count'].replace([np.inf, -np.inf], np.nan)\n",
    "    dep_hour = pd.concat([dep_hour, summary], ignore_index=True)\n",
    "\n",
    "    # summarise the order_name by deployment\n",
    "    input_df = input_df.loc[input_df['order_name'].isna() == False, ]\n",
    "    try:\n",
    "        input_df = input_df.loc[input_df['order_name'].str.contains(\"Lepidoptera\"), ]\n",
    "        summary_moth = input_df[['dep', 'session']].value_counts()\n",
    "        summary_moth = summary_moth.reset_index()\n",
    "        summary_moth['hourly_count'] = summary_moth['count'] / running_time\n",
    "        summary_moth['hourly_count'] = summary_moth['hourly_count'].replace([np.inf, -np.inf], 0)\n",
    "\n",
    "        summary_moth['file'] = os.path.basename(c)\n",
    "        dep_session_moth = pd.concat([dep_session_moth, summary_moth], ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(f\"  - Error processing moths in {c}: {e}\")\n",
    "\n",
    "    del input_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployments = dep_session['dep'].unique()\n",
    "print(deployments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_hour_df = dep_hour[['dep', 'adjusted_count', 'count', 'hour', 'order_name']].groupby(['dep', 'hour', 'order_name']).mean().reset_index()\n",
    "dep_hour_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_session_df = dep_session.loc[dep_session['session'] != 'No known date', ]\n",
    "dep_session_df = dep_session_df[['dep', 'hourly_count', 'session']].groupby(['dep', 'session']).mean().reset_index()\n",
    "dep_session_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_session_all_moth = dep_session_moth.loc[dep_session_moth['session'] != 'No known date', ]\n",
    "dep_session_all_moth = dep_session_all_moth[['hourly_count', 'session']].groupby(['session']).mean().reset_index()\n",
    "dep_session_all_moth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_session_df_moth = dep_session_moth.loc[dep_session_moth['session'] != 'No known date', ]\n",
    "dep_session_df_moth = dep_session_df_moth[['dep', 'hourly_count', 'session']].groupby(['dep', 'session']).mean().reset_index()\n",
    "dep_session_df_moth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_df = dep_session_df[['dep', 'hourly_count']].groupby(['dep']).mean().reset_index()\n",
    "dep_df = dep_df.sort_values(by=['dep', 'hourly_count'], ascending=[True, False])\n",
    "dep_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_session_df_all = dep_session_df[['session', 'hourly_count']].groupby(['session']).mean().reset_index()\n",
    "dep_session_df_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Github style activity plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.Greens\n",
    "newcolors = cmap(np.linspace(0, 1, 256))\n",
    "newcolors[0] = [1, 1, 1, 1]  # RGBA for white\n",
    "custom_cmap = ListedColormap(newcolors)\n",
    "custom_cmap.set_bad(color=(0.85, 0.85, 0.85, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 6))\n",
    "\n",
    "activity_plot(dep_session_df_all, ax, dep='All', min_date=pd.to_datetime('2024-01-01'),\n",
    "              vmin=1, vmax=dep_session_df_all['hourly_count'].max(), custom_cmap=custom_cmap,\n",
    "              label_buffer=1.5, count_column='hourly_count')\n",
    "\n",
    "ax.set_title(\"Average Hourly Activity\")\n",
    "ax.set_aspect('equal')\n",
    "fig.tight_layout(pad=1.0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_date=pd.to_datetime('2024-01-01')\n",
    "max_labels = ((pd.to_datetime(dep_session_df['session']).max() - min_date).days // 7)\n",
    "date_range = pd.date_range(start=min_date, end=pd.to_datetime(dep_session_df['session']).max(), freq='W')\n",
    "\n",
    "week_labels = ['W' + str(i+1) for i in range(len(date_range))]\n",
    "year_labels = [x.strftime('%Y') for x in date_range]\n",
    "month_labels = [x.strftime('%b') for x in date_range]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots with activity plots for each deployment\n",
    "ncols=1\n",
    "nrows=int(math.ceil(len(deployments)/ncols))\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(20, 20), sharex='all')\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, dep in enumerate(deployments):\n",
    "    c=activity_plot(\n",
    "        dep_session_df.loc[dep_session_df['dep'] == dep, ['dep', 'session', 'hourly_count']],\n",
    "        axs[i], dep=dep, min_date=pd.to_datetime('2024-01-01'),\n",
    "        vmin=1, vmax=dep_session_df['hourly_count'].max(), label_buffer=2,\n",
    "        custom_cmap=custom_cmap, drop_empty_days=True, include_month_labels=False,\n",
    "        end_date = dep_session_df['session'].max(), show_colourbar=False,\n",
    "        count_column='hourly_count'\n",
    "    )\n",
    "    axs[i].set_aspect('equal')\n",
    "\n",
    "axs[i].set_xticks(np.arange(0.5, len(week_labels), 1))\n",
    "axs[i].set_xticklabels(week_labels, rotation=90)\n",
    "axs[i-1].set_xticks(np.arange(0.5, len(week_labels), 1))\n",
    "axs[i-1].set_xticklabels(week_labels, rotation=90)\n",
    "\n",
    "cbar_ax = fig.add_axes([0.25, -0.1*ncols, 0.5, 0.03])  # [left, bottom, width, height]\n",
    "fig.colorbar(c, cax=cbar_ax, orientation='horizontal', label='Average number of crops per hour (log scale)')\n",
    "\n",
    "# Optional week index labels\n",
    "for i, label in enumerate(month_labels):\n",
    "    if i > 0 and month_labels[i] != month_labels[i - 1]:\n",
    "        for j in range(1, ncols+1):\n",
    "            axs[len(dep_df['dep'].unique())-j].text(i + 0.5, -3.5*ncols, label, ha='center', va='center')\n",
    "    elif i == 0:\n",
    "        for j in range(1, ncols+1):\n",
    "            axs[len(dep_df['dep'].unique())-j].text(i + 0.5, -3.5*ncols, label, ha='center', va='center')\n",
    "\n",
    "\n",
    "for i, label in enumerate(year_labels):\n",
    "    if i > 0 and year_labels[i] != year_labels[i - 1]:\n",
    "        for j in range(1, ncols+1):\n",
    "            axs[len(dep_df['dep'].unique())-j].text(i + 0.5, -5*ncols, label, ha='center', va='center')\n",
    "    elif i == 0:\n",
    "        for j in range(1, ncols+1):\n",
    "            axs[len(dep_df['dep'].unique())-j].text(i + 0.5, -5*ncols, label, ha='center', va='center')\n",
    "\n",
    "plt.suptitle(\"Average Hourly Activity\")\n",
    "fig.tight_layout(pad=1.0)\n",
    "plt.show()\n",
    "\n",
    "# save the plot\n",
    "fig.savefig(f\"{plot_dir}/average_hourly_activity.png\", bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.Purples\n",
    "newcolors = cmap(np.linspace(0, 1, 256))\n",
    "newcolors[0] = [1, 1, 1, 1]  # RGBA for white\n",
    "custom_cmap = ListedColormap(newcolors)\n",
    "custom_cmap.set_bad(color=(0.85, 0.85, 0.85, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_session_moth.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 4))\n",
    "\n",
    "activity_plot(dep_session_all_moth, ax, dep='All', min_date=pd.to_datetime('2024-01-01'),\n",
    "              vmin=1, vmax=dep_session_all_moth['hourly_count'].max(),\n",
    "              custom_cmap=custom_cmap, label_buffer=1.5, count_column='hourly_count')\n",
    "\n",
    "ax.set_title(\"Crop Activity Heatmap\")\n",
    "ax.set_aspect('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots with activity plots for each deployment\n",
    "ncols=1\n",
    "nrows=int(math.ceil(len(deployments)/ncols))\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(20, 25), sharex='all')\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, dep in enumerate(dep_df['dep'].unique()):\n",
    "    c=activity_plot(dep_session_df_moth.loc[dep_session_df_moth['dep'] == dep, ['dep', 'session', 'hourly_count']],\n",
    "                  axs[i], dep=dep, min_date=pd.to_datetime('2024-01-01'),\n",
    "                vmin=1, vmax=dep_session_df_moth['hourly_count'].max(), label_buffer=2,\n",
    "                custom_cmap=custom_cmap, include_month_labels=False,\n",
    "                end_date = pd.to_datetime(dep_session_df_moth['session']).max(),\n",
    "                    drop_empty_days=True,\n",
    "                show_colourbar=False, count_column='hourly_count')\n",
    "    axs[i].set_aspect('equal')\n",
    "\n",
    "axs[i].set_xticks(np.arange(0.5, len(week_labels), 1))\n",
    "axs[i].set_xticklabels(week_labels, rotation=90)\n",
    "axs[i-1].set_xticks(np.arange(0.5, len(week_labels), 1))\n",
    "axs[i-1].set_xticklabels(week_labels, rotation=90)\n",
    "\n",
    "cbar_ax = fig.add_axes([0.25, -0.1*ncols, 0.5, 0.03])  # [left, bottom, width, height]\n",
    "fig.colorbar(c, cax=cbar_ax, orientation='horizontal', label='Hourly moth count (log scale)')\n",
    "\n",
    "# Optional week index labels\n",
    "for i, label in enumerate(month_labels):\n",
    "    if i > 0 and month_labels[i] != month_labels[i - 1]:\n",
    "        for j in range(1, ncols+1):\n",
    "            axs[len(dep_df['dep'].unique())-j].text(i + 0.5, -3.5*ncols, label, ha='center', va='center')\n",
    "    elif i == 0:\n",
    "        for j in range(1, ncols+1):\n",
    "            axs[len(dep_df['dep'].unique())-j].text(i + 0.5, -3.5*ncols, label, ha='center', va='center')\n",
    "\n",
    "\n",
    "for i, label in enumerate(year_labels):\n",
    "    if i > 0 and year_labels[i] != year_labels[i - 1]:\n",
    "        for j in range(1, ncols+1):\n",
    "            axs[len(dep_df['dep'].unique())-j].text(i + 0.5, -5*ncols, label, ha='center', va='center')\n",
    "    elif i == 0:\n",
    "        for j in range(1, ncols+1):\n",
    "            axs[len(dep_df['dep'].unique())-j].text(i + 0.5, -5*ncols, label, ha='center', va='center')\n",
    "\n",
    "plt.suptitle(\"Moth Crop Activity Heatmap\")\n",
    "\n",
    "# save the plot\n",
    "fig.savefig(f\"{plot_dir}/moth_crop_activity_heatmap.png\", bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Polar Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_hour_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_hour_df_all = dep_hour_df[['hour',  'order_name', 'adjusted_count', 'count']].groupby(['hour', 'order_name']).mean().reset_index()\n",
    "\n",
    "dep_hour_df_all['adjust'] = dep_hour_df_all['adjusted_count'] != dep_hour_df_all['count']\n",
    "\n",
    "# sort the hour by 18, 19, 20, 21, 22, 23, 0, 1, 2, 3, 4\n",
    "dep_hour_df_all['recording_hour'] = dep_hour_df_all['hour'].apply(recording_index)\n",
    "dep_hour_df_all['recording_hour'] = dep_hour_df_all['recording_hour'] - dep_hour_df_all['recording_hour'].min() + 1 # rescale\n",
    "\n",
    "dep_hour_df_all = dep_hour_df_all.sort_values(by=['recording_hour', 'order_name'], ascending=[True, True])\n",
    "dep_hour_df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_hour_df_all['adjust'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_names = dep_hour_df_all['order_name'].unique()\n",
    "palette = sns.color_palette(\"tab20\", len(order_names))\n",
    "order_colors = dict(zip(order_names, palette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={'projection': 'polar'}, figsize=(10, 10))\n",
    "\n",
    "for order, group in dep_hour_df_all.groupby('order_name'):\n",
    "    df_loop = pd.concat([group, group.iloc[[0]]]).reset_index(drop=True)\n",
    "    df_loop = df_loop.sort_values(by='recording_hour').reset_index(drop=True)\n",
    "\n",
    "    # add a discontinuity between the first and last point\n",
    "    if any(df_loop['hour'] < 12):\n",
    "        end_of_session = df_loop.loc[df_loop['hour'] < 12, 'hour'].max()\n",
    "        end_of_session_index = df_loop.loc[df_loop['hour'] == end_of_session].index[0]\n",
    "        extra_row = pd.DataFrame({'hour': end_of_session, 'count': [np.nan], 'order_name': [order]})\n",
    "\n",
    "        # add a row after the end_of_session hour\n",
    "        if end_of_session_index < len(df_loop) - 1:\n",
    "            next_hour = df_loop.loc[end_of_session_index + 1, 'hour']\n",
    "            df_loop = pd.concat([df_loop.iloc[:end_of_session_index + 1], extra_row, df_loop.iloc[end_of_session_index + 1:]], ignore_index=True)\n",
    "        else:\n",
    "            df_loop = pd.concat([df_loop, extra_row], ignore_index=True)\n",
    "\n",
    "    theta = np.deg2rad(df_loop['hour'] * 15)  # 24h clock mapped to 360°\n",
    "\n",
    "    # Plot main line\n",
    "    line = ax.plot(theta, df_loop['adjusted_count'], label=order, color=order_colors[order])[0]\n",
    "\n",
    "    # Make first and last segments dashed if they exist\n",
    "    mask = df_loop['recording_hour'] < 3\n",
    "    ax.plot(theta[mask],\n",
    "            df_loop.loc[mask, 'adjusted_count'],\n",
    "           color='white', linestyle='--', linewidth=line.get_linewidth())\n",
    "\n",
    "    mask = df_loop['recording_hour'] > dep_hour_df_all['recording_hour'].max() - 2\n",
    "    ax.plot(theta[mask],\n",
    "            df_loop.loc[mask, 'adjusted_count'],\n",
    "           color='white', linestyle='--', linewidth=line.get_linewidth())\n",
    "\n",
    "ax.set_theta_direction(-1)\n",
    "ax.set_theta_offset(np.pi / 2)\n",
    "ax.set_xticks(np.linspace(0, 2 * np.pi, 24, endpoint=False))\n",
    "ax.set_xticklabels([f\"{h}:00\" for h in range(24)])\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# add legend for dashed line\n",
    "ax.plot([], [], color='white', linestyle='-', linewidth=2, label=' ')\n",
    "ax.plot([], [], color='black', linestyle='--', linewidth=2, label='Full hour projection')\n",
    "ax.legend(loc='upper left', bbox_to_anchor=(1.2, 1), title='Order Name', fontsize=10)\n",
    "\n",
    "plt.title('Average Activity by Hour of Day\\n\\n')\n",
    "# plt.show()\n",
    "\n",
    "# save the polar plot\n",
    "fig.savefig(f\"{plot_dir}/moth_activity_polar_plot.png\", bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots with activity plots for each deployment\n",
    "ncols=2\n",
    "nrows=int(math.ceil(len(deployments)/ncols))\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(12, 24), sharex='all', subplot_kw={'projection': 'polar'})\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, dep in enumerate(dep_df['dep'].unique()):\n",
    "    df = dep_hour_df.loc[dep_hour_df['dep'] == dep, ]\n",
    "\n",
    "    for order, group in df.groupby('order_name'):\n",
    "\n",
    "        df_loop = pd.concat([group, group.iloc[[0]]]).reset_index(drop=True)\n",
    "        df_loop['recording_hour'] = df_loop['hour'].apply(recording_index)\n",
    "        df_loop['recording_hour'] = df_loop['recording_hour'] - df_loop['recording_hour'].min() + 1 # rescale\n",
    "        df_loop = df_loop.sort_values(by='recording_hour').reset_index(drop=True)\n",
    "\n",
    "        # after the maximum hour under 12, add a discontinuity\n",
    "        if any(df_loop['hour'] < 12):\n",
    "            end_of_session = df_loop.loc[df_loop['hour'] < 12, 'hour'].max()\n",
    "            end_of_session_index = df_loop.loc[df_loop['hour'] == end_of_session].index[0]\n",
    "            # Create extra_row with all necessary columns including adjusted_count\n",
    "            extra_row = pd.DataFrame({\n",
    "                'dep': [dep],\n",
    "                'hour': [end_of_session],\n",
    "                'count': [np.nan],\n",
    "                'adjusted_count': [np.nan],  # Add this column\n",
    "                'order_name': [order],\n",
    "                'was_adjusted': [False]\n",
    "            })\n",
    "\n",
    "            # add a row after the end_of_session hour\n",
    "            if end_of_session_index < len(df_loop) - 1:\n",
    "                # get the next hour\n",
    "                next_hour = df_loop.loc[end_of_session_index + 1, 'hour']\n",
    "                df_loop = pd.concat([df_loop.iloc[:end_of_session_index + 1], extra_row, df_loop.iloc[end_of_session_index + 1:]], ignore_index=True)\n",
    "            else:\n",
    "                df_loop = pd.concat([df_loop, extra_row], ignore_index=True)\n",
    "\n",
    "        # Calculate was_adjusted after all concatenations are done, and handle NaN values\n",
    "        df_loop['was_adjusted'] = (df_loop['adjusted_count'] != df_loop['count']).fillna(False)\n",
    "\n",
    "        theta = np.deg2rad(df_loop['hour'] * 15)  # 24h clock mapped to 360°\n",
    "        line = axs[i].plot(theta, df_loop['adjusted_count'], label=order, color=order_colors[order])\n",
    "\n",
    "        # Make first and last segments dashed if they exist\n",
    "        mask = (df_loop['was_adjusted']) & (df_loop['hour'] > 12)\n",
    "        axs[i].plot(theta[mask], df_loop.loc[mask, 'adjusted_count'],\n",
    "            color='white', linestyle='--', linewidth=line[0].get_linewidth())\n",
    "\n",
    "        mask = (df_loop['was_adjusted']) & (df_loop['hour'] < 12)\n",
    "        axs[i].plot(theta[mask], df_loop.loc[mask, 'adjusted_count'],\n",
    "            color='white', linestyle='--', linewidth=line[0].get_linewidth())\n",
    "\n",
    "\n",
    "    axs[i].set_theta_direction(-1)\n",
    "    axs[i].set_theta_offset(np.pi / 2)\n",
    "    axs[i].set_xticks(np.linspace(0, 2 * np.pi, 24, endpoint=False))\n",
    "    axs[i].set_xticklabels([f\"{h}:00\" for h in range(24)])\n",
    "    axs[i].set_yscale('log')\n",
    "    axs[i].set_title(f'{dep}\\n\\n')\n",
    "\n",
    "# Create first legend for colors (order names)\n",
    "color_handles = [plt.Line2D([0], [0], color=color, label=order, linewidth=2)\n",
    "                for order, color in order_colors.items()]\n",
    "color_legend = fig.legend(handles=color_handles, title='Order Name', loc='lower center',\n",
    "                         ncol=3, bbox_to_anchor=(0.5, -0.12))\n",
    "\n",
    "# Create second legend for linestyle (discontinuity)\n",
    "style_handles = [plt.Line2D([0], [0], color='black', linestyle='-', linewidth=2, label='Recorded Hours'),\n",
    "                plt.Line2D([0], [0], color='black', linestyle='--', linewidth=2, label='Hour Projection')]\n",
    "style_legend = fig.legend(handles=style_handles, title='Line Type', loc='lower center',\n",
    "                         ncol=2, bbox_to_anchor=(0.5, -0.18))\n",
    "\n",
    "plt.suptitle('Average Insect Activity by Hour of Day\\n\\n')\n",
    "plt.tight_layout(pad=3.0)\n",
    "plt.subplots_adjust(top=0.93, bottom=0.25)  # Increased bottom margin to accommodate both legends\n",
    "# plt.show()\n",
    "\n",
    "fig.savefig(f\"{plot_dir}/moth_activity_polar_plot_by_deployment.png\", bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Line Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots with activity plots for each deployment\n",
    "ncols=3\n",
    "nrows=int(math.ceil(len(deployments)/ncols))\n",
    "fig, axs = plt.subplots(nrows, ncols, figsize=(12, 12))\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, dep in enumerate(dep_df['dep'].unique()):\n",
    "    df = dep_hour_df.loc[dep_hour_df['dep'] == dep, ]\n",
    "    # If the hour is < 12 then add 24 to the hour\n",
    "    df.loc[df['hour'] < 12, 'hour'] = df.loc[df['hour'] < 12, 'hour'] +24 # df['hour'].apply(lambda x: x + 24 if x < 12 else x)\n",
    "\n",
    "    for order, group in df.groupby('order_name'):\n",
    "        group = group.sort_values('hour')\n",
    "        axs[i].plot(group['hour'], group['adjusted_count'], label=order, color=order_colors[order])\n",
    "    axs[i].set_yscale('log')\n",
    "    axs[i].set_title(f'{dep}')\n",
    "    axs[i].set_ylabel('Count (log scale)')\n",
    "    axs[i].set_xlabel('Hour of Day')\n",
    "\n",
    "    min_hour = df['hour'].min()\n",
    "    max_hour = df['hour'].max()\n",
    "    times_raw = np.arange(16, 4+24 + 1, 1)\n",
    "    times_labels = [x - 24 if x > 24 else x for x in times_raw]\n",
    "    times_labels = [f\"{int(x)}:00\" for x in times_labels]\n",
    "    axs[i].set_xticks(times_raw)\n",
    "    axs[i].set_xlim(16, 4+24+1)\n",
    "    axs[i].set_xticklabels(times_labels, rotation=-45, ha='left')\n",
    "\n",
    "\n",
    "# Build a single legend using the shared color map\n",
    "handles = [plt.Line2D([0], [0], color=color, label=order) for order, color in order_colors.items()]\n",
    "fig.legend(handles=handles, title='Order Name', loc='lower center', ncol=3, bbox_to_anchor=(0.5, -0.2))\n",
    "\n",
    "plt.suptitle('Insect Activity by Hour of Day\\n\\n')\n",
    "plt.tight_layout()\n",
    "plt.subplots_adjust(top=0.93)\n",
    "fig.savefig(f\"{plot_dir}/moth_activity_line_plot_by_deployment.png\", bbox_inches='tight', dpi=300)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Violin Plots\n",
    "\n",
    "Average number of crops per night per deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_hour_sub = dep_hour[dep_hour['count'] == dep_hour['adjusted_count']]\n",
    "dep_hour_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplot for each order, with a violin plot of the deployment on the x-axis and the count on the y-axis\n",
    "order_names = sorted(dep_hour_sub['order_name'].unique())\n",
    "palette = sns.color_palette(\"tab20\", len(order_names))\n",
    "order_colors = dict(zip(order_names, palette))\n",
    "\n",
    "fig, axs = plt.subplots(math.ceil(len(order_names)/3), 3, figsize=(18, len(order_names)), sharex=True)\n",
    "axs = axs.flatten()\n",
    "\n",
    "for i, order in enumerate(order_names):\n",
    "    df = dep_hour_sub[dep_hour_sub['order_name'] == order]\n",
    "    sns.violinplot(x='dep', y='count', data=df, ax=axs[i], density_norm='width', color=order_colors[order], inner='quartile')\n",
    "    axs[i].set_title(f'{order} Count by Deployment')\n",
    "    axs[i].set_ylabel('Mean Count')\n",
    "    axs[i].set_xlabel('')\n",
    "    axs[i].set_xticks(range(len(dep_hour_sub['dep'].unique())))\n",
    "    axs[i].set_xticklabels(dep_hour_sub['dep'].unique(), rotation=45, ha='right')\n",
    "    axs[i].grid(True, which='both', linestyle='--', linewidth=0.5)\n",
    "    # axs[i].set_ylim(1, df['count'].max() * 1.1)\n",
    "    #\n",
    "\n",
    "# make the last plot empty\n",
    "if len(order_names) % 3 != 0:\n",
    "    axs[len(order_names)].axis('off')\n",
    "\n",
    "plt.suptitle('Distribution of Insect Counts per Night by Deployment and Order Name\\n\\n')\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.savefig(f\"{plot_dir}/count_violin_plot_by_deployment.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Crop counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3))\n",
    "sns.barplot(data=dep_df.sort_values('hourly_count', ascending=False), hue='hourly_count', y='hourly_count', x='dep')\n",
    "plt.title('All crops per deployment', size=8)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=6)\n",
    "plt.tight_layout()\n",
    "# plt.yscale('log')\n",
    "plt.yticks(fontsize=6)\n",
    "plt.legend().set_visible(False)\n",
    "plt.ylabel('Mean Nightly Count', size=8)\n",
    "plt.xlabel('')\n",
    "\n",
    "# save\n",
    "plt.savefig(f\"{plot_dir}/all_crops_per_deployment.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Moth Activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = moth_only_df(inference_csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image_path'].value_counts().plot(kind='hist', bins=100, figsize=(5, 3))\n",
    "plt.title('Moth crops per image')\n",
    "plt.xlabel('Number of moth crops per image (n > 0)')\n",
    "plt.yscale('log')\n",
    "plt.savefig(f\"{plot_dir}/moths_per_image.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['dep'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the confidence distribution for each deployment in subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 5), sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, dep in enumerate(df['dep'].unique()):\n",
    "    ax = axes[i]\n",
    "    dep_df = df.loc[df['dep'] == dep, ]\n",
    "    dep_df['image_path'].value_counts().plot(kind='hist', bins=100, ax=ax)\n",
    "    ax.set_title(dep)\n",
    "    ax.set_xlabel('Number of moth crops per image (n > 0)')\n",
    "    ax.set_yscale('log')\n",
    "\n",
    "plt.suptitle(f'Confidence Distribution of 5 Most Likely Species Predictions')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/confidence_distribution.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_counts = df['dep'].value_counts()\n",
    "dep_counts = dep_counts.reset_index()\n",
    "dep_counts.columns = ['deployment', 'count']\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(data=dep_counts, hue='count', y='count', x='deployment')\n",
    "plt.title('Moth crops per deployment')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.yscale('log')\n",
    "# plt.legend(visible=False)\n",
    "plt.savefig(f\"{plot_dir}/moth_crops.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Order Prediction Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot showing the correlation between order confidence and crop area\n",
    "# this plot takes a while to compile, uncomment if you want to run\n",
    "# sns.regplot(x=df['order_confidence'], y=df['crop_area'], logx=True, line_kws=dict(color=\"r\"))\n",
    "# plt.yscale('log')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_counts = cat_summary(inference_csvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_counts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(data=order_counts, hue='deployment', y='count', x='order_name')\n",
    "plt.title('Counts by Deployment and Order')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Order')\n",
    "plt.ylabel('Number of Crops')\n",
    "plt.legend(title='Order Name', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Species Prediction Plots (Macro only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_counts = cat_summary(inference_csvs, category='top_1_species', macro_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to only the top 10 species\n",
    "top_species = species_counts[['top_1_species',  'count']].groupby('top_1_species').sum().reset_index()\n",
    "top_species = top_species.sort_values(by='count', ascending=False)\n",
    "top_species = top_species.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match the order_name fromd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(data=top_species, y='count', x='top_1_species')\n",
    "plt.title('Top Macro Species Counts')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel('Most Likely Species')\n",
    "plt.ylabel('Number of Crops')\n",
    "plt.tight_layout()\n",
    "\n",
    "# save the plot\n",
    "plt.savefig(f\"{plot_dir}/top_species_counts.png\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_species.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_counts['deployment'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=math.ceil(len(species_counts['deployment'].unique()) / 2), figsize=(10, 7), sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, dep in enumerate(species_counts['deployment'].unique()):\n",
    "    ax = axes[i]\n",
    "    dep_df = species_counts.loc[species_counts['deployment'] == dep, ]\n",
    "    dep_df = dep_df.sort_values(by='count', ascending=False).head(10)\n",
    "    sns.barplot(data=dep_df, y='count', x='top_1_species', ax=ax)\n",
    "    ax.set_title(dep)\n",
    "    ax.set_xticks(range(len(dep_df)))\n",
    "    ax.set_xticklabels(dep_df['top_1_species'], rotation=45, ha='right')\n",
    "    ax.set_xlabel('')\n",
    "    ax.set_ylabel('Number of Crops')\n",
    "\n",
    "plt.suptitle('Top Species Observations by Deployment')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/top_species_counts_by_deploument.png\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7, 5))\n",
    "sns.barplot(data=species_counts.loc[species_counts['top_1_species'].isin(top_species['top_1_species']), ], hue='deployment', y='count', x='top_1_species')\n",
    "plt.title('Top Species Counts by Deployment')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.legend(title='Most popular species', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.xlabel('Most Likely Species')\n",
    "plt.ylabel('Number of Crops')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['top_1_confidence'].plot(kind='hist', bins=50, figsize=(6, 3))\n",
    "df['top_2_confidence'].plot(kind='hist', bins=50, color='orange', alpha=0.5)\n",
    "df['top_3_confidence'].plot(kind='hist', bins=50, color='yellow', alpha=0.5)\n",
    "df['top_4_confidence'].plot(kind='hist', bins=50, color='green', alpha=0.5)\n",
    "df['top_5_confidence'].plot(kind='hist', bins=50, color='purple', alpha=0.5)\n",
    "\n",
    "plt.legend(['Top 1 Confidence', 'Top 2 Confidence', 'Top 3 Confidence', 'Top 4 Confidence', 'Top 5 Confidence'])\n",
    "plt.title('Confidence Distribution of 5 Most Likely Species Predictions')\n",
    "plt.xlabel('Confidence')\n",
    "plt.savefig(f\"{plot_dir}/confidence_distribution.png\", bbox_inches='tight', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the confidence distribution for each deployment in subplots\n",
    "fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(10, 5), sharex=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, dep in enumerate(df['dep'].unique()):\n",
    "    ax = axes[i]\n",
    "    dep_df = df.loc[df['dep'] == dep, ]\n",
    "    dep_df['top_1_confidence'].plot(kind='hist', bins=50, ax=ax, alpha=0.5)\n",
    "    dep_df['top_2_confidence'].plot(kind='hist', bins=50, ax=ax, color='orange', alpha=0.5)\n",
    "    dep_df['top_3_confidence'].plot(kind='hist', bins=50, ax=ax, color='yellow', alpha=0.5)\n",
    "    dep_df['top_4_confidence'].plot(kind='hist', bins=50, ax=ax, color='green', alpha=0.5)\n",
    "    dep_df['top_5_confidence'].plot(kind='hist', bins=50, ax=ax, color='purple', alpha=0.5)\n",
    "    ax.legend().set_visible(False)\n",
    "    ax.set_title(dep)\n",
    "    ax.set_xlabel('Confidence')\n",
    "    ax.set_ylabel('Count')\n",
    "\n",
    "\n",
    "handles, _ = axes[0].get_legend_handles_labels()\n",
    "labels = ['Top 1 Confidence', 'Top 2 Confidence', 'Top 3 Confidence', 'Top 4 Confidence', 'Top 5 Confidence']\n",
    "fig.legend(handles, labels, loc='upper right', bbox_to_anchor=(1.2, 0.9))\n",
    "plt.suptitle(f'Confidence Distribution of 5 Most Likely Species Predictions')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/confidence_distribution_by_deployment.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Confident cases for each order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_df_from_file(inference_csvs, sort_by='top_1_confidence', ascending=False, n_keep=100, group_by_col=None, remove_none=True):\n",
    "    \"\"\"\n",
    "    Load the dataframe from the CSV file and sort it by the specified column and keep the top n entries (with an optional group by).\n",
    "\n",
    "    Args:\n",
    "        inference_csvs (list): List of CSV file paths to read.\n",
    "        sort_by (str): The column name to sort by.\n",
    "        ascending (bool): Whether to sort in ascending order.\n",
    "        n_keep (int): The number of entries to keep from each file.\n",
    "        group_by_col (str): The column name to group by before keeping the top n entries.\n",
    "        remove_none (bool): Whether to remove entries with 'NO DETECTIONS FOR IMAGE'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The sorted dataframe.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for c in tqdm(inference_csvs):\n",
    "        try:\n",
    "            input_df = pd.read_csv(c, low_memory=False)\n",
    "        except Exception as e:\n",
    "            print(f\" - Error reading {c}: {e}\")\n",
    "            continue\n",
    "        if remove_none:\n",
    "            input_df = input_df.loc[input_df['crop_status'] != 'NO DETECTIONS FOR IMAGE', ]\n",
    "        input_df = input_df.drop_duplicates(subset=[ 'x_min', 'x_max', 'y_min', 'y_max'])\n",
    "        input_df = input_df.sort_values(by=sort_by, ascending=ascending)\n",
    "\n",
    "        input_df['dep'] = os.path.basename(c).split('.')[0].split('_')[0]\n",
    "        input_df['crop_area'] = (input_df['x_max'] - input_df['x_min']) * (input_df['y_max'] - input_df['y_min'])\n",
    "\n",
    "        # set new keys column as 'dep' and 'image_path' combined\n",
    "        input_df['keys'] = input_df['image_path'].apply(lambda x: f\"{input_df['dep'].iloc[0]}/snapshot_images/{os.path.basename(x)}\")\n",
    "\n",
    "        if group_by_col is not None:\n",
    "            input_df = input_df.groupby(group_by_col).head(n_keep)\n",
    "        else:\n",
    "            input_df = input_df.head(n_keep)\n",
    "        df = pd.concat([df, input_df])\n",
    "        del input_df\n",
    "\n",
    "    df = df.sort_values(by=['dep', sort_by], ascending=[True, ascending]).reset_index(drop=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_df = load_df_from_file(inference_csvs, sort_by='order_confidence', ascending=False, n_keep=100, remove_none=True, group_by_col='order_name')\n",
    "\n",
    "# get *the* most confident image for each order and deployment\n",
    "order_df = order_df.groupby(['order_name', 'dep']).head(1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by confidence and subset\n",
    "df_order = order_df.sort_values(by='order_confidence', ascending=False)\n",
    "df_order = df_order.loc[df_order['order_confidence'] > 0.99, ]\n",
    "df_order.reset_index(drop=True, inplace=True)\n",
    "\n",
    "print(df_order.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the relevant images\n",
    "os.makedirs(os.path.join(download_dir, 'order'), exist_ok=True)\n",
    "\n",
    "for i, row in df_order.iterrows():\n",
    "    download_images(client, transfer_config, row['keys'], os.path.join(download_dir, 'order'), region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "from PIL import ImageDraw, Image, ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_image2(image_path, dir, df, ax, scaling_required=False, crop_to_highlight=None, buffer=5, subtitle=None):\n",
    "    df_image = df.loc[df['image_path'] == image_path, ]\n",
    "\n",
    "    img = plt.imread(f\"{dir}/{os.path.basename(image_path)}\")\n",
    "    image = Image.open(f\"{dir}/{os.path.basename(image_path)}\").convert(\"RGB\")\n",
    "    subp = ax.imshow(img, origin='lower')\n",
    "\n",
    "    draw = ImageDraw.Draw(image)\n",
    "\n",
    "    for j, row in df_image.iterrows():\n",
    "        x_min = float(row['x_min']) -buffer\n",
    "        y_min = float(row['y_min']) -buffer\n",
    "        x_max = float(row['x_max']) +buffer\n",
    "        y_max = float(row['y_max']) +buffer\n",
    "\n",
    "        if scaling_required:\n",
    "            original_width, original_height = image.size\n",
    "            x_min = x_min /300 * original_width\n",
    "            y_min = y_min /300 * original_height\n",
    "            x_max = x_max /300 * original_width\n",
    "            y_max = y_max /300 * original_height\n",
    "\n",
    "        x = float(x_min)\n",
    "        y = float(y_min)\n",
    "        w = float(x_max - x_min)\n",
    "        h = float(y_max - y_min)\n",
    "\n",
    "        if (row['class_name'] == \"moth\") and ('Lepidoptera' not in row['order_name']):\n",
    "            col = 'orange'\n",
    "        elif (row['class_name'] != \"moth\") and ('Lepidoptera' in row['order_name']):\n",
    "            col = 'purple'\n",
    "        elif (row['class_name'] == \"moth\") and ('Lepidoptera' in row['order_name']):\n",
    "                col = 'green'\n",
    "        else:\n",
    "            col = 'red'\n",
    "\n",
    "        alph = 1\n",
    "        if crop_to_highlight is not None:\n",
    "            if row['crop_status'] != crop_to_highlight:\n",
    "                alph = 0.2\n",
    "\n",
    "        try:\n",
    "            if os.path.exists(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"):\n",
    "                font_path = \"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\"\n",
    "            else:\n",
    "                import cv2\n",
    "\n",
    "                font_path = os.path.join(\n",
    "                    cv2.__path__[0], \"qt\", \"fonts\", \"DejaVuSans.ttf\"\n",
    "                )\n",
    "            font = ImageFont.truetype(font_path, size=50)\n",
    "        except Exception as e:\n",
    "            print(f\"Loading default font, could not another: {e}\")\n",
    "            font = ImageFont.load_default()\n",
    "\n",
    "\n",
    "        if (row['class_name'] == \"moth\") or (\"Lepidoptera\" in row['order_name']):\n",
    "            draw.text(\n",
    "                (x_min, y_min),\n",
    "                f\"{row['top_1_species']}: {row['top_1_confidence']:.2f}\",\n",
    "                fill=col,\n",
    "                font=font,\n",
    "            )\n",
    "\n",
    "        draw.rectangle([x_min, y_min, x_max, y_max], outline=col, width=3)\n",
    "\n",
    "        # remove axes\n",
    "        ax.axis('off')\n",
    "\n",
    "        if subtitle is not None:\n",
    "            ax.set_title(subtitle, fontsize=16, pad=20)\n",
    "\n",
    "        rect = plt.Rectangle((x, y), w, h, fill=False, edgecolor=col, linewidth=1, alpha=alph)\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each unique image, annotate the bounding box\n",
    "fig, ax = plt.subplots(13, 3, figsize=(10, 17))\n",
    "ax = ax.ravel()\n",
    "\n",
    "df_order = df_order.sort_values(by='order_name', ascending=False)\n",
    "\n",
    "# Use enumerate to get the correct loop index for ax[]\n",
    "for j, (_, row) in enumerate(df_order.iterrows()):\n",
    "    annotate_image2(\n",
    "        image_path=row['image_path'],\n",
    "        dir=os.path.join(download_dir, 'order'),\n",
    "        df=df_order,\n",
    "        ax=ax[j],\n",
    "        crop_to_highlight=row['crop_status'],\n",
    "        buffer=20,\n",
    "        scaling_required=False,\n",
    "        subtitle=f\"{row['dep']}\\n{row['order_name']}, {row['order_confidence']:.2f}\"\n",
    "    )\n",
    "\n",
    "# Turn off any unused axes\n",
    "for i in range(len(df_order), len(ax)):\n",
    "    ax[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/order_plots.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Confident Species Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confident = load_df_from_file(inference_csvs, sort_by='top_1_confidence', ascending=False, n_keep=10, remove_none=True, group_by_col='top_1_species')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confident_backup = df_confident.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by top_1_confidence\n",
    "# df_confident = df.sort_values(by='top_1_confidence', ascending=False)\n",
    "# df_confident.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # remove duplicated rows by image path, and bounding box\n",
    "# df_confident = df_confident.drop_duplicates(subset=['image_path', 'x_min', 'y_min', 'x_max', 'y_max'])\n",
    "# df_confident = df_confident.drop_duplicates(subset=['top_1_species'])\n",
    "\n",
    "top_n=20\n",
    "\n",
    "df_confident = df_confident.head(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort by image_path\n",
    "df_confident.sort_values(by='image_path', inplace=True)\n",
    "df_confident.reset_index(drop=True, inplace=True)\n",
    "df_confident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confident.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the relevant images\n",
    "os.makedirs(os.path.join(download_dir, 'confident'), exist_ok=True)\n",
    "\n",
    "for i, row in df_confident.head(top_n).iterrows():\n",
    "    download_images(client, transfer_config, row['keys'], os.path.join(download_dir, 'confident'), region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_confident.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each unique image, annotate the bounding box\n",
    "fig, ax = plt.subplots(4, 5, figsize=(13.33, 7.5))\n",
    "ax = ax.ravel()\n",
    "\n",
    "df_confident = df_confident.sort_values(by='top_1_confidence', ascending=False)\n",
    "\n",
    "# for eah row in df_confident, get the image_path\n",
    "for i, row in df_confident.iterrows():\n",
    "    annotate_image2(\n",
    "        row['image_path'],\n",
    "        os.path.join(download_dir, 'confident'),\n",
    "        df_confident,\n",
    "        ax[i],\n",
    "        crop_to_highlight=row['crop_status'],\n",
    "        buffer=20,\n",
    "        scaling_required=False,\n",
    "        subtitle=f\"{row['dep']}\\n{row['top_1_species']}, {row['top_1_confidence']:.2f}\"\n",
    "    )\n",
    "\n",
    "for i in range(len(df_confident), len(ax)):\n",
    "    ax[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Largest Moths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_area = load_df_from_file(inference_csvs, sort_by='crop_area', ascending=False, n_keep=10, remove_none=True, group_by_col='order_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_area_backup = df_area.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First look at large moths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by moth size\n",
    "df_area = df_area_backup.sort_values(by='crop_area', ascending=False)\n",
    "df_area.reset_index(drop=True, inplace=True)\n",
    "df_area = df_area.drop_duplicates(subset=['top_1_species'])\n",
    "\n",
    "\n",
    "top_n=20\n",
    "\n",
    "df_area = df_area.head(top_n)\n",
    "\n",
    "df_area.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the relevant images\n",
    "os.makedirs(os.path.join(download_dir, 'largest'), exist_ok=True)\n",
    "\n",
    "for i, row in df_area.iterrows():\n",
    "    download_images(client, transfer_config, row['keys'], os.path.join(download_dir, 'largest'), region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each unique image, annotate the bounding box\n",
    "fig, ax = plt.subplots(4, 5, figsize=(13.33, 7.5))\n",
    "ax = ax.ravel()\n",
    "\n",
    "df_area = df_area.sort_values(by='crop_area', ascending=False)\n",
    "\n",
    "# for eah row in df_confident, get the image_path\n",
    "for i, row in df_area.iterrows():\n",
    "    annotate_image2(\n",
    "        row['image_path'],\n",
    "        os.path.join(download_dir, 'largest'),\n",
    "        df_area,\n",
    "        ax[i],\n",
    "        crop_to_highlight=row['crop_status'],\n",
    "        buffer=20,\n",
    "        scaling_required=False,\n",
    "        subtitle=f\"{row['top_1_species']}, ({row['top_1_confidence']:.2f})\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(13.33, 7.5))\n",
    "# ax = ax.ravel()\n",
    "\n",
    "df_area = df_area.sort_values(by='crop_area', ascending=False)\n",
    "\n",
    "# for eah row in df_confident, get the image_path\n",
    "for i, row in df_area.iloc[1:2, ].iterrows():\n",
    "    annotate_image2(\n",
    "        row['image_path'],\n",
    "        os.path.join(download_dir, 'largest'),\n",
    "        df_area,\n",
    "        ax,\n",
    "        crop_to_highlight=row['crop_status'],\n",
    "        buffer=20,\n",
    "        scaling_required=False,\n",
    "        subtitle=f\"{row['top_1_species']}, ({row['top_1_confidence']:.2f})\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Look at large non-moths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by moth size\n",
    "df_area = df_area_backup.sort_values(by='crop_area', ascending=False)\n",
    "df_area.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "df_area = df_area.loc[df_area['order_name'] != 'Lepidoptera Macros', ]\n",
    "df_area = df_area.loc[df_area['order_name'] != 'Lepidoptera Micros', ]\n",
    "df_area = df_area.loc[df_area['class_name'] != 'moth', ]\n",
    "\n",
    "top_n=20\n",
    "\n",
    "df_area = df_area.head(top_n)\n",
    "\n",
    "df_area.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the relevant images\n",
    "os.makedirs(os.path.join(download_dir, 'largest_nm'), exist_ok=True)\n",
    "\n",
    "for i, row in df_area.iterrows():\n",
    "    download_images(client, transfer_config, row['keys'], os.path.join(download_dir, 'largest_nm'), region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each unique image, annotate the bounding box\n",
    "fig, ax = plt.subplots(4, 5, figsize=(13.33, 7.5))\n",
    "ax = ax.ravel()\n",
    "\n",
    "df_area = df_area.sort_values(by='crop_area', ascending=False)\n",
    "\n",
    "# for eah row in df_confident, get the image_path\n",
    "for i, row in df_area.iterrows():\n",
    "    annotate_image2(\n",
    "        row['image_path'],\n",
    "        os.path.join(download_dir, 'largest_nm'),\n",
    "        df_area,\n",
    "        ax[i],\n",
    "        crop_to_highlight=row['crop_status'],\n",
    "        buffer=20,\n",
    "        scaling_required=False,\n",
    "        subtitle=f\"{row['top_1_species']}, ({row['top_1_confidence']:.2f})\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_order.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busiest Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from each inference_script subset to the most common 5 images\n",
    "cutoff=100\n",
    "all_paths = pd.DataFrame()\n",
    "\n",
    "for x in tqdm(inference_csvs):\n",
    "    try:\n",
    "        df = pd.read_csv(x, low_memory=False)\n",
    "    except Exception as e:\n",
    "        print(f\" - Error reading {x}: {e}\")\n",
    "        continue\n",
    "    df = df.loc[df['crop_status'] != 'no detections for image.', ]\n",
    "    df = df.drop_duplicates(subset=['x_min', 'x_max', 'y_min', 'y_max'])\n",
    "\n",
    "    # get the most popular image_path\n",
    "    paths = df['image_path'].value_counts()\n",
    "    paths = paths[paths > cutoff].index.tolist()\n",
    "\n",
    "    if len(paths) > 5:\n",
    "        paths = paths[:5]\n",
    "\n",
    "    df = df.loc[df['image_path'].isin(paths), ]\n",
    "    df['dep'] = os.path.basename(x).split('.')[0].split('_')[0]\n",
    "\n",
    "        # set new keys column as 'dep' and 'image_path' combined\n",
    "    df['keys'] = df['image_path'].apply(lambda x: f\"{df['dep'].iloc[0]}/snapshot_images/{os.path.basename(x)}\")\n",
    "\n",
    "    all_paths = pd.concat([all_paths, df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_paths.shape\n",
    "\n",
    "# subset to the 20 most common paths\n",
    "all_paths_df = all_paths.groupby('image_path').head(5).reset_index(drop=True)\n",
    "\n",
    "all_paths_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the relevant images\n",
    "os.makedirs(os.path.join(download_dir, 'busy'), exist_ok=True)\n",
    "\n",
    "for i, row in all_paths_df.iterrows():\n",
    "    download_images(client, transfer_config, row['keys'], os.path.join(download_dir, 'busy'), region)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blurriest Crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amber_inferences.utils.inference_scripts import download_image_from_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by moth size\n",
    "df_blur = df.sort_values(by='crop_bluriness', ascending=False)\n",
    "df_blur.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_blur = df_blur.drop_duplicates(subset=['top_1_species'])\n",
    "top_n=20\n",
    "\n",
    "df_blur = df_blur.head(top_n)\n",
    "\n",
    "df_blur.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir = '/home/users/katriona/amber-inferences/data/qc_plots/singapore'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the relevant images\n",
    "os.makedirs(os.path.join(download_dir, 'blur'), exist_ok=True)\n",
    "\n",
    "for i, row in df_blur.iterrows():\n",
    "    print(row['keys'])\n",
    "    download_image_from_key(client, row['keys'], region, os.path.join(download_dir, 'blur'))\n",
    "    # download_images(client, transfer_config, row['keys'], os.path.join(download_dir, 'blur'), region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each unique image, annotate the bounding box\n",
    "fig, ax = plt.subplots(4, 5, figsize=(13.33, 7.5))\n",
    "ax = ax.ravel()\n",
    "\n",
    "df_blur = df_blur.sort_values(by='crop_bluriness', ascending=False)\n",
    "\n",
    "# for eah row in df_blur, get the image_path\n",
    "for i, row in df_blur.iterrows():\n",
    "    annotate_image(\n",
    "        row['image_path'],\n",
    "        os.path.join(download_dir, 'blur'),\n",
    "        df_blur,\n",
    "        ax[i],\n",
    "        crop_to_highlight=row['crop_status'],\n",
    "        buffer=20,\n",
    "        scaling_required=False,\n",
    "        subtitle=f\"Crop blur: {float(row['crop_bluriness']):.3f}\\nImage blur: {float(row['image_bluriness']):.3f}\"\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bluriest Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eco = df.loc[df['dep'] == \"ECOLINK\", ]\n",
    "# df_eco = df.loc[df['image_bluriness'] < 50, ]\n",
    "\n",
    "df_eco = df_eco.drop_duplicates(subset=['image_path'])\n",
    "\n",
    "df_eco['date'] = pd.to_datetime(df_eco['image_datetime'])\n",
    "df_eco['date'] = df_eco['date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# plot date vs image_bluriness\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "sns.scatterplot(data=df_eco, x='date', y='image_bluriness', alpha=0.7)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Image Bluriness by Date for Ecolink Deployment')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each deployment print the average image bluriness\n",
    "dep_bluriness = df.drop_duplicates('image_path').groupby('dep')['image_bluriness'].mean().reset_index()\n",
    "\n",
    "print(dep_bluriness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(df_eco.loc['image_bluriness'] > 50).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the relevant images\n",
    "os.makedirs(os.path.join(download_dir, 'ecolink2'), exist_ok=True)\n",
    "\n",
    "for i, row in df_eco.loc[df_eco['image_bluriness'] < 50, ].sort_values('image_bluriness').head().iterrows():\n",
    "    print(row['keys'])\n",
    "    download_image_from_key(client, row['keys'], region, os.path.join(download_dir, 'ecolink2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots for each deployment, plot crop_blurriness vs top_1_confidence\n",
    "fig, ax = plt.subplots(2, 5, figsize=(10, 5), sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i, dep in enumerate(df['dep'].unique()):\n",
    "    dep_df = df.loc[df['dep'] == dep, ]\n",
    "    sns.scatterplot(data=dep_df, x='crop_bluriness', y='top_1_confidence', ax=ax[i], alpha=0.5)\n",
    "    ax[i].set_title(dep)\n",
    "    ax[i].set_xlabel('Crop Blur')\n",
    "    ax[i].set_ylabel('Top 1 Confidence')\n",
    "plt.suptitle('Crop Blur vs Top 1 Confidence by Deployment')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/crop_blur_vs_top_1_confidence_by_deployment.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subplots by deployment, plot a histogram of image blurriness\n",
    "fig, ax = plt.subplots(2, 5, figsize=(10, 5), sharex=True, sharey=True)\n",
    "ax = ax.flatten()\n",
    "\n",
    "for i, dep in enumerate(df['dep'].unique()):\n",
    "    dep_df_sub = df.loc[df['dep'] == dep, ]\n",
    "    # drop duplicated image\n",
    "    dep_df_sub = dep_df_sub.drop_duplicates(subset=['image_path'])\n",
    "    sns.histplot(data=dep_df_sub, x='image_bluriness', ax=ax[i], bins=50, kde=True)\n",
    "    ax[i].set_title(dep)\n",
    "    ax[i].set_xlabel('Image Blur')\n",
    "    ax[i].set_ylabel('Count')\n",
    "    # ax[i].set_xlim(0, 1)\n",
    "plt.suptitle('Image Blur Distribution by Deployment')\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{plot_dir}/image_blur_distribution_by_deployment.png\", bbox_inches='tight', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order by moth size\n",
    "df = df.astype({'image_bluriness': 'float'})\n",
    "df_blur = df.sort_values(by='image_bluriness', ascending=False)\n",
    "\n",
    "df_blur.reset_index(drop=True, inplace=True)\n",
    "\n",
    "df_blur = df_blur.drop_duplicates(subset=['top_1_species'])\n",
    "df_blur = df_blur.drop_duplicates(subset=['image_bluriness'])\n",
    "top_n=20\n",
    "\n",
    "df_blur = df_blur.head(top_n)\n",
    "\n",
    "df_blur.reset_index(drop=True, inplace=True)\n",
    "df_blur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the relevant images\n",
    "os.makedirs(os.path.join(download_dir, 'blur'), exist_ok=True)\n",
    "\n",
    "for i, row in df_blur.iterrows():\n",
    "    download_images(client, transfer_config, row['keys'], os.path.join(download_dir, 'blur'), region)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each unique image, annotate the bounding box\n",
    "fig, ax = plt.subplots(4, 5, figsize=(13.33, 7.5))\n",
    "ax = ax.ravel()\n",
    "\n",
    "df_blur = df_blur.sort_values(by='crop_bluriness', ascending=False)\n",
    "\n",
    "# for eah row in df_blur, get the image_path\n",
    "for i, row in df_blur.iterrows():\n",
    "    annotate_image(\n",
    "        row['image_path'],\n",
    "        os.path.join(download_dir, 'blur'),\n",
    "        df_blur,\n",
    "        ax[i],\n",
    "        crop_to_highlight=row['crop_status'],\n",
    "        buffer=20,\n",
    "        scaling_required=False,\n",
    "        subtitle=f\"Crop blur: {float(row['crop_bluriness']):.3f}\\nImage blur: {float(row['image_bluriness']):.3f}\"\n",
    "\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "json_file = '/home/users/katriona/amber-inferences/keys/thailand_final/dep000074.json'\n",
    "\n",
    "with open(json_file, \"r\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "\n",
    "# create a dataframe from the json file as filename column\n",
    "df_json = pd.DataFrame(chunks, columns=['filename'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json['datetime'] = df_json['filename'].apply(lambda x: x.split('/')[2].replace('-snapshot.jpg', '') )\n",
    "df_json['datetime'] = pd.to_datetime(df_json['datetime'], format='%Y%m%d%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the time is < 12, add 24 hours to the date\n",
    "df_json['session'] = df_json['datetime']\n",
    "df_json['session'] = df_json['datetime'].apply(lambda x: x - pd.Timedelta(days=1) if x.hour < 12 else x)\n",
    "df_json['session'] = df_json['session'].dt.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(df_json['session'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a dict grouped by session\n",
    "sessions = df_json.groupby('session')['filename'].apply(list).to_dict()\n",
    "\n",
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions['2024-07-24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file = '/gws/nopw/j04/ceh_generic/kgoldmann/costarica_inferences_tracking/dep000031/dep000031_2024-06-03.csv'\n",
    "df = pd.read_csv(csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df = track_id_calc(df, cost_threshold=1)\n",
    "print(f\"Number of tracks for {os.path.basename(csv_file)}: {track_df['track_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image_path_basename'] = df['image_path'].apply(lambda x: os.path.basename(x))\n",
    "\n",
    "df = df.merge(track_df, how='left', left_on=['image_path_basename', 'crop_status'], right_on=['image_path', 'crop_id'])\n",
    "df = df.reset_index(drop=True)\n",
    "df = df.loc[:, ~df.columns.str.contains('_y')]\n",
    "df = df.rename(columns=lambda x: x.replace('_x', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('.test.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['track_id'].value_counts(dropna=False))\n",
    "df['track_id'].value_counts().plot(kind='hist', bins=100, figsize=(5, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "\n",
    "df = df.loc[df['crop_status'] != 'No detections for this image.', ]\n",
    "\n",
    "# drop nan\n",
    "df = df.loc[df['best_match_crop'].notna(), ]\n",
    "df = df.loc[df['total_cost'].notna(), ]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['best_match_crop'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#histogram of total cost\n",
    "df['total_cost'] = df['total_cost'].astype(float)\n",
    "df['total_cost'].plot(kind='hist', bins=50, figsize=(10, 5))\n",
    "plt.title('Total Cost Distribution')\n",
    "plt.xlabel('Total Cost')\n",
    "plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_id_calc(best_matches, cost_threshold=1):\n",
    "        # Make a copy and rename the relevant columns\n",
    "    best_matches = best_matches.copy()\n",
    "\n",
    "    best_matches = best_matches.loc[best_matches['total_cost'] != \"\", ]\n",
    "    best_matches = best_matches.loc[best_matches['best_match_crop'].notna(), ]\n",
    "    best_matches = best_matches.loc[best_matches['total_cost'].notna(), ]\n",
    "    best_matches['image1'] = [os.path.basename(x) for x in best_matches['image_path']]\n",
    "    best_matches['image2'] = best_matches['previous_image']\n",
    "    best_matches['crop1'] = best_matches['crop_status']\n",
    "    best_matches['crop2'] = best_matches['best_match_crop']\n",
    "\n",
    "    # Filter based on the cost threshold\n",
    "    filtered_matches = best_matches[best_matches[\"total_cost\"] < cost_threshold]\n",
    "\n",
    "    def node_id(image_path, crop_id):\n",
    "        return f\"{image_path}|{crop_id}\"\n",
    "\n",
    "    # Build a graph of matches under the threshold\n",
    "    G_thresh = nx.Graph()\n",
    "    for _, row in filtered_matches.iterrows():\n",
    "        n1 = node_id(row['image1'], row['crop1'])\n",
    "        n2 = node_id(row['image2'], row['crop2'])\n",
    "        G_thresh.add_edge(n1, n2)\n",
    "\n",
    "    # Assign track IDs from connected components\n",
    "    track_mapping = {}\n",
    "    for tid, component in enumerate(nx.connected_components(G_thresh)):\n",
    "        for node in component:\n",
    "            track_mapping[node] = f\"Track_{str(tid).rjust(5, '0')}\"\n",
    "\n",
    "    # Collect all unique nodes from both sides\n",
    "    all_nodes = set()\n",
    "    for _, row in best_matches.iterrows():\n",
    "        all_nodes.add(node_id(row['image1'], row['crop1']))\n",
    "        all_nodes.add(node_id(row['image2'], row['crop2']))\n",
    "\n",
    "    # Create lookup for cost (minimum per crop node)\n",
    "    cost_lookup = {}\n",
    "    for _, row in best_matches.iterrows():\n",
    "        for prefix in [\"1\", \"2\"]:\n",
    "            nid = node_id(row[f\"image{prefix}\"], row[f\"crop{prefix}\"])\n",
    "            cost = row[\"total_cost\"]\n",
    "            cost_lookup[nid] = min(cost_lookup.get(nid, float(\"inf\")), cost)\n",
    "\n",
    "    # Assemble the final output rows\n",
    "    output_rows = []\n",
    "    for node in all_nodes:\n",
    "        image_path, crop_id = node.rsplit(\"|\", 2)\n",
    "        output_rows.append({\n",
    "            \"image_path\": image_path,\n",
    "            \"crop_id\": crop_id,\n",
    "            \"track_id\": track_mapping.get(node),  # May be None if unmatched under threshold\n",
    "            \"total_cost\": cost_lookup.get(node)\n",
    "        })\n",
    "\n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "\n",
    "    # Assign unique track IDs to unmatched crops\n",
    "    max_existing_id = (\n",
    "        max(\n",
    "            [int(tid.replace(\"Track_\", \"\")) for tid in output_df[\"track_id\"].dropna()],\n",
    "            default=-1\n",
    "        )\n",
    "    )\n",
    "    unmatched_mask = output_df[\"track_id\"].isnull()\n",
    "    unmatched_indices = output_df[unmatched_mask].index\n",
    "    new_ids = [\n",
    "        f\"Track_{str(i).rjust(5, '0')}\"\n",
    "        for i in range(max_existing_id + 1, max_existing_id + 1 + unmatched_mask.sum())\n",
    "    ]\n",
    "    output_df.loc[unmatched_indices, \"track_id\"] = new_ids\n",
    "\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from itertools import product\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df = track_id_calc(df, cost_threshold=1)\n",
    "output_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a colour label\n",
    "# Generate N unique colors for each track\n",
    "num_tracks = output_df['track_id'].nunique()\n",
    "\n",
    "# Use a colormap to get visually distinct colors\n",
    "col_palette = 'tab20'  # You can choose any colormap from matplotlib\n",
    "cmap = plt.cm.get_cmap(col_palette, num_tracks)\n",
    "\n",
    "# Map track_id to hex colors\n",
    "track_id_to_color = {\n",
    "    track_id: mcolors.to_hex(cmap(i)) for i, track_id in enumerate(sorted(output_df['track_id'].unique()))\n",
    "}\n",
    "\n",
    "# Add color column to DataFrame\n",
    "output_df['colour'] = output_df['track_id'].map(track_id_to_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_df['track_id'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['image_path_basename'] = df['image_path'].apply(lambda x: os.path.basename(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(output_df, how='left', left_on=['image_path_basename', 'crop_status'], right_on=['image_path', 'crop_id'])\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns[df.columns.str.contains('_y')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove columns with '_y' suffix\n",
    "df = df.loc[:, ~df.columns.str.contains('_y')]\n",
    "# rename columns with '_x' suffix to remove it\n",
    "df = df.rename(columns=lambda x: x.replace('_x', ''))\n",
    "\n",
    "# drop the colour column\n",
    "df = df.drop(columns=['colour'])\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the df\n",
    "output_file = './dep000031_2024-06-03_tracked.csv'\n",
    "df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the number of snapshots per crop:\n",
    "pd.DataFrame(df['track_id'].value_counts())['count'].value_counts().plot(kind='bar', figsize=(10, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['track_id'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to the 3 most common track IDs\n",
    "top_tracks = df['track_id'].value_counts().head(3).index.tolist()\n",
    "df = df.loc[df['track_id'] == 'Track_00119', ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['track_id'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = 'dep000031/snapshot_images/' + df['image_path_basename'].unique()\n",
    "print(len(keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(keys)):\n",
    "    try:\n",
    "        download_images(client, transfer_config, keys[i], '/gws/nopw/j04/ceh_generic/kgoldmann/test', 'cri',)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {keys[i]}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub = df.loc[df['image_path_basename'].isin(os.listdir('/gws/nopw/j04/ceh_generic/kgoldmann/test')), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unique track_ids\n",
    "frame_cutoff = 1\n",
    "\n",
    "track_counts = df_sub[\"track_id\"].value_counts()\n",
    "valid_tracks = track_counts[track_counts > frame_cutoff].index\n",
    "df_sub = df_sub[df_sub[\"track_id\"].isin(valid_tracks)].reset_index(drop=True)\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amber_inferences.utils.plotting import *\n",
    "from amber_inferences.utils.tracking import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub['image_path_basename']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "\n",
    "for i, image_path in enumerate(df_sub['image_path_basename'].unique()):\n",
    "    image_path_full = f'/gws/nopw/j04/ceh_generic/kgoldmann/test/{image_path}'\n",
    "    imge = Image.open(image_path_full).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = df_sub.loc[df_sub['image_path_basename'] == os.path.basename(image_path_full), ]\n",
    "    boxes = []\n",
    "    if crops_df.shape[0] > 0:\n",
    "        for j, row in crops_df.iterrows():\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': row['track_id'],\n",
    "                'ann_col': 'black' #row['colour']\n",
    "            })\n",
    "\n",
    "    im = image_annotation(image_path_full, boxes=boxes, scale=False)\n",
    "    out_path = f'/gws/nopw/j04/ceh_generic/kgoldmann/ann_test/{os.path.basename(image_path_full)}'\n",
    "    im.save(out_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 + Jaspy",
   "language": "python",
   "name": "jaspy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
