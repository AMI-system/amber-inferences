{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "\n",
    "This tutorial runs you through the process of running inferences for a deployments in Costa Rica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wd\n",
    "os.chdir(os.path.expanduser('~/amber-inferences'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package if required\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "\n",
    "# import boto3\n",
    "# import pandas as pd\n",
    "# from tqdm import tqdm\n",
    "\n",
    "from amber_inferences.utils.config import load_credentials\n",
    "from amber_inferences.utils.api_utils import deployments_summary, get_deployments\n",
    "from amber_inferences.utils.custom_models import *\n",
    "from amber_inferences.utils.inference_scripts import *\n",
    "from amber_inferences.utils.plotting import *\n",
    "from amber_inferences.utils.tracking import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "\n",
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data on the Object Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance for the object store\n",
    "aws_credentials = load_credentials('./credentials.json')\n",
    "session = boto3.Session(\n",
    "        aws_access_key_id=aws_credentials[\"AWS_ACCESS_KEY_ID\"],\n",
    "        aws_secret_access_key=aws_credentials[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        region_name=aws_credentials[\"AWS_REGION\"],\n",
    "    )\n",
    "s3_client = session.client(\"s3\", endpoint_url=aws_credentials[\"AWS_URL_ENDPOINT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the deployments available on the object store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deployments = get_deployments(aws_credentials['UKCEH_username'], aws_credentials['UKCEH_password'])\n",
    "all_deployments = pd.DataFrame(all_deployments)\n",
    "all_deployments[all_deployments['status'] == 'active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All countries:\")\n",
    "for x in all_deployments['country'].unique():\n",
    "    print(f\"- {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick one, cri (Costa Rica) and check out the data attached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_deployments = deployments_summary(\n",
    "    aws_credentials,\n",
    "    subset_countries=[\"Costa Rica\"],\n",
    "    subset_deployments=[\"dep000035\", \"dep000036\"],\n",
    "    include_image_count=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the files for a given deployment(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log the image keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amber_inferences.utils.key_utils import save_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes some time to commenting out to save time\n",
    "# save_keys(\n",
    "#     s3_client,\n",
    "#     bucket=\"cri\",\n",
    "#     deployment_id=\"dep000035\",\n",
    "#     output_file=\"./examples/example_keys/dep000035_keys.json\",\n",
    "#     subdir=\"snapshot_images\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the keys\n",
    "!head ./examples/example_keys/interesting_timelapse.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and View the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from amber_inferences.utils.inference_scripts import download_image_from_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the first image in the keys file and open\n",
    "with open('./examples/example_keys/interesting_timelapse.json') as f:\n",
    "    keys = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./examples/images/dep000035/interesting_timelapse/raw/', exist_ok=True)\n",
    "\n",
    "for i in range(len(keys)):\n",
    "    download_image_from_key(s3_client, keys[i], 'cri', './examples/images/dep000035/interesting_timelapse/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/raw/')\n",
    "image_paths = [os.path.join('./examples/images/dep000035/interesting_timelapse/raw/', x) for x in image_paths]\n",
    "\n",
    "image_paths = [x for x in image_paths if x.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "os.makedirs('./examples/images/dep000035/interesting_timelapse/gifs', exist_ok=True)\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/raw_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Object Detection on the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the torch device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_load = load_models(\n",
    "    device=device,\n",
    "    localisation_model_path='./models/v1_localizmodel_2021-08-17-12-06.pt',\n",
    "    binary_model_path='./models/moth-nonmoth-effv2b3_20220506_061527_30.pth',\n",
    "    order_model_path='./models/dhc_best_128.pth',\n",
    "    order_threshold_path='./models/thresholdsTestTrain.csv',\n",
    "    species_model_path='./models/turing-costarica_v03_resnet50_2024-06-04-16-17_state.pt',\n",
    "    species_labels='./models/03_costarica_data_category_map.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops_loc = []\n",
    "\n",
    "os.makedirs('./examples/images/crops/interesting_timelapse/', exist_ok=True)\n",
    "\n",
    "for i, img_path in enumerate(tqdm(image_paths)):\n",
    "    crops = crop_image_only(\n",
    "        image_path=img_path,\n",
    "        bucket_name=\"cri\",\n",
    "        localisation_model=models_load['localisation_model'],\n",
    "        proc_device=device,\n",
    "        csv_file=\"./examples/interesting_timelapse_crops.csv\",\n",
    "        save_crops=True,\n",
    "        box_threshold=0.95,\n",
    "        crop_dir=\"./examples/images/crops/interesting_timelapse\",\n",
    "        job_name=None,\n",
    "    )\n",
    "\n",
    "    boxes = []\n",
    "    all_crops_loc.append(crops)\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    for j, row in crops.iterrows():\n",
    "        boxes.append({\n",
    "            'x_min': row['x_min'],\n",
    "            'y_min': row['y_min'],\n",
    "            'x_max': row['x_max'],\n",
    "            'y_max': row['y_max'],\n",
    "            'label': row['crop_status'],\n",
    "            'ann_col': 'grey'\n",
    "        })\n",
    "\n",
    "        # Crop original image and extract embedding\n",
    "        crop = image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "\n",
    "    del crops\n",
    "    img = image_annotation(img_path, boxes=boxes)\n",
    "\n",
    "    # save the image\n",
    "    img.save(f'{output_dir}/{os.path.basename(img_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops_loc = pd.concat(all_crops_loc)\n",
    "all_crops_loc = all_crops_loc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/object_detection_images.gif\"\n",
    "\n",
    "gif_creater(output_dir, gif_path)\n",
    "\n",
    "# show the gif\n",
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatbug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ This section is only advised if cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load_models(\n",
    "    device=device,\n",
    "    localisation_model_path='./models/flat_bug_M.pt',\n",
    "    binary_model_path='./models/moth-nonmoth-effv2b3_20220506_061527_30.pth',\n",
    "    order_model_path='./models/dhc_best_128.pth',\n",
    "    order_threshold_path='./models/thresholdsTestTrain.csv',\n",
    "    species_model_path='./models/turing-costarica_v03_resnet50_2024-06-04-16-17_state.pt',\n",
    "    species_labels='./models/03_costarica_data_category_map.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_flatbug/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    files = os.listdir(output_dir)\n",
    "    if len(files) > 0:\n",
    "        for f in files:\n",
    "            os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    os.makedirs('./examples/images/crops/interesting_timelapse_flatbug/', exist_ok=True)\n",
    "\n",
    "    all_crops_flatbug = []\n",
    "\n",
    "    for i, img_path in enumerate(tqdm(image_paths)):\n",
    "        crops = crop_image_only(\n",
    "            image_path=img_path,\n",
    "            bucket_name=\"cri\",\n",
    "            localisation_model=models['localisation_model'],\n",
    "            proc_device=device,\n",
    "            csv_file=\"./examples/interesting_timelapse_flatbug.csv\",\n",
    "            save_crops=True,\n",
    "            box_threshold=0,\n",
    "            crop_dir=\"./examples/images/crops/interesting_timelapse_flatbug\",\n",
    "            job_name=None,\n",
    "        )\n",
    "        crops = crops.loc[crops['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "        all_crops = all_crops + [crops]\n",
    "        if crops.shape[0] > 0:\n",
    "            boxes = []\n",
    "            for j, row in crops.iterrows():\n",
    "                boxes.append({\n",
    "                    'x_min': row['x_min'],\n",
    "                    'y_min': row['y_min'],\n",
    "                    'x_max': row['x_max'],\n",
    "                    'y_max': row['y_max'],\n",
    "                    'label': '',\n",
    "                    'ann_col': 'grey'\n",
    "                })\n",
    "            del crops\n",
    "            img = image_annotation(img_path, boxes=boxes, scale=False)\n",
    "\n",
    "            # save the image\n",
    "            img.save(f'{output_dir}/{os.path.basename(img_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Open images and convert to a sequence\n",
    "    image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/annotated_boxes_flatbug/')\n",
    "    image_paths = [os.path.join(output_dir, x) for x in image_paths]\n",
    "    images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "    # Save as GIF\n",
    "    gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/flatbug_detection_images.gif\"\n",
    "    images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "    # display(Image.open(gif_path))\n",
    "    del images\n",
    "\n",
    "    all_crops_flatbug = pd.concat(all_crops_flatbug)\n",
    "    all_crops_flatbug = all_crops_flatbug.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objects for Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which crops to use (localisation or flatbug)\n",
    "all_crops = all_crops_loc # or all_crops_flatbug\n",
    "\n",
    "transform_species = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((300, 300)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "image_paths_raw = image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/raw/')\n",
    "image_paths_raw = [os.path.abspath(os.path.join('./examples/images/dep000035/interesting_timelapse/raw/', x)) for x in image_paths_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_binary'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image_path in enumerate(tqdm(image_paths_raw)):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops.loc[all_crops['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            ann_col='red'\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(device)\n",
    "            binary_prediction = classify_box(cropped_tensor, models_load['classification_model'])\n",
    "            if binary_prediction[0] == 'moth':\n",
    "                ann_col='green'\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': binary_prediction[0],\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'{output_dir}/{os.path.basename(image_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/binary_images.gif\"\n",
    "\n",
    "gif_creater(output_dir, gif_path)\n",
    "\n",
    "# show the gif\n",
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_order/'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))\n",
    "\n",
    "imgs = []\n",
    "for i, image_path in enumerate(tqdm(image_paths_raw)):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops.loc[all_crops['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            ann_col = 'red'\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(device)\n",
    "            order_prediction = classify_order(\n",
    "                cropped_tensor,\n",
    "                models_load['order_model'],\n",
    "                models_load['order_model_labels'],\n",
    "                models_load['order_model_thresholds']\n",
    "            )\n",
    "            if 'Lepidoptera' in order_prediction[0]:\n",
    "                ann_col = 'green'\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': order_prediction[0],\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'{output_dir}/{os.path.basename(image_path)}')\n",
    "\n",
    "        imgs = imgs + [im]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "image_paths = os.listdir(output_dir)\n",
    "image_paths = [os.path.join(output_dir, x) for x in image_paths]\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/order_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![orderGif](\" + os.path.abspath(gif_path) + \" 'order')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Species Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_species/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))\n",
    "\n",
    "imgs = []\n",
    "for i, image_path in enumerate(tqdm(image_paths_raw)):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops.loc[all_crops['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(device)\n",
    "            order_prediction = classify_order(\n",
    "                cropped_tensor,\n",
    "                models_load['order_model'],\n",
    "                models_load['order_model_labels'],\n",
    "                models_load['order_model_thresholds']\n",
    "            )\n",
    "            label = \"\"\n",
    "            ann_col = 'red'\n",
    "\n",
    "            if 'Lepidoptera' in order_prediction[0]:\n",
    "                species_names, species_confidences = classify_species(\n",
    "                    cropped_tensor,\n",
    "                    models_load['species_model'],\n",
    "                    models_load['species_model_labels'],\n",
    "                    5\n",
    "                )\n",
    "                label = f\"{species_names[0]}, {'{:.2f}'.format(species_confidences[0]*100)}%\"\n",
    "                ann_col='green'\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': label,\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'{output_dir}/{os.path.basename(image_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "image_paths = os.listdir(output_dir)\n",
    "image_paths = [os.path.join(output_dir, x) for x in image_paths]\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/species_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![speciesGif](\" + os.path.abspath(gif_path) + \" 'species')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to be able to track individual insects across frames. This is done by using the tracking model. The tracking model takes in a list of detections and returns a list of tracks. Each track is a list of detections that belong to the same insect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_tracking/'\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of embeddings\n",
    "embeddings_list = {}\n",
    "\n",
    "for i in all_crops['image_path'].unique():\n",
    "    embeddings_list[i] = {}\n",
    "\n",
    "for i, crops in tqdm(all_crops.iterrows()):\n",
    "    img_path = crops['image_path']\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    cropped_image = image.crop((crops['x_min'], crops['y_min'], crops['x_max'], crops['y_max']))\n",
    "\n",
    "    embedding = extract_embedding(cropped_image, models_load['species_model'], device=device)\n",
    "    embeddings_list[img_path][crops['crop_status']] = {\n",
    "        'embedding': embedding,\n",
    "        'file': os.path.basename(crops['image_path']),\n",
    "        'crop': crops['crop_status'],\n",
    "        'crop_image': cropped_image,\n",
    "        'box': {'xmin':crops['x_min'], 'ymin':crops['y_min'], 'xmax':crops['x_max'], 'ymax':crops['y_max']}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amber_inferences.utils.tracking import *\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each image, get the combinations between those crops and those of the next image\n",
    "all_crop_pairs = []\n",
    "image_paths = list(embeddings_list.keys())\n",
    "\n",
    "for i in range(len(image_paths) - 1):\n",
    "    img1 = image_paths[i]\n",
    "    img2 = image_paths[i + 1]\n",
    "\n",
    "    crops1 = embeddings_list[img1]\n",
    "    crops2 = embeddings_list[img2]\n",
    "\n",
    "    for c1, c2 in product(crops1, crops2):\n",
    "        all_crop_pairs.append((img1, c1, img2, c2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the similarity between crops in subsequent images\n",
    "results = []\n",
    "\n",
    "for image_a, crop_a, image_b, crop_b in tqdm(all_crop_pairs):\n",
    "    c_a = embeddings_list[image_a][crop_a]\n",
    "    c_a['image_path'] = image_a\n",
    "    c_b = embeddings_list[image_b][crop_b]\n",
    "    c_b['image_path'] = image_b\n",
    "\n",
    "    res = calculate_cost(c_a, c_b)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"image_path1\",\n",
    "    \"crop1_id\",\n",
    "    \"image_path2\",\n",
    "    \"crop2_id\",\n",
    "    \"cnn_cost\",\n",
    "    \"iou_cost\",\n",
    "    \"box_ratio_cost\",\n",
    "    \"dist_ratio_cost\",\n",
    "    \"total_cost\"\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(results).reset_index(drop=True)\n",
    "results_df.columns = columns\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of the costs\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(results_df['total_cost'], bins=50, color='blue', alpha=0.7)\n",
    "plt.xlabel('Total Cost')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Total Costs')\n",
    "\n",
    "plt.annotate('<- increasing similarity', xy=(0, 0.9), xycoords='axes fraction',\n",
    "             fontsize=8, color='black', ha='left')\n",
    "plt.annotate('decreasing similarity ->', xy=(1, 0.9), xycoords='axes fraction',\n",
    "fontsize=8, color='black', ha='right')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # plot the crops for the highest cost\n",
    "# df_subset = results_df.loc[results_df['total_cost'] < 1,]\n",
    "# df_subset = df_subset.reset_index(drop=True)\n",
    "\n",
    "# fig, axs = plt.subplots(df_subset.shape[0], 2, figsize=(6, 26))\n",
    "# axs = axs.flatten()\n",
    "\n",
    "# for i, row in df_subset.iterrows():\n",
    "\n",
    "#     crop1 = row['crop1_id']\n",
    "#     crop2 = row['crop2_id']\n",
    "\n",
    "#     crop1 = df.loc[(df['crop'] == crop1), ]\n",
    "#     crop1 = crop1.loc[crop1['image_path'] == row['image_path1'],]\n",
    "#     crop2 = df.loc[df['crop'] == crop2,]\n",
    "#     crop2 = crop2.loc[crop2['image_path'] == row['image_path2'],]\n",
    "\n",
    "#     # plot the crops\n",
    "#     axs[i*2].imshow(crop1['cropped_image'].values[0])\n",
    "#     axs[i*2].set_title(f\"Image 1: {crop1['crop'].values[0]}\")\n",
    "#     axs[i*2].axis('off')\n",
    "#     axs[i*2+1].imshow(crop2['cropped_image'].values[0])\n",
    "#     axs[i*2+1].set_title(f\"Image 2: {crop2['crop'].values[0]}\")\n",
    "#     axs[i*2+1].axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the tracks. A track is the series of crops we consider to belong to one individual. This is based on a cost threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Filter low-cost matches\n",
    "# filtered_df = results_df.copy()\n",
    "\n",
    "# # Keep only best match for each (image_path1, crop1_id)\n",
    "# filtered_df.sort_values(\"total_cost\", inplace=True)\n",
    "# filtered_df = filtered_df.drop_duplicates(subset=[\"image_path1\", \"crop1_id\"], keep=\"first\")\n",
    "\n",
    "# # Create a graph of matched crops\n",
    "# G = nx.Graph()\n",
    "\n",
    "# # Each node is a unique crop: image_path|crop_id\n",
    "# def node_id(row, prefix):\n",
    "#     return f\"{row[f'image_path{prefix}']}|{row[f'crop{prefix}_id']}\"\n",
    "\n",
    "# for _, row in filtered_df.iterrows():\n",
    "#     n1 = node_id(row, \"1\")\n",
    "#     n2 = node_id(row, \"2\")\n",
    "#     G.add_edge(n1, n2)\n",
    "\n",
    "# # Connected components become track IDs\n",
    "# track_mapping = {}\n",
    "# for track_id, component in enumerate(nx.connected_components(G)):\n",
    "#     for node in component:\n",
    "#         track_mapping[node] = track_id\n",
    "\n",
    "\n",
    "# track_mapping\n",
    "\n",
    "# # Extract nodes back into image_path and crop_id\n",
    "# output_rows = []\n",
    "# for node, track_id in track_mapping.items():\n",
    "#     image_path, crop_id = node.rsplit(\"|\", 1)\n",
    "#     output_rows.append({\n",
    "#         \"image_path\": image_path,\n",
    "#         \"crop_id\": crop_id,\n",
    "#         \"track_id\": f\"Track_{str(track_id).rjust(9, '0')}\"\n",
    "#     })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "\n",
    "# Filter low-cost matches\n",
    "filtered_df = results_df.copy()\n",
    "\n",
    "# Keep only best match for each (image_path1, crop1_id)\n",
    "filtered_df.sort_values(\"total_cost\", inplace=True)\n",
    "filtered_df = filtered_df.drop_duplicates(subset=[\"image_path1\", \"crop1_id\"], keep=\"first\")\n",
    "\n",
    "# Helper function for node naming\n",
    "def node_id(row, prefix):\n",
    "    return f\"{row[f'image_path{prefix}']}|{row[f'crop{prefix}_id']}\"\n",
    "\n",
    "# Sort for best matches\n",
    "results_df_sorted = results_df.sort_values(\"total_cost\")\n",
    "\n",
    "# Keep best match per crop1 (base tracking logic)\n",
    "best_matches = results_df_sorted.drop_duplicates(subset=[\"image_path1\", \"crop1_id\"], keep=\"first\")\n",
    "\n",
    "# --- Base Track Graph ---\n",
    "G_base = nx.Graph()\n",
    "for _, row in best_matches.iterrows():\n",
    "    n1 = node_id(row, \"1\")\n",
    "    n2 = node_id(row, \"2\")\n",
    "    G_base.add_edge(n1, n2)\n",
    "\n",
    "# Assign base track IDs\n",
    "base_track_mapping = {}\n",
    "for base_id, component in enumerate(nx.connected_components(G_base)):\n",
    "    for node in component:\n",
    "        base_track_mapping[node] = f\"BaseTrack_{str(base_id).rjust(5, '0')}\"\n",
    "\n",
    "# Thresholded Track Graph\n",
    "cost_threshold = 1.0\n",
    "filtered_matches = best_matches[best_matches[\"total_cost\"] < cost_threshold]\n",
    "\n",
    "G_thresh = nx.Graph()\n",
    "for _, row in filtered_matches.iterrows():\n",
    "    n1 = node_id(row, \"1\")\n",
    "    n2 = node_id(row, \"2\")\n",
    "    G_thresh.add_edge(n1, n2)\n",
    "\n",
    "# Assign track IDs\n",
    "track_mapping = {}\n",
    "for tid, component in enumerate(nx.connected_components(G_thresh)):\n",
    "    for node in component:\n",
    "        track_mapping[node] = f\"Track_{str(tid).rjust(5, '0')}\"\n",
    "\n",
    "# Collect all unique nodes\n",
    "all_nodes = set(base_track_mapping.keys()).union(track_mapping.keys())\n",
    "\n",
    "# Create lookup for cost (minimum for each crop)\n",
    "cost_lookup = {}\n",
    "for _, row in best_matches.iterrows():\n",
    "    for prefix in [\"1\", \"2\"]:\n",
    "        nid = node_id(row, prefix)\n",
    "        cost = row[\"total_cost\"]\n",
    "        cost_lookup[nid] = min(cost_lookup.get(nid, float(\"inf\")), cost)\n",
    "\n",
    "# Final Output\n",
    "output_rows = []\n",
    "for node in all_nodes:\n",
    "    image_path, crop_id = node.rsplit(\"|\", 1)\n",
    "    output_rows.append({\n",
    "        \"image_path\": image_path,\n",
    "        \"crop_id\": crop_id,\n",
    "        \"base_track_id\": base_track_mapping.get(node),\n",
    "        \"track_id\": track_mapping.get(node),  # May be None if not matched under threshold\n",
    "        \"total_cost\": cost_lookup.get(node)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_track = int(sorted([x for x in tracks_df['track_id'].unique() if x != None])[-1].replace('Track_', ''))\n",
    "tracks_df = pd.DataFrame(output_rows)\n",
    "non_vals = tracks_df['track_id'][tracks_df['track_id'].isnull()]\n",
    "non_ids = range(max_track+1, len(non_vals)+max_track+1)\n",
    "non_ids = [f\"Track_{str(x).rjust(5, '0')}\" for x in non_ids]\n",
    "tracks_df.loc[tracks_df['track_id'].isnull(), 'track_id'] = non_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "\n",
    "# # drop unique track\n",
    "# plot_df = tracks_df.loc[tracks_df['track_id'].duplicated(keep=False),]\n",
    "# plot_df = plot_df.reset_index(drop=True)\n",
    "\n",
    "# fig, axs = plt.subplots(plot_df['track_id'].nunique(),\n",
    "#                         tracks_df['track_id'].value_counts().max(),\n",
    "#                         figsize=(6, 12))\n",
    "\n",
    "# for i, track_id in enumerate(plot_df['track_id'].unique()):\n",
    "#     sub_track_df = plot_df.loc[plot_df['track_id'] == track_id,]\n",
    "#     for j, row in enumerate(sub_track_df.iterrows()):\n",
    "#         image_path = row[1]['image_path']\n",
    "#         crop_id = row[1]['crop_id']\n",
    "\n",
    "#         cropped_image = embeddings_list[image_path][crop_id]['crop_image']\n",
    "\n",
    "#         # get the image\n",
    "#         axs[i, j].imshow(cropped_image)\n",
    "#         # axs[i, j].set_title(f\"Track: {track_id}\")\n",
    "#         axs[i, j].axis('off')\n",
    "\n",
    "#     # turn of axis for the rest of the images\n",
    "#     for k in range(j+1, axs.shape[1]):\n",
    "#         axs[i, k].axis('off')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Generate N unique colors for each track\n",
    "num_tracks = tracks_df['track_id'].nunique()\n",
    "\n",
    "# Use a colormap to get visually distinct colors\n",
    "cmap = plt.cm.get_cmap('hsv', num_tracks)  # tab20\n",
    "\n",
    "# Map track_id to hex colors\n",
    "track_id_to_color = {\n",
    "    track_id: mcolors.to_hex(cmap(i)) for i, track_id in enumerate(sorted(tracks_df['track_id'].unique()))\n",
    "}\n",
    "\n",
    "# Add color column to DataFrame\n",
    "tracks_df['colour'] = tracks_df['track_id'].map(track_id_to_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops_merge = all_crops.merge(tracks_df, how='left', left_on=['image_path', 'crop_status'], right_on=['image_path', 'crop_id'])\n",
    "all_crops_merge = all_crops_merge.reset_index(drop=True)\n",
    "all_crops_merge.sort_values('track_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for i, image_path in enumerate(image_paths_raw):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops_merge.loc[all_crops_merge['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "    crops_df = crops_df.loc[crops_df['track_id'].notna(),]\n",
    "\n",
    "    # drop unique track_ids\n",
    "    # Count occurrences of each track_id\n",
    "    track_counts = crops_df[\"track_id\"].value_counts()\n",
    "    valid_tracks = track_counts[track_counts > 1].index\n",
    "    crops_df = crops_df[crops_df[\"track_id\"].isin(valid_tracks)].reset_index(drop=True)\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(device)\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': row['track_id'],\n",
    "                'ann_col': row['colour']\n",
    "            })\n",
    "\n",
    "    im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "    out_path = f'{output_dir}/{os.path.basename(image_path)}'\n",
    "    im.save(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "image_paths = os.listdir(output_dir)\n",
    "image_paths = [os.path.join(output_dir, x) for x in image_paths]\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/tracking_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![trackingGif](\" + os.path.abspath(gif_path) + \" 'tracking')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Pipeline from Command Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire pipeline can be run from the command line. The commands are shown below for demonstrative purposes using subprocess, but for high throughput analysis we recommend using slurm. There are examples of slurm scripts in the ./slurm_scripts directory: each regional bash file (e.g. `costarica_final.sh`) calls on the sbatch file `array_processor.sh`. \n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "chunk_id = 1\n",
    "batch_size = 20 # runs for 20 images at a time\n",
    "\n",
    "country='costarica'\n",
    "region=\"cri\"\n",
    "\n",
    "credentials_file=\"./credentials.json\"\n",
    "\n",
    "deployment_id = \"dep000035\"\n",
    "output_base_dir=f\"./data/{deployment_id}/{country}_test\"\n",
    "json_file = f\"./examples/example_keys/interesting_timelapse.json\"\n",
    "\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "os.makedirs(f\"{output_base_dir}/{deployment_id}\", exist_ok=True)\n",
    "\n",
    "species_model=\"./models/turing-costarica_v03_resnet50_2024-06-04-16-17_state.pt\"\n",
    "species_labels=\"./models/03_costarica_data_category_map.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_number_padded = f\"{chunk_id:04d}\"\n",
    "csv_file = f\"{output_base_dir}/{deployment_id}_{batch_number_padded}.csv\"\n",
    "print(f\"Results will save to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = [\n",
    "    \"python3\", \"-m\",\n",
    "    \"amber_inferences.cli.perform_inferences\",\n",
    "    \"--chunk_id\", str(chunk_id),\n",
    "    \"--batch_size\", str(batch_size),\n",
    "    \"--json_file\", json_file,\n",
    "    \"--output_dir\", output_base_dir,\n",
    "    \"--bucket_name\", region,\n",
    "    \"--credentials_file\", credentials_file,\n",
    "    \"--csv_file\", csv_file,\n",
    "    \"--species_model_path\", species_model,\n",
    "    \"--species_labels\", species_labels,\n",
    "    \"--perform_inference\",\n",
    "    \"--remove_image\",\n",
    "    \"--box_threshold\", \"0\",\n",
    "    \"--binary_model_path\", \"./models/moth-nonmoth-effv2b3_20220506_061527_30.pth\",\n",
    "    \"--localisation_model_path\", \"./models/flat_bug_M.pt\",\n",
    "    \"--order_model_path\", \"./models/dhc_best_128.pth\",\n",
    "    \"--order_thresholds_path\", \"./models/thresholdsTestTrain.csv\",\n",
    "    \"--skip_processed\",\n",
    "    \"--verbose\"\n",
    "]\n",
    "\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "print(result.stdout)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"STDERR:\\n\", result.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
