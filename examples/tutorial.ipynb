{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "\n",
    "This tutorial runs you through the process of running inferences for a deployments in Costa Rica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wd\n",
    "os.chdir(os.path.expanduser('~/amber-inferences'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package if required\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amber_inferences.utils.config import load_credentials\n",
    "from amber_inferences.utils.api_utils import deployments_summary, get_deployments\n",
    "from amber_inferences.utils.custom_models import *\n",
    "from amber_inferences.utils.inference_scripts import *\n",
    "from amber_inferences.utils.plotting import *\n",
    "from amber_inferences.utils.tracking import *\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data on the Object Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance for the object store\n",
    "aws_credentials = load_credentials('./credentials.json')\n",
    "session = boto3.Session(\n",
    "        aws_access_key_id=aws_credentials[\"AWS_ACCESS_KEY_ID\"],\n",
    "        aws_secret_access_key=aws_credentials[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        region_name=aws_credentials[\"AWS_REGION\"],\n",
    "    )\n",
    "s3_client = session.client(\"s3\", endpoint_url=aws_credentials[\"AWS_URL_ENDPOINT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the deployments available on the object store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deployments = get_deployments(aws_credentials['UKCEH_username'], aws_credentials['UKCEH_password'])\n",
    "all_deployments = pd.DataFrame(all_deployments)\n",
    "all_deployments[all_deployments['status'] == 'active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All countries:\")\n",
    "for x in all_deployments['country'].unique():\n",
    "    print(f\"- {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick one, cri (Costa Rica) and check out the data attached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_deployments = deployments_summary(\n",
    "    aws_credentials,\n",
    "    subset_countries=[\"Costa Rica\"],\n",
    "    subset_deployments=[\"dep000035\", \"dep000036\"],\n",
    "    include_image_count=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the files for a given deployment(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Log the image keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amber_inferences.utils.key_utils import save_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes some time to commenting out to save time\n",
    "save_keys(\n",
    "    s3_client,\n",
    "    bucket=\"cri\",\n",
    "    deployment_id=\"dep000035\",\n",
    "    output_file=\"./examples/example_keys/dep000035_keys.json\",\n",
    "    subdir=\"snapshot_images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a subset which just looks at one night:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./examples/example_keys/dep000035_keys.json\", \"r\") as f:\n",
    "    dep000035_keys = json.load(f)\n",
    "\n",
    "dep000035_keys = {list(dep000035_keys.keys())[0]: dep000035_keys[list(dep000035_keys.keys())[0]] }\n",
    "dep000035_keys\n",
    "\n",
    "# save to file\n",
    "with open(\"./examples/example_keys/interesting_timelapse.json\", \"w\") as f:\n",
    "    json.dump(dep000035_keys, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the keys\n",
    "!head ./examples/example_keys/interesting_timelapse.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Download and View the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from amber_inferences.utils.inference_scripts import download_image_from_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the first image in the keys file and open\n",
    "with open('./examples/example_keys/interesting_timelapse.json') as f:\n",
    "    keys = json.load(f)\n",
    "\n",
    "keys = keys[list(keys.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./examples/images/dep000035/interesting_timelapse/raw/', exist_ok=True)\n",
    "\n",
    "for i in range(len(keys)):\n",
    "    download_image_from_key(s3_client, keys[i], 'cri', './examples/images/dep000035/interesting_timelapse/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/raw/')\n",
    "image_paths = [os.path.join('./examples/images/dep000035/interesting_timelapse/raw/', x) for x in image_paths]\n",
    "\n",
    "image_paths = [x for x in image_paths if x.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "os.makedirs('./examples/images/dep000035/interesting_timelapse/gifs', exist_ok=True)\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/raw_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Object Detection on the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the torch device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.tensor([1.0], device=device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_load = load_models(\n",
    "    device=device,\n",
    "    localisation_model_path='./models/v1_localizmodel_2021-08-17-12-06.pt',\n",
    "    binary_model_path='./models/moth-nonmoth-effv2b3_20220506_061527_30.pth',\n",
    "    order_model_path='./models/dhc_best_128.pth',\n",
    "    order_threshold_path='./models/thresholdsTestTrain.csv',\n",
    "    species_model_path='./models/turing-costarica_v03_resnet50_2024-06-04-16-17_state.pt',\n",
    "    species_labels='./models/03_costarica_data_category_map.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops_loc = []\n",
    "\n",
    "os.makedirs('./examples/images/crops/interesting_timelapse/', exist_ok=True)\n",
    "\n",
    "for i, img_path in enumerate(tqdm(image_paths)):\n",
    "    crops = crop_image_only(\n",
    "        image_path=img_path,\n",
    "        bucket_name=\"cri\",\n",
    "        localisation_model=models_load['localisation_model'],\n",
    "        proc_device=device,\n",
    "        csv_file=\"./examples/interesting_timelapse_crops.csv\",\n",
    "        save_crops=True,\n",
    "        box_threshold=0.95,\n",
    "        crop_dir=\"./examples/images/crops/interesting_timelapse\",\n",
    "        job_name=None,\n",
    "    )\n",
    "\n",
    "    boxes = []\n",
    "    all_crops_loc.append(crops)\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    for j, row in crops.iterrows():\n",
    "        boxes.append({\n",
    "            'x_min': row['x_min'],\n",
    "            'y_min': row['y_min'],\n",
    "            'x_max': row['x_max'],\n",
    "            'y_max': row['y_max'],\n",
    "            'label': row['crop_status'],\n",
    "            'ann_col': 'grey'\n",
    "        })\n",
    "\n",
    "        # Crop original image and extract embedding\n",
    "        crop = image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "\n",
    "    del crops\n",
    "    img = image_annotation(img_path, boxes=boxes)\n",
    "\n",
    "    # save the image\n",
    "    img.save(f'{output_dir}/{os.path.basename(img_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops_loc = pd.concat(all_crops_loc)\n",
    "all_crops_loc = all_crops_loc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/object_detection_images.gif\"\n",
    "\n",
    "gif_creater(output_dir, gif_path)\n",
    "\n",
    "# show the gif\n",
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatbug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ This section is only advised if cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    models = load_models(\n",
    "        device=device,\n",
    "        localisation_model_path='./models/flat_bug_M.pt',\n",
    "        binary_model_path='./models/moth-nonmoth-effv2b3_20220506_061527_30.pth',\n",
    "        order_model_path='./models/dhc_best_128.pth',\n",
    "        order_threshold_path='./models/thresholdsTestTrain.csv',\n",
    "        species_model_path='./models/turing-costarica_v03_resnet50_2024-06-04-16-17_state.pt',\n",
    "        species_labels='./models/03_costarica_data_category_map.json'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_flatbug/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    files = os.listdir(output_dir)\n",
    "    if len(files) > 0:\n",
    "        for f in files:\n",
    "            os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    os.makedirs('./examples/images/crops/interesting_timelapse_flatbug/', exist_ok=True)\n",
    "\n",
    "    all_crops_flatbug = []\n",
    "\n",
    "    for i, img_path in enumerate(tqdm(image_paths)):\n",
    "        crops = crop_image_only(\n",
    "            image_path=img_path,\n",
    "            bucket_name=\"cri\",\n",
    "            localisation_model=models['localisation_model'],\n",
    "            proc_device=device,\n",
    "            csv_file=\"./examples/interesting_timelapse_flatbug.csv\",\n",
    "            save_crops=True,\n",
    "            box_threshold=0,\n",
    "            crop_dir=\"./examples/images/crops/interesting_timelapse_flatbug\",\n",
    "            job_name=None,\n",
    "        )\n",
    "        crops = crops.loc[crops['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "        all_crops_flatbug = all_crops_flatbug + [crops]\n",
    "        if crops.shape[0] > 0:\n",
    "            boxes = []\n",
    "            for j, row in crops.iterrows():\n",
    "                boxes.append({\n",
    "                    'x_min': row['x_min'],\n",
    "                    'y_min': row['y_min'],\n",
    "                    'x_max': row['x_max'],\n",
    "                    'y_max': row['y_max'],\n",
    "                    'label': '',\n",
    "                    'ann_col': 'grey'\n",
    "                })\n",
    "            del crops\n",
    "            img = image_annotation(img_path, boxes=boxes, scale=False)\n",
    "\n",
    "            # save the image\n",
    "            img.save(f'{output_dir}/{os.path.basename(img_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Open images and convert to a sequence\n",
    "    image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/annotated_boxes_flatbug/')\n",
    "    image_paths = [os.path.join(output_dir, x) for x in image_paths]\n",
    "    images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "    # Save as GIF\n",
    "    gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/flatbug_detection_images.gif\"\n",
    "    images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "    # display(Image.open(gif_path))\n",
    "    del images\n",
    "\n",
    "    all_crops_flatbug = pd.concat(all_crops_flatbug)\n",
    "    all_crops_flatbug = all_crops_flatbug.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Objects for Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which crops to use (localisation or flatbug)\n",
    "all_crops = all_crops_flatbug # or all_crops_loc\n",
    "\n",
    "transform_species = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((300, 300)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "image_paths_raw = image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/raw/')\n",
    "image_paths_raw = [os.path.abspath(os.path.join('./examples/images/dep000035/interesting_timelapse/raw/', x)) for x in image_paths_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_binary'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image_path in enumerate(tqdm(image_paths_raw)):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops.loc[all_crops['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            ann_col='red'\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(device)\n",
    "            binary_prediction = classify_box(cropped_tensor, models_load['classification_model'])\n",
    "            if binary_prediction[0] == 'moth':\n",
    "                ann_col='green'\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': binary_prediction[0],\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'{output_dir}/{os.path.basename(image_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/binary_images.gif\"\n",
    "\n",
    "gif_creater(output_dir, gif_path)\n",
    "\n",
    "# show the gif\n",
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_order/'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))\n",
    "\n",
    "imgs = []\n",
    "for i, image_path in enumerate(tqdm(image_paths_raw)):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops.loc[all_crops['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            ann_col = 'red'\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(device)\n",
    "            order_prediction = classify_order(\n",
    "                cropped_tensor,\n",
    "                models_load['order_model'],\n",
    "                models_load['order_model_labels'],\n",
    "                models_load['order_model_thresholds']\n",
    "            )\n",
    "            if 'Lepidoptera' in order_prediction[0]:\n",
    "                ann_col = 'green'\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': order_prediction[0],\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'{output_dir}/{os.path.basename(image_path)}')\n",
    "\n",
    "        imgs = imgs + [im]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "image_paths = os.listdir(output_dir)\n",
    "image_paths = [os.path.join(output_dir, x) for x in image_paths]\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/order_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![orderGif](\" + os.path.abspath(gif_path) + \" 'order')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Species Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_species/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "all_embeddings = {}\n",
    "for i, image_path in enumerate(tqdm(image_paths_raw)):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops.loc[all_crops['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "    all_embeddings[image_path] = {}\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(device)\n",
    "            order_prediction = classify_order(\n",
    "                cropped_tensor,\n",
    "                models_load['order_model'],\n",
    "                models_load['order_model_labels'],\n",
    "                models_load['order_model_thresholds']\n",
    "            )\n",
    "            label = \"\"\n",
    "            ann_col = 'red'\n",
    "\n",
    "            if 'Lepidoptera' in order_prediction[0]:\n",
    "                species_names, species_confidences, embeddings = classify_species(\n",
    "                    cropped_tensor,\n",
    "                    models_load['species_model'],\n",
    "                    models_load['species_model_labels'],\n",
    "                    5\n",
    "                )\n",
    "                label = f\"{species_names[0]}, {'{:.2f}'.format(species_confidences[0]*100)}%\"\n",
    "                ann_col='green'\n",
    "                all_embeddings[image_path][row['crop_status']] = {\n",
    "                    'embedding': embeddings,\n",
    "                    'image_path': os.path.basename(image_path),\n",
    "                    'crop': row['crop_status'],\n",
    "                    'box': {'xmin':row['x_min'], 'ymin':row['y_min'], 'xmax':row['x_max'], 'ymax':row['y_max']},\n",
    "                    'image_size': [original_width, original_height]\n",
    "                }\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': label,\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'{output_dir}/{os.path.basename(image_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "image_paths = os.listdir(output_dir)\n",
    "image_paths = [os.path.join(output_dir, x) for x in image_paths]\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/species_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![speciesGif](\" + os.path.abspath(gif_path) + \" 'species')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to be able to track individual insects across frames. This is done by using the tracking model. The tracking model takes in a list of detections and returns a list of tracks. Each track is a list of detections that belong to the same insect.\n",
    "\n",
    "A track is defined by the IoU, distance between crops, similarity in features, and area. So we start by taking the embeddings from the species classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amber_inferences.utils.tracking import *\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each image, get the combinations between those crops and those of the next image\n",
    "all_crop_pairs = pd.DataFrame({})\n",
    "image_paths = list(set(list(all_crops['image_path'])))\n",
    "image_paths.sort()\n",
    "\n",
    "for i in tqdm(range(1, len(image_paths))):\n",
    "    img1 = image_paths[i - 1] # previous image\n",
    "    img2 = image_paths[i] # current image\n",
    "\n",
    "    crops1 = all_embeddings[img1]\n",
    "    crops2 = all_embeddings[img2]\n",
    "\n",
    "    for c1, c2 in product(crops1, crops2):\n",
    "        all_crop_pairs = pd.concat([all_crop_pairs,\n",
    "                                    pd.DataFrame({'image1':[img1], 'crop1':[c1],\n",
    "                                               'image2':[img2], 'crop2':[c2]})])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crop_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the similarity between crops in subsequent images\n",
    "results = pd.DataFrame({})\n",
    "\n",
    "for i, row in tqdm(all_crop_pairs.iterrows(), total=all_crop_pairs.shape[0]):\n",
    "    c_a = all_embeddings[str(row['image1'])][str(row['crop1'])]\n",
    "    c_a['image_path'] = str(row['image1'])\n",
    "    c_b = all_embeddings[str(row['image2'])][str(row['crop2'])]\n",
    "    c_b['image_path'] = str(row['image2'])\n",
    "\n",
    "    res = calculate_cost(c_a, c_b)\n",
    "    results = pd.concat([results, res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of the costs\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(results['total_cost'], bins=50, color='blue', alpha=0.7)\n",
    "plt.xlabel('Total Cost')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Total Costs')\n",
    "\n",
    "plt.annotate('<- increasing similarity', xy=(0, 0.9), xycoords='axes fraction',\n",
    "             fontsize=8, color='black', ha='left')\n",
    "plt.annotate('decreasing similarity ->', xy=(1, 0.9), xycoords='axes fraction',\n",
    "fontsize=8, color='black', ha='right')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the tracks. A track is the series of crops we consider to belong to one individual. This is based on a cost threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best match for crops from last image\n",
    "best_matches = find_best_matches(results)\n",
    "best_matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ac = list(all_crops['image_path'] + all_crops['crop_status'])\n",
    "bm = list(best_matches['image_path'] + best_matches['crop_status'])\n",
    "res1 = list(set(list(results['crop1_path']+results['crop1_crop'])))\n",
    "res2 = list(set(list(results['crop2_path']+results['crop2_crop'])))\n",
    "acp1 = list(set(list(all_crop_pairs['image1']+all_crop_pairs['crop1'])))\n",
    "acp2 = list(set(list(all_crop_pairs['image2']+all_crop_pairs['crop2'])))\n",
    "\n",
    "# TODO create tests for this\n",
    "assert(len(list(set(acp1) - set(ac))) == 0)\n",
    "assert(len(list(set(acp2) - set(ac))) == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def track_id_calc2(best_matches, cost_threshold=1, col_palette='tab20'):\n",
    "        # Make a copy and rename the relevant columns\n",
    "    best_matches = best_matches.copy()\n",
    "    rename_map = {\n",
    "        best_matches.columns[0]: 'image1',\n",
    "        best_matches.columns[1]: 'crop1',\n",
    "        best_matches.columns[2]: 'image2',\n",
    "        best_matches.columns[3]: 'crop2',\n",
    "    }\n",
    "    best_matches = best_matches.rename(columns=rename_map)\n",
    "\n",
    "    # Filter based on the cost threshold\n",
    "    filtered_matches = best_matches[best_matches[\"total_cost\"] < cost_threshold]\n",
    "\n",
    "    def node_id(image_path, crop_id):\n",
    "        return f\"{image_path}|{crop_id}\"\n",
    "\n",
    "    # Build a graph of matches under the threshold\n",
    "    G_thresh = nx.Graph()\n",
    "    for _, row in filtered_matches.iterrows():\n",
    "        n1 = node_id(row['image1'], row['crop1'])\n",
    "        n2 = node_id(row['image2'], row['crop2'])\n",
    "        G_thresh.add_edge(n1, n2)\n",
    "\n",
    "    # Assign track IDs from connected components\n",
    "    track_mapping = {}\n",
    "    for tid, component in enumerate(nx.connected_components(G_thresh)):\n",
    "        for node in component:\n",
    "            track_mapping[node] = f\"Track_{str(tid).rjust(5, '0')}\"\n",
    "\n",
    "    # Collect all unique nodes from both sides\n",
    "    all_nodes = set()\n",
    "    for _, row in best_matches.iterrows():\n",
    "        all_nodes.add(node_id(row['image1'], row['crop1']))\n",
    "        all_nodes.add(node_id(row['image2'], row['crop2']))\n",
    "\n",
    "    # Create lookup for cost (minimum per crop node)\n",
    "    cost_lookup = {}\n",
    "    for _, row in best_matches.iterrows():\n",
    "        for prefix in [\"1\", \"2\"]:\n",
    "            nid = node_id(row[f\"image{prefix}\"], row[f\"crop{prefix}\"])\n",
    "            cost = row[\"total_cost\"]\n",
    "            cost_lookup[nid] = min(cost_lookup.get(nid, float(\"inf\")), cost)\n",
    "\n",
    "    # Assemble the final output rows\n",
    "    output_rows = []\n",
    "    for node in all_nodes:\n",
    "        image_path, crop_id = node.rsplit(\"|\", 2)\n",
    "        output_rows.append({\n",
    "            \"image_path\": image_path,\n",
    "            \"crop_id\": crop_id,\n",
    "            \"track_id\": track_mapping.get(node),  # May be None if unmatched under threshold\n",
    "            \"total_cost\": cost_lookup.get(node)\n",
    "        })\n",
    "\n",
    "    output_df = pd.DataFrame(output_rows)\n",
    "\n",
    "    # Assign unique track IDs to unmatched crops\n",
    "    max_existing_id = (\n",
    "        max(\n",
    "            [int(tid.replace(\"Track_\", \"\")) for tid in output_df[\"track_id\"].dropna()],\n",
    "            default=-1\n",
    "        )\n",
    "    )\n",
    "    unmatched_mask = output_df[\"track_id\"].isnull()\n",
    "    unmatched_indices = output_df[unmatched_mask].index\n",
    "    new_ids = [\n",
    "        f\"Track_{str(i).rjust(5, '0')}\"\n",
    "        for i in range(max_existing_id + 1, max_existing_id + 1 + unmatched_mask.sum())\n",
    "    ]\n",
    "    output_df.loc[unmatched_indices, \"track_id\"] = new_ids\n",
    "\n",
    "\n",
    "    # create a colour label\n",
    "    # Generate N unique colors for each track\n",
    "    num_tracks = output_df['track_id'].nunique()\n",
    "\n",
    "    # Use a colormap to get visually distinct colors\n",
    "    cmap = plt.cm.get_cmap(col_palette, num_tracks)  # tab20\n",
    "\n",
    "    # Map track_id to hex colors\n",
    "    track_id_to_color = {\n",
    "        track_id: mcolors.to_hex(cmap(i)) for i, track_id in enumerate(sorted(output_df['track_id'].unique()))\n",
    "    }\n",
    "\n",
    "    # Add color column to DataFrame\n",
    "    output_df['colour'] = output_df['track_id'].map(track_id_to_color)\n",
    "\n",
    "    return output_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_df = track_id_calc2(best_matches, cost_threshold=1.5)\n",
    "tracks_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_df['track_id'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops_merge = all_crops.merge(tracks_df, how='left', left_on=['image_path', 'crop_status'], right_on=['image_path', 'crop_id'])\n",
    "all_crops_merge = all_crops_merge.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_tracking/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "files = [x for x in files if x.endswith('jpg')]\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unique track_ids\n",
    "frame_cutoff = 1\n",
    "\n",
    "all_crops_merge_subset = all_crops_merge.copy()\n",
    "track_counts = all_crops_merge_subset[\"track_id\"].value_counts()\n",
    "valid_tracks = track_counts[track_counts > frame_cutoff].index\n",
    "all_crops_merge_subset = all_crops_merge_subset[all_crops_merge_subset[\"track_id\"].isin(valid_tracks)].reset_index(drop=True)\n",
    "all_crops_merge_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "\n",
    "for i, image_path in enumerate(image_paths_raw):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops_merge_subset.loc[all_crops_merge_subset['image_path'] == image_path, ]\n",
    "    boxes = []\n",
    "    if crops_df.shape[0] > 0:\n",
    "        for j, row in crops_df.iterrows():\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': row['track_id'],\n",
    "                'ann_col': row['colour']\n",
    "            })\n",
    "\n",
    "    im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "    out_path = f'{output_dir}/{os.path.basename(image_path)}'\n",
    "    im.save(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "image_paths = os.listdir(output_dir)\n",
    "image_paths = [os.path.join(output_dir, x) for x in image_paths]\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/tracking_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![trackingGif](\" + os.path.abspath(gif_path) + \" 'tracking')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
