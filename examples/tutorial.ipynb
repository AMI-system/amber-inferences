{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "\n",
    "This tutorial runs you through the process of running inferences for a deployments in Costa Rica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wd\n",
    "os.chdir(os.path.expanduser('~/amber-inferences'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package if required\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amber_inferences.utils.config import load_credentials\n",
    "from amber_inferences.utils.api_utils import deployments_summary, get_deployments\n",
    "from amber_inferences.utils.custom_models import *\n",
    "from amber_inferences.utils.inference_scripts import *\n",
    "from amber_inferences.utils.plotting import *\n",
    "from amber_inferences.utils.tracking import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "import torch\n",
    "import requests\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data on the Object Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance for the object store\n",
    "aws_credentials = load_credentials('./credentials.json')\n",
    "session = boto3.Session(\n",
    "        aws_access_key_id=aws_credentials[\"AWS_ACCESS_KEY_ID\"],\n",
    "        aws_secret_access_key=aws_credentials[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        region_name=aws_credentials[\"AWS_REGION\"],\n",
    "    )\n",
    "s3_client = session.client(\"s3\", endpoint_url=aws_credentials[\"AWS_URL_ENDPOINT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the deployments available on the object store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deployments = get_deployments(aws_credentials['UKCEH_username'], aws_credentials['UKCEH_password'])\n",
    "all_deployments = pd.DataFrame(all_deployments)\n",
    "all_deployments[all_deployments['status'] == 'active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All countries:\")\n",
    "for x in all_deployments['country'].unique():\n",
    "    print(f\"- {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick one, cri (Costa Rica) and check out the data attached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_deployments = deployments_summary(\n",
    "    aws_credentials,\n",
    "    subset_countries=[\"Costa Rica\"],\n",
    "    subset_deployments=[\"dep000035\", \"dep000036\"],\n",
    "    include_image_count=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the files for a given deployment(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log the image keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amber_inferences.utils.key_utils import save_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes some time to commenting out to save time\n",
    "# save_keys(\n",
    "#     s3_client,\n",
    "#     bucket=\"cri\",\n",
    "#     deployment_id=\"dep000035\",\n",
    "#     output_file=\"./examples/example_keys/dep000035_keys.json\",\n",
    "#     subdir=\"snapshot_images\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the keys\n",
    "!head ./examples/example_keys/interesting_timelapse.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download and View the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from amber_inferences.utils.inference_scripts import download_image_from_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the first image in the keys file and open\n",
    "with open('./examples/example_keys/interesting_timelapse.json') as f:\n",
    "    keys = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./examples/images/dep000035/interesting_timelapse/raw/', exist_ok=True)\n",
    "\n",
    "for i in range(len(keys)):\n",
    "    download_image_from_key(s3_client, keys[i], 'cri', './examples/images/dep000035/interesting_timelapse/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/raw/')\n",
    "image_paths = [os.path.join('./examples/images/dep000035/interesting_timelapse/raw/', x) for x in image_paths]\n",
    "\n",
    "image_paths = [x for x in image_paths if x.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "os.makedirs('./examples/images/dep000035/interesting_timelapse/gifs', exist_ok=True)\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/raw_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Object Detection on the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the torch device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.tensor([1.0], device=device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_load = load_models(\n",
    "    device=device,\n",
    "    localisation_model_path='./models/v1_localizmodel_2021-08-17-12-06.pt',\n",
    "    binary_model_path='./models/moth-nonmoth-effv2b3_20220506_061527_30.pth',\n",
    "    order_model_path='./models/dhc_best_128.pth',\n",
    "    order_threshold_path='./models/thresholdsTestTrain.csv',\n",
    "    species_model_path='./models/turing-costarica_v03_resnet50_2024-06-04-16-17_state.pt',\n",
    "    species_labels='./models/03_costarica_data_category_map.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops_loc = []\n",
    "\n",
    "os.makedirs('./examples/images/crops/interesting_timelapse/', exist_ok=True)\n",
    "\n",
    "for i, img_path in enumerate(tqdm(image_paths)):\n",
    "    crops = crop_image_only(\n",
    "        image_path=img_path,\n",
    "        bucket_name=\"cri\",\n",
    "        localisation_model=models_load['localisation_model'],\n",
    "        proc_device=device,\n",
    "        csv_file=\"./examples/interesting_timelapse_crops.csv\",\n",
    "        save_crops=True,\n",
    "        box_threshold=0.95,\n",
    "        crop_dir=\"./examples/images/crops/interesting_timelapse\",\n",
    "        job_name=None,\n",
    "    )\n",
    "\n",
    "    boxes = []\n",
    "    all_crops_loc.append(crops)\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    for j, row in crops.iterrows():\n",
    "        boxes.append({\n",
    "            'x_min': row['x_min'],\n",
    "            'y_min': row['y_min'],\n",
    "            'x_max': row['x_max'],\n",
    "            'y_max': row['y_max'],\n",
    "            'label': row['crop_status'],\n",
    "            'ann_col': 'grey'\n",
    "        })\n",
    "\n",
    "        # Crop original image and extract embedding\n",
    "        crop = image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "\n",
    "    del crops\n",
    "    img = image_annotation(img_path, boxes=boxes)\n",
    "\n",
    "    # save the image\n",
    "    img.save(f'{output_dir}/{os.path.basename(img_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops_loc = pd.concat(all_crops_loc)\n",
    "all_crops_loc = all_crops_loc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/object_detection_images.gif\"\n",
    "\n",
    "gif_creater(output_dir, gif_path)\n",
    "\n",
    "# show the gif\n",
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatbug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ This section is only advised if cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load_models(\n",
    "    device=device,\n",
    "    localisation_model_path='./models/flat_bug_M.pt',\n",
    "    binary_model_path='./models/moth-nonmoth-effv2b3_20220506_061527_30.pth',\n",
    "    order_model_path='./models/dhc_best_128.pth',\n",
    "    order_threshold_path='./models/thresholdsTestTrain.csv',\n",
    "    species_model_path='./models/turing-costarica_v03_resnet50_2024-06-04-16-17_state.pt',\n",
    "    species_labels='./models/03_costarica_data_category_map.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_flatbug/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    files = os.listdir(output_dir)\n",
    "    if len(files) > 0:\n",
    "        for f in files:\n",
    "            os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    os.makedirs('./examples/images/crops/interesting_timelapse_flatbug/', exist_ok=True)\n",
    "\n",
    "    all_crops_flatbug = []\n",
    "\n",
    "    for i, img_path in enumerate(tqdm(image_paths)):\n",
    "        crops = crop_image_only(\n",
    "            image_path=img_path,\n",
    "            bucket_name=\"cri\",\n",
    "            localisation_model=models['localisation_model'],\n",
    "            proc_device=device,\n",
    "            csv_file=\"./examples/interesting_timelapse_flatbug.csv\",\n",
    "            save_crops=True,\n",
    "            box_threshold=0,\n",
    "            crop_dir=\"./examples/images/crops/interesting_timelapse_flatbug\",\n",
    "            job_name=None,\n",
    "        )\n",
    "        crops = crops.loc[crops['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "        all_crops_flatbug = all_crops_flatbug + [crops]\n",
    "        if crops.shape[0] > 0:\n",
    "            boxes = []\n",
    "            for j, row in crops.iterrows():\n",
    "                boxes.append({\n",
    "                    'x_min': row['x_min'],\n",
    "                    'y_min': row['y_min'],\n",
    "                    'x_max': row['x_max'],\n",
    "                    'y_max': row['y_max'],\n",
    "                    'label': '',\n",
    "                    'ann_col': 'grey'\n",
    "                })\n",
    "            del crops\n",
    "            img = image_annotation(img_path, boxes=boxes, scale=False)\n",
    "\n",
    "            # save the image\n",
    "            img.save(f'{output_dir}/{os.path.basename(img_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Open images and convert to a sequence\n",
    "    image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/annotated_boxes_flatbug/')\n",
    "    image_paths = [os.path.join(output_dir, x) for x in image_paths]\n",
    "    images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "    # Save as GIF\n",
    "    gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/flatbug_detection_images.gif\"\n",
    "    images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "    # display(Image.open(gif_path))\n",
    "    del images\n",
    "\n",
    "    all_crops_flatbug = pd.concat(all_crops_flatbug)\n",
    "    all_crops_flatbug = all_crops_flatbug.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objects for Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which crops to use (localisation or flatbug)\n",
    "all_crops = all_crops_flatbug # or all_crops_loc\n",
    "\n",
    "transform_species = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((300, 300)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "image_paths_raw = image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/raw/')\n",
    "image_paths_raw = [os.path.abspath(os.path.join('./examples/images/dep000035/interesting_timelapse/raw/', x)) for x in image_paths_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_binary'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image_path in enumerate(tqdm(image_paths_raw)):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops.loc[all_crops['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            ann_col='red'\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(device)\n",
    "            binary_prediction = classify_box(cropped_tensor, models_load['classification_model'])\n",
    "            if binary_prediction[0] == 'moth':\n",
    "                ann_col='green'\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': binary_prediction[0],\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'{output_dir}/{os.path.basename(image_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/binary_images.gif\"\n",
    "\n",
    "gif_creater(output_dir, gif_path)\n",
    "\n",
    "# show the gif\n",
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_order/'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))\n",
    "\n",
    "imgs = []\n",
    "for i, image_path in enumerate(tqdm(image_paths_raw)):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops.loc[all_crops['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            ann_col = 'red'\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(device)\n",
    "            order_prediction = classify_order(\n",
    "                cropped_tensor,\n",
    "                models_load['order_model'],\n",
    "                models_load['order_model_labels'],\n",
    "                models_load['order_model_thresholds']\n",
    "            )\n",
    "            if 'Lepidoptera' in order_prediction[0]:\n",
    "                ann_col = 'green'\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': order_prediction[0],\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'{output_dir}/{os.path.basename(image_path)}')\n",
    "\n",
    "        imgs = imgs + [im]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "image_paths = os.listdir(output_dir)\n",
    "image_paths = [os.path.join(output_dir, x) for x in image_paths]\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/order_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![orderGif](\" + os.path.abspath(gif_path) + \" 'order')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Species Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_species/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))\n",
    "\n",
    "imgs = []\n",
    "all_embeddings = {}\n",
    "for i, image_path in enumerate(tqdm(image_paths_raw)):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops.loc[all_crops['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "    all_embeddings[image_path] = {}\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(device)\n",
    "            order_prediction = classify_order(\n",
    "                cropped_tensor,\n",
    "                models_load['order_model'],\n",
    "                models_load['order_model_labels'],\n",
    "                models_load['order_model_thresholds']\n",
    "            )\n",
    "            label = \"\"\n",
    "            ann_col = 'red'\n",
    "\n",
    "            if 'Lepidoptera' in order_prediction[0]:\n",
    "                species_names, species_confidences, embeddings = classify_species(\n",
    "                    cropped_tensor,\n",
    "                    models_load['species_model'],\n",
    "                    models_load['species_model_labels'],\n",
    "                    5\n",
    "                )\n",
    "                label = f\"{species_names[0]}, {'{:.2f}'.format(species_confidences[0]*100)}%\"\n",
    "                ann_col='green'\n",
    "                all_embeddings[image_path][f'crop_{j}'] = {\n",
    "                    'embedding': embeddings,\n",
    "                    'file': os.path.basename(image_path),\n",
    "                    'crop': f'crop_{j+1}',\n",
    "                    'box': {'xmin':row['x_min'], 'ymin':row['y_min'], 'xmax':row['x_max'], 'ymax':row['y_max']}\n",
    "                }\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': label,\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'{output_dir}/{os.path.basename(image_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "image_paths = os.listdir(output_dir)\n",
    "image_paths = [os.path.join(output_dir, x) for x in image_paths]\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/species_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![speciesGif](\" + os.path.abspath(gif_path) + \" 'species')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to be able to track individual insects across frames. This is done by using the tracking model. The tracking model takes in a list of detections and returns a list of tracks. Each track is a list of detections that belong to the same insect.\n",
    "\n",
    "A track is defined by the IoU, distance between crops, similarity in features, and area. So we start by taking the embeddings from the species classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create a list of embeddings\n",
    "# embeddings_list = {}\n",
    "\n",
    "# for i in all_crops['image_path'].unique():\n",
    "#     embeddings_list[i] = {}\n",
    "\n",
    "# for i, crops in tqdm(all_crops.iterrows()):\n",
    "#     img_path = crops['image_path']\n",
    "#     image = Image.open(img_path).convert(\"RGB\")\n",
    "#     cropped_image = image.crop((crops['x_min'], crops['y_min'], crops['x_max'], crops['y_max']))\n",
    "\n",
    "#     embedding = extract_embedding(cropped_image, models_load['species_model'], device=device)\n",
    "#     embeddings_list[img_path][crops['crop_status']] = {\n",
    "#         'embedding': embedding,\n",
    "#         'file': os.path.basename(crops['image_path']),\n",
    "#         'crop': crops['crop_status'],\n",
    "#         'crop_image': cropped_image,\n",
    "#         'box': {'xmin':crops['x_min'], 'ymin':crops['y_min'], 'xmax':crops['x_max'], 'ymax':crops['y_max']}\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amber_inferences.utils.tracking import *\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each image, get the combinations between those crops and those of the next image\n",
    "all_crop_pairs = []\n",
    "image_paths = list(all_embeddings.keys())\n",
    "\n",
    "for i in range(len(image_paths) - 1):\n",
    "    img1 = image_paths[i]\n",
    "    img2 = image_paths[i + 1]\n",
    "\n",
    "    crops1 = all_embeddings[img1]\n",
    "    crops2 = all_embeddings[img2]\n",
    "\n",
    "    for c1, c2 in product(crops1, crops2):\n",
    "        all_crop_pairs.append((img1, c1, img2, c2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crop_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the similarity between crops in subsequent images\n",
    "results = []\n",
    "\n",
    "for image_a, crop_a, image_b, crop_b in tqdm(all_crop_pairs):\n",
    "    c_a = all_embeddings[image_a][crop_a]\n",
    "    c_a['image_path'] = image_a\n",
    "    c_b = all_embeddings[image_b][crop_b]\n",
    "    c_b['image_path'] = image_b\n",
    "\n",
    "    res = calculate_cost(c_a, c_b)\n",
    "    results.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"image_path1\",\n",
    "    \"crop1_id\",\n",
    "    \"image_path2\",\n",
    "    \"crop2_id\",\n",
    "    \"cnn_cost\",\n",
    "    \"iou_cost\",\n",
    "    \"box_ratio_cost\",\n",
    "    \"dist_ratio_cost\",\n",
    "    \"total_cost\"\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(results).reset_index(drop=True)\n",
    "results_df.columns = columns\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot a histogram of the costs\n",
    "plt.figure(figsize=(5, 3))\n",
    "plt.hist(results_df['total_cost'], bins=50, color='blue', alpha=0.7)\n",
    "plt.xlabel('Total Cost')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Total Costs')\n",
    "\n",
    "plt.annotate('<- increasing similarity', xy=(0, 0.9), xycoords='axes fraction',\n",
    "             fontsize=8, color='black', ha='left')\n",
    "plt.annotate('decreasing similarity ->', xy=(1, 0.9), xycoords='axes fraction',\n",
    "fontsize=8, color='black', ha='right')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define the tracks. A track is the series of crops we consider to belong to one individual. This is based on a cost threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best match for crops from last image\n",
    "best_matches = find_best_matches(results_df)\n",
    "best_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracks_df = track_id_calc(best_matches, cost_threshold=1)\n",
    "tracks_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Generate N unique colors for each track\n",
    "num_tracks = tracks_df['track_id'].nunique()\n",
    "\n",
    "# Use a colormap to get visually distinct colors\n",
    "cmap = plt.cm.get_cmap('hsv', num_tracks)  # tab20\n",
    "\n",
    "# Map track_id to hex colors\n",
    "track_id_to_color = {\n",
    "    track_id: mcolors.to_hex(cmap(i)) for i, track_id in enumerate(sorted(tracks_df['track_id'].unique()))\n",
    "}\n",
    "\n",
    "# Add color column to DataFrame\n",
    "tracks_df['colour'] = tracks_df['track_id'].map(track_id_to_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops_merge = all_crops.merge(tracks_df, how='left', left_on=['image_path', 'crop_status'], right_on=['image_path', 'crop_id'])\n",
    "all_crops_merge = all_crops_merge.reset_index(drop=True)\n",
    "all_crops_merge.sort_values('track_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_tracking/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "\n",
    "# drop unique track_ids\n",
    "all_crops_merge_subset = all_crops_merge.copy()\n",
    "track_counts = all_crops_merge_subset[\"track_id\"].value_counts()\n",
    "valid_tracks = track_counts[track_counts > 1].index\n",
    "all_crops_merge_subset = all_crops_merge_subset[all_crops_merge_subset[\"track_id\"].isin(valid_tracks)].reset_index(drop=True)\n",
    "\n",
    "for i, image_path in enumerate(image_paths_raw):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops_merge_subset.loc[all_crops_merge_subset['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "    crops_df = crops_df.loc[crops_df['track_id'].notna(),]\n",
    "\n",
    "    boxes = []\n",
    "    if crops_df.shape[0] > 0:\n",
    "        for j, row in crops_df.iterrows():\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': row['track_id'],\n",
    "                'ann_col': row['colour']\n",
    "            })\n",
    "\n",
    "    im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "    out_path = f'{output_dir}/{os.path.basename(image_path)}'\n",
    "    im.save(out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "image_paths = os.listdir(output_dir)\n",
    "image_paths = [os.path.join(output_dir, x) for x in image_paths]\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/tracking_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![trackingGif](\" + os.path.abspath(gif_path) + \" 'tracking')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
