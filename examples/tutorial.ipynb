{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "\n",
    "This tutorial runs you through the process of running inferences for a deployments in Costa Rica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wd\n",
    "os.chdir(os.path.expanduser('~/amber-inferences'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package if required\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFont, ImageDraw, Image\n",
    "\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import amber_inferences\n",
    "from amber_inferences.utils.config import load_credentials\n",
    "from amber_inferences.utils.api_utils import get_buckets, deployments_summary, get_deployments\n",
    "from amber_inferences.utils.custom_models import *\n",
    "from amber_inferences.utils.inference_scripts import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data on the Object Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance for the object store\n",
    "aws_credentials = load_credentials('./credentials.json')\n",
    "session = boto3.Session(\n",
    "        aws_access_key_id=aws_credentials[\"AWS_ACCESS_KEY_ID\"],\n",
    "        aws_secret_access_key=aws_credentials[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        region_name=aws_credentials[\"AWS_REGION\"],\n",
    "    )\n",
    "s3_client = session.client(\"s3\", endpoint_url=aws_credentials[\"AWS_URL_ENDPOINT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ðŸš¨ Note: this feature has been taken down by Posit so the app is not currently available ðŸš¨**\n",
    "\n",
    "Look at the deployments available on the object store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_deployments = get_deployments(aws_credentials['UKCEH_username'], aws_credentials['UKCEH_password'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "List the buckets/countries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_deployments = pd.DataFrame(all_deployments)\n",
    "# all_deployments[all_deployments['status'] == 'active']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick one, cri (Costa Rica) and check out the data attached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cr_deployments = deployments_summary(\n",
    "#     aws_credentials,\n",
    "#     subset_countries=[\"Costa Rica\"],\n",
    "#     subset_deployments=[\"dep000035\", \"dep000036\"],\n",
    "#     include_image_count=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the files for a given deployment(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cr_deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Log the image keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amber_inferences.utils.key_utils import save_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes some time to commenting out to save time\n",
    "# save_keys(\n",
    "#     s3_client,\n",
    "#     bucket=\"cri\",\n",
    "#     deployment_id=\"dep000035\",\n",
    "#     output_file=\"./examples/example_keys/dep000035_keys.json\",\n",
    "#     subdir=\"snapshot_images\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the keys\n",
    "!head ./examples/example_keys/interesting_timelapse.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from amber_inferences.utils.inference_scripts import download_image_from_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the first image in the keys file and open\n",
    "with open('./examples/example_keys/interesting_timelapse.json') as f:\n",
    "    keys = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./examples/images/dep000035/interesting_timelapse/raw/', exist_ok=True)\n",
    "\n",
    "for i in range(len(keys)):\n",
    "    download_image_from_key(s3_client, keys[i], 'cri', './examples/images/dep000035/interesting_timelapse/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/raw/')\n",
    "image_paths = [os.path.join('./examples/images/dep000035/interesting_timelapse/raw/', x) for x in image_paths]\n",
    "\n",
    "image_paths = [x for x in image_paths if x.endswith('.jpg')]\n",
    "\n",
    "len(image_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open the images in a 2x5 grid\n",
    "# fig, axs = plt.subplots(5, 6, figsize=(20, 20))\n",
    "# axs = axs.ravel()\n",
    "\n",
    "# for i, img_path in enumerate(image_paths):\n",
    "#     if os.path.exists( img_path):  # Ensure the file exists\n",
    "#         img = mpimg.imread(img_path)\n",
    "#         axs[i].imshow(img)\n",
    "#         axs[i].axis(\"off\")  # Hide axes for better visualization\n",
    "#         axs[i].set_title(f\"Image {i+1}\")\n",
    "#     else:\n",
    "#         axs[i].axis(\"off\")\n",
    "#         axs[i].set_title(\"Missing Image\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "os.makedirs('./examples/images/dep000035/interesting_timelapse/gifs', exist_ok=True)\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/raw_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Object Detection on the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the torch device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_load = load_models(\n",
    "    device=device,\n",
    "    localisation_model_path='./models/v1_localizmodel_2021-08-17-12-06.pt',\n",
    "    binary_model_path='./models/moth-nonmoth-effv2b3_20220506_061527_30.pth',\n",
    "    order_model_path='./models/dhc_best_128.pth',\n",
    "    order_threshold_path='./models/thresholdsTestTrain.csv',\n",
    "    species_model_path='./models/turing-costarica_v03_resnet50_2024-06-04-16-17_state.pt',\n",
    "    species_labels='./models/03_costarica_data_category_map.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_annotation(image_path, img=None, boxes={}, scale=False, default_colour='grey'):\n",
    "    if img is None:\n",
    "        img = Image.open(image_path)\n",
    "\n",
    "    draw = ImageDraw.Draw(img)\n",
    "\n",
    "    for box in boxes:\n",
    "        x0 = float(box['x_min'])\n",
    "        y0 = float(box['y_min'])\n",
    "        x1 = float(box['x_max'])\n",
    "        y1 = float(box['y_max'])\n",
    "        if scale:\n",
    "            og_width, og_height = img.size\n",
    "            x0 = x0/300*og_width\n",
    "            y0 = y0/300*og_height\n",
    "            x1 = x1/300*og_width\n",
    "            y1 = y1/300*og_height\n",
    "        if 'ann_col' not in box.keys():\n",
    "            box['ann_col'] = default_colour\n",
    "        if 'label' not in box.keys():\n",
    "            box['label'] = ''\n",
    "\n",
    "        draw.rectangle([x0, y0, x1, y1], outline=box['ann_col'], width=3)\n",
    "        draw.text((x0, y0), box['label'], fill=box['ann_col'],\n",
    "                  font=ImageFont.truetype(\"/usr/share/fonts/truetype/dejavu/DejaVuSans-Bold.ttf\", size=50) )\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load a pretrained feature extractor (e.g., ResNet50 without classifier)\n",
    "resnet = models.resnet50(pretrained=True)\n",
    "feature_extractor = torch.nn.Sequential(*list(resnet.children())[:-1])  # remove final FC\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Transform for input crops\n",
    "embedding_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((300, 300)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                             std=[0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "def extract_embedding(crop):\n",
    "    \"\"\"Crop: a PIL image.\"\"\"\n",
    "    input_tensor = embedding_transform(crop).unsqueeze(0)  # Add batch dim\n",
    "    with torch.no_grad():\n",
    "        embedding = feature_extractor(input_tensor).squeeze().numpy()  # Shape (2048,)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops_loc = []\n",
    "embeddings_list = {}\n",
    "\n",
    "os.makedirs('./examples/images/dep000035/interesting_timelapse/annotated_boxes/', exist_ok=True)\n",
    "os.makedirs('./examples/images/crops/interesting_timelapse/', exist_ok=True)\n",
    "\n",
    "for i, img_path in tqdm(enumerate(image_paths[0:5])):\n",
    "    crops = crop_image_only(\n",
    "        image_path=img_path,\n",
    "        bucket_name=\"cri\",\n",
    "        localisation_model=models_load['localisation_model'],\n",
    "        proc_device=device,\n",
    "        csv_file=\"./examples/interesting_timelapse_crops.csv\",\n",
    "        save_crops=True,\n",
    "        box_threshold=0.95,\n",
    "        crop_dir=\"./examples/images/crops/interesting_timelapse\",\n",
    "        job_name=None,\n",
    "    )\n",
    "\n",
    "    boxes = []\n",
    "    all_crops_loc.append(crops)\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    embeddings_list[img_path] = {}\n",
    "\n",
    "    for j, row in crops.iterrows():\n",
    "        boxes.append({\n",
    "            'x_min': row['x_min'],\n",
    "            'y_min': row['y_min'],\n",
    "            'x_max': row['x_max'],\n",
    "            'y_max': row['y_max'],\n",
    "            'label': row['crop_status'],\n",
    "            'ann_col': 'grey'\n",
    "        })\n",
    "\n",
    "\n",
    "        # Crop original image and extract embedding\n",
    "        crop = image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "        embedding = extract_embedding(crop)\n",
    "        embeddings_list[img_path][row['crop_status']] = {\n",
    "            'embedding': embedding,\n",
    "            'file': os.path.basename(row['image_path']),\n",
    "            'crop': row['crop_status'],\n",
    "            'crop_image': crop,\n",
    "            'box': {'xmin':row['x_min'], 'ymin':row['y_min'], 'xmax':row['x_max'], 'ymax':row['y_max']}\n",
    "        }\n",
    "\n",
    "    del crops\n",
    "    img = image_annotation(img_path, boxes=boxes)\n",
    "\n",
    "    # save the image\n",
    "    img.save(f'./examples/images/dep000035/interesting_timelapse/annotated_boxes/{os.path.basename(img_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops_loc = pd.concat(all_crops_loc)\n",
    "all_crops_loc = all_crops_loc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(8, 4, figsize=(20, 30))\n",
    "# axs = axs.ravel()\n",
    "\n",
    "# for i, img in enumerate(imgs):\n",
    "#     axs[i].imshow(img)\n",
    "#     axs[i].axis('off')\n",
    "#     axs[i].set_title(f\"Image {i+1}\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/annotated_boxes/')\n",
    "image_paths = [os.path.join('./examples/images/dep000035/interesting_timelapse/annotated_boxes/', x) for x in image_paths]\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/object_detection_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatbug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš ï¸ This section is only advised if cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = load_models(\n",
    "    device=device,\n",
    "    localisation_model_path='./models/flat_bug_M.pt',\n",
    "    binary_model_path='./models/moth-nonmoth-effv2b3_20220506_061527_30.pth',\n",
    "    order_model_path='./models/dhc_best_128.pth',\n",
    "    order_threshold_path='./models/thresholdsTestTrain.csv',\n",
    "    species_model_path='./models/turing-costarica_v03_resnet50_2024-06-04-16-17_state.pt',\n",
    "    species_labels='./models/03_costarica_data_category_map.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    os.makedirs('./examples/images/dep000035/interesting_timelapse/annotated_boxes_flatbug/', exist_ok=True)\n",
    "    os.makedirs('./examples/images/crops/interesting_timelapse_flatbug/', exist_ok=True)\n",
    "\n",
    "    all_crops = []\n",
    "\n",
    "    for i, img_path in tqdm(enumerate(image_paths)):\n",
    "        crops = crop_image_only(\n",
    "            image_path=img_path,\n",
    "            bucket_name=\"cri\",\n",
    "            localisation_model=models['localisation_model'],\n",
    "            proc_device=device,\n",
    "            csv_file=\"./examples/interesting_timelapse_flatbug.csv\",\n",
    "            save_crops=True,\n",
    "            box_threshold=0,\n",
    "            crop_dir=\"./examples/images/crops/interesting_timelapse_flatbug\",\n",
    "            job_name=None,\n",
    "        )\n",
    "        crops = crops.loc[crops['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "        all_crops = all_crops + [crops]\n",
    "        if crops.shape[0] > 0:\n",
    "            boxes = []\n",
    "            for j, row in crops.iterrows():\n",
    "                boxes.append({\n",
    "                    'x_min': row['x_min'],\n",
    "                    'y_min': row['y_min'],\n",
    "                    'x_max': row['x_max'],\n",
    "                    'y_max': row['y_max'],\n",
    "                    'label': '',\n",
    "                    'ann_col': 'grey'\n",
    "                })\n",
    "            del crops\n",
    "            img = image_annotation(img_path, boxes=boxes, scale=False)\n",
    "\n",
    "            # save the image\n",
    "            img.save(f'./examples/images/dep000035/interesting_timelapse/annotated_boxes_flatbug/{os.path.basename(img_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(8, 4, figsize=(20, 30))\n",
    "# axs = axs.ravel()\n",
    "\n",
    "# for i, img in enumerate(imgs):\n",
    "#     axs[i].imshow(img)\n",
    "#     axs[i].axis('off')\n",
    "#     axs[i].set_title(f\"Image {i+1}\")\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    # Open images and convert to a sequence\n",
    "    image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/annotated_boxes_flatbug/')\n",
    "    image_paths = [os.path.join('./examples/images/dep000035/interesting_timelapse/annotated_boxes_flatbug/', x) for x in image_paths]\n",
    "    images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "    # Save as GIF\n",
    "    gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/flatbug_detection_images.gif\"\n",
    "    images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "    # display(Image.open(gif_path))\n",
    "    del images\n",
    "\n",
    "    all_crops = pd.concat(all_crops)\n",
    "    all_crops = all_crops.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./examples/images/dep000035/interesting_timelapse/annotated_boxes_binary/', exist_ok=True)\n",
    "\n",
    "transform_species = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((300, 300)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/raw/')\n",
    "image_paths = [os.path.abspath(os.path.join('./examples/images/dep000035/interesting_timelapse/raw/', x)) for x in image_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image_path in tqdm(enumerate(image_paths))):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops.loc[all_crops['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            ann_col='red'\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(device)\n",
    "            binary_prediction = classify_box(cropped_tensor, models['classification_model'])\n",
    "            if binary_prediction[0] == 'moth':\n",
    "                ann_col='green'\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': binary_prediction[0],\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'./examples/images/dep000035/interesting_timelapse/annotated_boxes_binary/{os.path.basename(image_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/annotated_boxes_binary/')\n",
    "image_paths = [os.path.join('./examples/images/dep000035/interesting_timelapse/annotated_boxes_binary/', x) for x in image_paths]\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/binary_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "# display(Image.open(gif_path))\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops['image_path'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/raw/')\n",
    "image_paths = [os.path.abspath(os.path.join('./examples/images/dep000035/interesting_timelapse/raw/', x)) for x in image_paths]\n",
    "image_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./examples/images/dep000035/interesting_timelapse/annotated_boxes_order/', exist_ok=True)\n",
    "\n",
    "imgs = []\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops.loc[all_crops['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            ann_col = 'red'\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(torch.device('cuda:0'))\n",
    "            order_prediction = classify_order(\n",
    "                cropped_tensor,\n",
    "                models['order_model'],\n",
    "                models['order_model_labels'],\n",
    "                models['order_model_thresholds']\n",
    "            )\n",
    "            if 'Lepidoptera' in order_prediction[0]:\n",
    "                ann_col = 'green'\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': order_prediction[0],\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'./examples/images/dep000035/interesting_timelapse/annotated_boxes_order/{os.path.basename(image_path)}')\n",
    "\n",
    "        imgs = imgs + [im]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/annotated_boxes_order/')\n",
    "image_paths = [os.path.join('./examples/images/dep000035/interesting_timelapse/annotated_boxes_order/', x) for x in image_paths]\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/order_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "# display(Image.open(gif_path))\n",
    "del images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Species Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/raw/')\n",
    "image_paths = [os.path.abspath(os.path.join('./examples/images/dep000035/interesting_timelapse/raw/', x)) for x in image_paths]\n",
    "\n",
    "# all_crops['image_path'] = all_crops['image_path'].replace('annotated_boxes', 'raw')\n",
    "\n",
    "print(image_paths[0])\n",
    "print(all_crops['image_path'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./examples/images/dep000035/interesting_timelapse/annotated_boxes_species/', exist_ok=True)\n",
    "\n",
    "imgs = []\n",
    "for i, image_path in enumerate(image_paths):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops.loc[all_crops['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(torch.device('cuda:0'))\n",
    "            order_prediction = classify_order(\n",
    "                cropped_tensor,\n",
    "                models['order_model'],\n",
    "                models['order_model_labels'],\n",
    "                models['order_model_thresholds']\n",
    "            )\n",
    "            label = \"\"\n",
    "            ann_col = 'red'\n",
    "\n",
    "            if 'Lepidoptera' in order_prediction[0]:\n",
    "                species_names, species_confidences = classify_species(\n",
    "                    cropped_tensor,\n",
    "                    models['species_model'],\n",
    "                    models['species_model_labels'],\n",
    "                    5\n",
    "                )\n",
    "                label = f\"{species_names[0]}, {'%.2f'.format(species_confidences[0]*100)}%\"\n",
    "                ann_col='green'\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': label,\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'./examples/images/dep000035/interesting_timelapse/annotated_boxes_species/{os.path.basename(image_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/annotated_boxes_species/')\n",
    "image_paths = [os.path.join('./examples/images/dep000035/interesting_timelapse/annotated_boxes_species/', x) for x in image_paths]\n",
    "print(len(image_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/annotated_boxes_species/')\n",
    "image_paths = [os.path.join('./examples/images/dep000035/interesting_timelapse/annotated_boxes_species/', x) for x in image_paths]\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/species_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "# display(Image.open(gif_path))\n",
    "del images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Pipeline from Command Line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entire pipeline can be run from the command line. The commands are shown below for demonstrative purposes using subprocess, but for high throughput analysis we recommend using slurm. There are examples of slurm scripts in the ./slurm_scripts directory: each regional bash file (e.g. `costarica_final.sh`) calls on the sbatch file `array_processor.sh`. \n",
    "\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "chunk_id = 1\n",
    "batch_size = 20 # runs for 20 images at a time\n",
    "\n",
    "country='costarica'\n",
    "region=\"cri\"\n",
    "\n",
    "credentials_file=\"./credentials.json\"\n",
    "\n",
    "deployment_id = \"dep000035\"\n",
    "output_base_dir=f\"./data/{deployment_id}/{country}_test\"\n",
    "json_file = f\"./examples/example_keys/interesting_timelapse.json\"\n",
    "\n",
    "os.makedirs(output_base_dir, exist_ok=True)\n",
    "os.makedirs(f\"{output_base_dir}/{deployment_id}\", exist_ok=True)\n",
    "\n",
    "species_model=\"./models/turing-costarica_v03_resnet50_2024-06-04-16-17_state.pt\"\n",
    "species_labels=\"./models/03_costarica_data_category_map.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_number_padded = f\"{chunk_id:04d}\"\n",
    "csv_file = f\"{output_base_dir}/{deployment_id}_{batch_number_padded}.csv\"\n",
    "print(f\"Results will save to {csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = [\n",
    "    \"python3\", \"-m\",\n",
    "    \"amber_inferences.cli.perform_inferences\",\n",
    "    \"--chunk_id\", str(chunk_id),\n",
    "    \"--batch_size\", str(batch_size),\n",
    "    \"--json_file\", json_file,\n",
    "    \"--output_dir\", output_base_dir,\n",
    "    \"--bucket_name\", region,\n",
    "    \"--credentials_file\", credentials_file,\n",
    "    \"--csv_file\", csv_file,\n",
    "    \"--species_model_path\", species_model,\n",
    "    \"--species_labels\", species_labels,\n",
    "    \"--perform_inference\",\n",
    "    \"--remove_image\",\n",
    "    \"--box_threshold\", \"0\",\n",
    "    \"--binary_model_path\", \"./models/moth-nonmoth-effv2b3_20220506_061527_30.pth\",\n",
    "    \"--localisation_model_path\", \"./models/flat_bug_M.pt\",\n",
    "    \"--order_model_path\", \"./models/dhc_best_128.pth\",\n",
    "    \"--order_thresholds_path\", \"./models/thresholdsTestTrain.csv\",\n",
    "    \"--skip_processed\",\n",
    "    \"--verbose\"\n",
    "]\n",
    "\n",
    "result = subprocess.run(command, capture_output=True, text=True)\n",
    "\n",
    "print(result.stdout)\n",
    "\n",
    "if result.returncode != 0:\n",
    "    print(\"STDERR:\\n\", result.stderr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Flatten the embeddings dictionary\n",
    "def flatten_embeddings_dict(embeddings_dict):\n",
    "    records = []\n",
    "    for image_path in embeddings_dict.keys():\n",
    "        for crop in embeddings_dict[image_path].keys():\n",
    "            crop_data = embeddings_dict[image_path][crop]\n",
    "            record = {\n",
    "                \"image_path\": image_path,\n",
    "                \"file\": crop_data[\"file\"],\n",
    "                \"crop\": crop_data[\"crop\"],\n",
    "                \"embedding\": crop_data[\"embedding\"],\n",
    "                \"box\": crop_data[\"box\"],\n",
    "                \"cropped_image\": crop_data[\"crop_image\"]\n",
    "            }\n",
    "            records.append(record)\n",
    "    return records\n",
    "\n",
    "# Convert to DataFrame\n",
    "records = flatten_embeddings_dict(embeddings_list)\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Extract embeddings and compute pairwise cosine similarity\n",
    "embeddings = np.stack(df[\"embedding\"].values)\n",
    "\n",
    "# similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# # Create labels for the index/columns of the similarity matrix\n",
    "# labels = df[\"file\"] + \"_\" + df[\"crop\"]\n",
    "# similarity_df = pd.DataFrame(similarity_matrix, index=labels, columns=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iou(bb1, bb2) -> float:\n",
    "    \"\"\"Finds intersection over union for a bounding box pair\"\"\"\n",
    "\n",
    "    assert bb1[0] < bb1[2], f\"Issue in bounding box 1 x_annotation: {bb1[0]} < {bb1[2]}\"\n",
    "    assert bb1[1] < bb1[3], f\"Issue in bounding box 1 y_annotation: {bb1[1]} < {bb1[3]}\"\n",
    "    assert bb2[0] < bb2[2], f\"Issue in bounding box 2 x_annotation: {bb2[0]} < {bb2[2]}\"\n",
    "    assert bb2[1] < bb2[3], f\"Issue in bounding box 2 y_annotation: {bb2[1]} < {bb2[3]}\"\n",
    "\n",
    "    bb1_area = (bb1[2] - bb1[0] + 1) * (bb1[3] - bb1[1] + 1)\n",
    "    bb2_area = (bb2[2] - bb2[0] + 1) * (bb2[3] - bb2[1] + 1)\n",
    "\n",
    "    x_min = max(bb1[0], bb2[0])\n",
    "    x_max = min(bb1[2], bb2[2])\n",
    "    width = max(0, x_max - x_min + 1)\n",
    "\n",
    "    y_min = max(bb1[1], bb2[1])\n",
    "    y_max = min(bb1[3], bb2[3])\n",
    "    height = max(0, y_max - y_min + 1)\n",
    "\n",
    "    intersec_area = width * height\n",
    "    union_area = bb1_area + bb2_area - intersec_area\n",
    "\n",
    "    iou = np.around(intersec_area / union_area, 2)\n",
    "    assert 0 <= iou <= 1, \"IoU out of bounds\"\n",
    "\n",
    "    return iou\n",
    "\n",
    "\n",
    "def box_ratio(bb1, bb2) -> float:\n",
    "    \"\"\"Finds the ratio of the two bounding boxes\"\"\"\n",
    "\n",
    "    bb1_area = (bb1[2] - bb1[0] + 1) * (bb1[3] - bb1[1] + 1)\n",
    "    bb2_area = (bb2[2] - bb2[0] + 1) * (bb2[3] - bb2[1] + 1)\n",
    "\n",
    "    min_area = min(bb1_area, bb2_area)\n",
    "    max_area = max(bb1_area, bb2_area)\n",
    "\n",
    "    box_ratio = min_area / max_area\n",
    "    assert 0 <= box_ratio <= 1, \"box ratio out of bounds\"\n",
    "\n",
    "    return box_ratio\n",
    "\n",
    "\n",
    "def distance_ratio(bb1, bb2, img_diag: float) -> float:\n",
    "    \"\"\"finds the distance between the two bounding boxes and normalizes\n",
    "    by the image diagonal length\n",
    "    \"\"\"\n",
    "\n",
    "    centre_x_bb1 = bb1[0] + (bb1[2] - bb1[0]) / 2\n",
    "    centre_y_bb1 = bb1[1] + (bb1[3] - bb1[1]) / 2\n",
    "\n",
    "    centre_x_bb2 = bb2[0] + (bb2[2] - bb2[0]) / 2\n",
    "    centre_y_bb2 = bb2[1] + (bb2[3] - bb2[1]) / 2\n",
    "\n",
    "    dist = math.sqrt(\n",
    "        (centre_x_bb2 - centre_x_bb1) ** 2 + (centre_y_bb2 - centre_y_bb1) ** 2\n",
    "    )\n",
    "    max_dist = img_diag\n",
    "\n",
    "    assert dist <= max_dist, \"distance between bounding boxes more than max distance\"\n",
    "\n",
    "    return dist / max_dist\n",
    "\n",
    "\n",
    "def cosine_similarity(img1_ftrs, img2_ftrs) -> float:\n",
    "    \"\"\"\n",
    "    Finds cosine similarity between a pair of cropped images.\n",
    "\n",
    "    Uses the feature embeddings array computed from a CNN model.\n",
    "    \"\"\"\n",
    "\n",
    "    cosine_sim = np.dot(img1_ftrs, img2_ftrs) / (\n",
    "        np.linalg.norm(img1_ftrs) * np.linalg.norm(img2_ftrs)\n",
    "    )\n",
    "    assert 0 <= cosine_sim <= 1.000000001, \"Cosine similarity score out of bounds\"\n",
    "\n",
    "    return cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import math\n",
    "\n",
    "results = []\n",
    "w_cnn =1\n",
    "w_iou =1\n",
    "w_box =1\n",
    "w_dis =1\n",
    "for crop1, crop2 in combinations(records, 2):\n",
    "    features1 = crop1[\"embedding\"]\n",
    "    features2 = crop2[\"embedding\"]\n",
    "    bb1 = crop1[\"box\"]\n",
    "    bb2 = crop2[\"box\"]\n",
    "\n",
    "    # reorder bb1\n",
    "    bb1 = [bb1['xmin'], bb1['ymin'], bb1['xmax'], bb1['ymax']]\n",
    "    bb2 = [bb2['xmin'], bb2['ymin'], bb2['xmax'], bb2['ymax']]\n",
    "\n",
    "    image = Image.open(crop1['image_path']).convert(\"RGB\")\n",
    "    image_width, image_height = image.size\n",
    "\n",
    "    diag = math.sqrt(image_width**2 + image_height**2)\n",
    "\n",
    "    cnn_cost = 1 - cosine_similarity(features1, features2)\n",
    "    iou_cost = 1 - iou(bb1, bb2)\n",
    "    box_ratio_cost = 1 - box_ratio(bb1, bb2)\n",
    "    dist_ratio_cost = distance_ratio(bb1, bb2, diag)\n",
    "\n",
    "    total_cost = (\n",
    "        w_cnn * cnn_cost\n",
    "        + w_iou * iou_cost\n",
    "        + w_box * box_ratio_cost\n",
    "        + w_dis * dist_ratio_cost\n",
    "    )\n",
    "\n",
    "    results.append([\n",
    "        crop1['image_path'],\n",
    "        crop1['crop'],\n",
    "        crop2['crop'],\n",
    "        cnn_cost,\n",
    "        iou_cost,\n",
    "        box_ratio_cost,\n",
    "        dist_ratio_cost,\n",
    "        total_cost\n",
    "    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"image_path\",\n",
    "    \"crop1_id\",\n",
    "    \"crop2_id\",\n",
    "    \"cnn_cost\",\n",
    "    \"iou_cost\",\n",
    "    \"box_ratio_cost\",\n",
    "    \"dist_ratio_cost\",\n",
    "    \"total_cost\"\n",
    "]\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Find pairs with similarity > 0.8 (excluding diagonal)\n",
    "threshold = 0.9\n",
    "similar_pairs = []\n",
    "for i in range(len(df)):\n",
    "    for j in range(i + 1, len(df)):\n",
    "        if similarity_matrix[i, j] > threshold:\n",
    "            similar_pairs.append((i, j, similarity_matrix[i, j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j, sim in similar_pairs:\n",
    "    crop1, crop2 = df.iloc[i], df.iloc[j]\n",
    "\n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    axes[0].imshow(crop1['cropped_image'])\n",
    "    axes[0].set_title(f\"{crop1['file']}\\n{crop1['crop']}\")\n",
    "    axes[0].axis(\"off\")\n",
    "\n",
    "    axes[1].imshow(crop2['cropped_image'])\n",
    "    axes[1].set_title(f\"{crop2['file']}\\n{crop2['crop']}\")\n",
    "    axes[1].axis(\"off\")\n",
    "\n",
    "    plt.suptitle(f\"Similarity: {sim:.3f}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
