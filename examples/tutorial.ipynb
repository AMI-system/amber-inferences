{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial\n",
    "\n",
    "\n",
    "This tutorial runs you through the process of running inferences for a deployments in Costa Rica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the wd\n",
    "os.chdir(os.path.expanduser('~/amber-inferences'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package if required\n",
    "%pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amber_inferences.utils.config import load_credentials\n",
    "from amber_inferences.utils.api_utils import get_deployments\n",
    "from amber_inferences.utils.deployment_summary import deployment_data\n",
    "from amber_inferences.utils.custom_models import *\n",
    "from amber_inferences.utils.inference_scripts import *\n",
    "from amber_inferences.utils.plotting import *\n",
    "from amber_inferences.utils.tracking import *\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "from IPython.display import Markdown as md\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore the Data on the Object Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance for the object store\n",
    "aws_credentials = load_credentials('./credentials.json')\n",
    "session = boto3.Session(\n",
    "        aws_access_key_id=aws_credentials[\"AWS_ACCESS_KEY_ID\"],\n",
    "        aws_secret_access_key=aws_credentials[\"AWS_SECRET_ACCESS_KEY\"],\n",
    "        region_name=aws_credentials[\"AWS_REGION\"],\n",
    "    )\n",
    "s3_client = session.client(\"s3\", endpoint_url=aws_credentials[\"AWS_URL_ENDPOINT\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the deployments available on the object store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_deployments = get_deployments(aws_credentials['UKCEH_username'], aws_credentials['UKCEH_password'])\n",
    "all_deployments = pd.DataFrame(all_deployments)\n",
    "all_deployments[all_deployments['status'] == 'active']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_deployment_names(username, password, bucket):\n",
    "    response = get_deployments(username, password)\n",
    "    response = [x for x in response if x[\"country_code\"].lower() == bucket]\n",
    "\n",
    "    # create a list of deployment names\n",
    "    deployment_names = [x['deployment_id'] for x in response]\n",
    "    return deployment_names\n",
    "\n",
    "get_deployment_names(aws_credentials['UKCEH_username'], aws_credentials['UKCEH_password'], 'cri')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"All countries:\")\n",
    "for x in all_deployments['country'].unique():\n",
    "    print(f\"- {x}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick one, cri (Costa Rica) and check out the data attached. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_deployments = deployment_data(\n",
    "    aws_credentials,\n",
    "    subset_countries=[\"cri\"],\n",
    "    subset_deployments=[\"dep000035\", \"dep000036\"],\n",
    "    include_file_count=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the files for a given deployment(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cr_deployments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Log the image keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amber_inferences.utils.key_utils import save_keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes some time to commenting out to save time\n",
    "save_keys(\n",
    "    s3_client,\n",
    "    bucket=\"cri\",\n",
    "    deployment_id=\"dep000035\",\n",
    "    output_file=\"./examples/example_keys/dep000035_keys.json\",\n",
    "    subdir=\"snapshot_images\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a subset which just looks at 10 minutes from one night:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./examples/example_keys/dep000035_keys.json\", \"r\") as f:\n",
    "    dep000035_keys = json.load(f)\n",
    "\n",
    "dep000035_keys = {list(dep000035_keys.keys())[0]: dep000035_keys[list(dep000035_keys.keys())[0]] }\n",
    "dep000035_keys[list(dep000035_keys.keys())[0]] = [x for x in dep000035_keys[list(dep000035_keys.keys())[0]] if \"20240430004\" in x][0:10]\n",
    "\n",
    "# save to file\n",
    "with open(\"./examples/example_keys/interesting_timelapse.json\", \"w\") as f:\n",
    "    json.dump(dep000035_keys, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the keys\n",
    "!head ./examples/example_keys/interesting_timelapse.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Download and View the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from amber_inferences.utils.inference_scripts import download_image_from_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the first image in the keys file and open\n",
    "with open('./examples/example_keys/interesting_timelapse.json') as f:\n",
    "    keys = json.load(f)\n",
    "\n",
    "keys = keys[list(keys.keys())[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('./examples/images/dep000035/interesting_timelapse/raw/', exist_ok=True)\n",
    "\n",
    "for i in range(len(keys)):\n",
    "    download_image_from_key(s3_client, keys[i], 'cri', './examples/images/dep000035/interesting_timelapse/raw/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/raw/')\n",
    "image_paths = [os.path.join('./examples/images/dep000035/interesting_timelapse/raw/', x) for x in image_paths]\n",
    "\n",
    "image_paths = [x for x in image_paths if x.endswith('.jpg')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open images and convert to a sequence\n",
    "images = [Image.open(img) for img in image_paths]\n",
    "\n",
    "# Save as GIF\n",
    "os.makedirs('./examples/images/dep000035/interesting_timelapse/gifs', exist_ok=True)\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/raw_images.gif\"\n",
    "images[0].save(gif_path, save_all=True, append_images=images[1:], duration=500, loop=0)\n",
    "\n",
    "del images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perform Object Detection on the Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the torch device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    torch.tensor([1.0], device=device)\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_data=deployment_data(\n",
    "    aws_credentials,\n",
    "    subset_countries=[\"cri\"],\n",
    "    subset_deployments=[\"dep000035\"],\n",
    "    include_file_count=False\n",
    ")[\"dep000035\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_load = load_models(\n",
    "    device=device,\n",
    "    localisation_model_path='./models/v1_localizmodel_2021-08-17-12-06.pt',\n",
    "    binary_model_path='./models/moth-nonmoth-effv2b3_20220506_061527_30.pth',\n",
    "    order_model_path='./models/dhc_best_128.pth',\n",
    "    order_threshold_path='./models/thresholdsTestTrain.csv',\n",
    "    species_model_path='./models/turing-costarica_v03_resnet50_2024-06-04-16-17_state.pt',\n",
    "    species_labels='./models/03_costarica_data_category_map.json'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove old runs\n",
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_id = \"dep000035\"\n",
    "dep_data = deployment_data(\n",
    "    aws_credentials,\n",
    "    subset_countries=[\"cri\"],\n",
    "    subset_deployments=[deployment_id],\n",
    "    include_file_count=False\n",
    ")[deployment_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops_loc = []\n",
    "\n",
    "for i, img_path in enumerate(tqdm(image_paths)):\n",
    "    crops = crop_image_only(\n",
    "        image_path=img_path,\n",
    "        dep_data=dep_data,\n",
    "        localisation_model=models_load['localisation_model'],\n",
    "        proc_device=device,\n",
    "        csv_file=\"./examples/interesting_timelapse_crops.csv\",\n",
    "        save_crops=True,\n",
    "        box_threshold=0.95,\n",
    "        crop_dir=\"./examples/images/crops/interesting_timelapse\",\n",
    "        job_name=None,\n",
    "    )\n",
    "\n",
    "    boxes = []\n",
    "    all_crops_loc.append(crops)\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "    for j, row in crops.iterrows():\n",
    "        boxes.append({\n",
    "            'x_min': row['x_min'],\n",
    "            'y_min': row['y_min'],\n",
    "            'x_max': row['x_max'],\n",
    "            'y_max': row['y_max'],\n",
    "            'label': row['crop_status'],\n",
    "            'ann_col': 'grey'\n",
    "        })\n",
    "\n",
    "        # Crop original image and extract embedding\n",
    "        crop = image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "\n",
    "    del crops\n",
    "    img = image_annotation(img_path, boxes=boxes)\n",
    "\n",
    "    # save the image\n",
    "    img.save(f'{output_dir}/{os.path.basename(img_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops_loc = pd.concat(all_crops_loc)\n",
    "all_crops_loc = all_crops_loc.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save to gif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/object_detection_images.gif\"\n",
    "\n",
    "gif_creater(output_dir, gif_path)\n",
    "\n",
    "# show the gif\n",
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatbug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "⚠️ This section is only advised if cuda is available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    models_load = load_models(\n",
    "        device=device,\n",
    "        localisation_model_path='./models/flat_bug_M.pt',\n",
    "        binary_model_path='./models/moth-nonmoth-effv2b3_20220506_061527_30.pth',\n",
    "        order_model_path='./models/dhc_best_128.pth',\n",
    "        order_threshold_path='./models/thresholdsTestTrain.csv',\n",
    "        species_model_path='./models/turing-costarica_v03_resnet50_2024-06-04-16-17_state.pt',\n",
    "        species_labels='./models/03_costarica_data_category_map.json'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_flatbug/'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    files = os.listdir(output_dir)\n",
    "    if len(files) > 0:\n",
    "        for f in files:\n",
    "            os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_data=deployment_data(\n",
    "    aws_credentials,\n",
    "    subset_countries=[\"cri\"],\n",
    "    subset_deployments=[\"dep000035\"],\n",
    "    include_file_count=False\n",
    ")[\"dep000035\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path  =image_paths[0]\n",
    "\n",
    "image_path=img_path\n",
    "dep_data=dep_data\n",
    "localisation_model=models_load['localisation_model']\n",
    "proc_device=device\n",
    "csv_file=\"./examples/interesting_timelapse_flatbug.csv\"\n",
    "save_crops=True\n",
    "box_threshold=0\n",
    "crop_dir=\"./examples/images/crops/interesting_timelapse_flatbug\"\n",
    "job_name=None\n",
    "\n",
    "image_path = Path(image_path)\n",
    "\n",
    "\n",
    "\n",
    "image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "original_image = image.copy()\n",
    "original_width, original_height = image.size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatbug(image_path, flatbug_model, save_annotation=False):\n",
    "    output = flatbug_model(str(image_path))\n",
    "\n",
    "    # Save a visualisation of the predictions\n",
    "    if len(output.json_data[\"boxes\"]) > 0 and save_annotation:\n",
    "        print(f\"Saving annotated image: {image_path}\")\n",
    "        output.plot(\n",
    "            outpath=f\"{os.path.dirname(image_path)}/flatbug/flatbug_{os.path.basename(image_path)}\"\n",
    "        )\n",
    "\n",
    "    # rename the confs item as scores\n",
    "    crop_info = output.json_data\n",
    "    crop_info[\"scores\"] = crop_info.pop(\"confs\")\n",
    "    crop_info[\"labels\"] = crop_info.pop(\"classes\")\n",
    "\n",
    "    return crop_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "localisation_outputs = flatbug(image_path, localisation_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatbug_model = localisation_model\n",
    "localisation_outputs, box_coords = get_boxes(\n",
    "        localisation_model,\n",
    "        image,\n",
    "        image_path,\n",
    "        original_width,\n",
    "        original_height,\n",
    "        torch.device(\"cuda:0\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    os.makedirs('./examples/images/crops/interesting_timelapse_flatbug/', exist_ok=True)\n",
    "\n",
    "    all_crops_flatbug = []\n",
    "\n",
    "    for i, img_path in enumerate(tqdm(image_paths)):\n",
    "        crops = crop_image_only(\n",
    "            image_path=img_path,\n",
    "            dep_data=dep_data,\n",
    "            localisation_model=models_load['localisation_model'],\n",
    "            proc_device=device,\n",
    "            csv_file=\"./examples/interesting_timelapse_flatbug.csv\",\n",
    "            save_crops=True,\n",
    "            box_threshold=0,\n",
    "            crop_dir=\"./examples/images/crops/interesting_timelapse_flatbug\",\n",
    "            job_name=None,\n",
    "        )\n",
    "        crops = crops.loc[crops['crop_status'] != 'No detections for image.',]\n",
    "\n",
    "        all_crops_flatbug = all_crops_flatbug + [crops]\n",
    "        if crops.shape[0] > 0:\n",
    "            boxes = []\n",
    "            for j, row in crops.iterrows():\n",
    "                boxes.append({\n",
    "                    'x_min': row['x_min'],\n",
    "                    'y_min': row['y_min'],\n",
    "                    'x_max': row['x_max'],\n",
    "                    'y_max': row['y_max'],\n",
    "                    'label': '',\n",
    "                    'ann_col': 'grey'\n",
    "                })\n",
    "            del crops\n",
    "            img = image_annotation(img_path, boxes=boxes, scale=False)\n",
    "\n",
    "            # save the image\n",
    "            img.save(f'{output_dir}/{os.path.basename(img_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops_flatbug = pd.concat(all_crops_flatbug)\n",
    "all_crops_flatbug = all_crops_flatbug.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    import os\n",
    "    gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/flatbug_detection_images.gif\"\n",
    "\n",
    "    gif_creater(output_dir, gif_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Objects for Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose which crops to use (localisation or flatbug)\n",
    "all_crops = all_crops_flatbug # or all_crops_loc\n",
    "\n",
    "transform_species = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((300, 300)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "all_crops = all_crops.reset_index(drop=True)\n",
    "\n",
    "all_crops['image_path'] = all_crops['image_path'].apply(lambda x: os.path.abspath(x))\n",
    "all_crops['base_image_path'] = all_crops['image_path'].apply(lambda x: os.path.basename(x))\n",
    "\n",
    "image_paths_raw = image_paths = os.listdir('./examples/images/dep000035/interesting_timelapse/raw/')\n",
    "image_paths_raw = [os.path.abspath(os.path.join('./examples/images/dep000035/interesting_timelapse/raw/', x)) for x in image_paths_raw]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_binary'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_crops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image_path in enumerate(tqdm(image_paths_raw)):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops.loc[all_crops['image_path'].str.contains(image_path, na=False), ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'No detections for image.',]\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            ann_col='red'\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(device)\n",
    "            binary_prediction = classify_box(cropped_tensor, models_load['classification_model'])\n",
    "            if binary_prediction[0] == 'moth':\n",
    "                ann_col='green'\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': binary_prediction[0],\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'{output_dir}/{os.path.basename(image_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/binary_images.gif\"\n",
    "\n",
    "gif_creater(output_dir, gif_path)\n",
    "\n",
    "# show the gif\n",
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Order Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_order/'\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))\n",
    "\n",
    "imgs = []\n",
    "for i, image_path in enumerate(tqdm(image_paths_raw)):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops.loc[all_crops['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'NO DETECTIONS FOR IMAGE',]\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            ann_col = 'red'\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(device)\n",
    "            order_prediction = classify_order(\n",
    "                cropped_tensor,\n",
    "                models_load['order_model'],\n",
    "                models_load['order_model_labels'],\n",
    "                models_load['order_model_thresholds']\n",
    "            )\n",
    "            if 'Lepidoptera' in order_prediction[0]:\n",
    "                ann_col = 'green'\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': order_prediction[0],\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'{output_dir}/{os.path.basename(image_path)}')\n",
    "\n",
    "        imgs = imgs + [im]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/order_images.gif\"\n",
    "gif_creater(output_dir, gif_path)\n",
    "\n",
    "# show the gif\n",
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Species Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_species/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "all_embeddings = {}\n",
    "for i, image_path in enumerate(tqdm(image_paths_raw)):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops.loc[all_crops['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'No detections for image',]\n",
    "\n",
    "    all_embeddings[image_path] = {}\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            cropped_image = original_image.crop((row['x_min'], row['y_min'], row['x_max'], row['y_max']))\n",
    "            cropped_tensor = transform_species(cropped_image).unsqueeze(0).to(device)\n",
    "            order_prediction = classify_order(\n",
    "                cropped_tensor,\n",
    "                models_load['order_model'],\n",
    "                models_load['order_model_labels'],\n",
    "                models_load['order_model_thresholds']\n",
    "            )\n",
    "            label = \"\"\n",
    "            ann_col = 'red'\n",
    "\n",
    "            if 'Lepidoptera' in order_prediction[0]:\n",
    "                species_names, species_confidences, embeddings = classify_species(\n",
    "                    cropped_tensor,\n",
    "                    models_load['species_model'],\n",
    "                    models_load['species_model_labels'],\n",
    "                    5\n",
    "                )\n",
    "                label = f\"{species_names[0]}, {'{:.2f}'.format(species_confidences[0]*100)}%\"\n",
    "                ann_col='green'\n",
    "                all_embeddings[image_path][row['crop_status']] = {\n",
    "                    'embedding': embeddings,\n",
    "                    'image_path': os.path.basename(image_path),\n",
    "                    'crop': row['crop_status'],\n",
    "                    'box': {'xmin':row['x_min'], 'ymin':row['y_min'], 'xmax':row['x_max'], 'ymax':row['y_max']},\n",
    "                    'image_size': [original_width, original_height]\n",
    "                }\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': label,\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'{output_dir}/{os.path.basename(image_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/species_images.gif\"\n",
    "gif_creater(output_dir, gif_path)\n",
    "\n",
    "# show the gif\n",
    "md(\"![mothGif](\" + os.path.abspath(gif_path) + \" 'moth')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to be able to track individual insects across frames. This is done by using the tracking model. The tracking model takes in a list of detections and returns a list of tracks. Each track is a list of detections that belong to the same insect.\n",
    "\n",
    "A track is defined by the IoU, distance between crops, similarity in features, and area. So we start by taking the embeddings from the species classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amber_inferences.utils.tracking import calculate_cost, find_best_matches\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Track insects across consecutive frames using embeddings, and get the similarity scores, to define the best matches between crops\n",
    "track_results = {}\n",
    "image_paths_sorted = sorted(all_embeddings.keys())\n",
    "\n",
    "for idx in range(1, len(image_paths_sorted)):\n",
    "    prev_img = image_paths_sorted[idx - 1]\n",
    "    curr_img = image_paths_sorted[idx]\n",
    "    prev_embeds = all_embeddings[prev_img]\n",
    "    curr_embeds = all_embeddings[curr_img]\n",
    "    for crop_status, curr_data in curr_embeds.items():\n",
    "        # Compare each crop in current image to all crops in previous image\n",
    "        similarities = []\n",
    "        for prev_status, prev_data in prev_embeds.items():\n",
    "            # Calculate cost (distance, iou, etc.) between embeddings\n",
    "            cost_df = calculate_cost(prev_data, curr_data)\n",
    "            similarities.append(cost_df)\n",
    "        if similarities:\n",
    "            # Concatenate all cost DataFrames and find best match\n",
    "            all_costs = pd.concat(similarities, ignore_index=True)\n",
    "            best_match = find_best_matches(all_costs)\n",
    "            track_results[(curr_img, crop_status)] = best_match\n",
    "        else:\n",
    "            # No previous crops to match\n",
    "            track_results[(curr_img, crop_status)] = pd.DataFrame({\n",
    "                'previous_image': [None],\n",
    "                'best_match_crop': ['No crops from previous image. Tracking not possible.'],\n",
    "                'cnn_cost': [''],\n",
    "                'iou_cost': [''],\n",
    "                'box_ratio_cost': [''],\n",
    "                'dist_ratio_cost': [''],\n",
    "                'total_cost': ['']\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_results_df = pd.concat(track_results).reset_index()\n",
    "track_results_df.rename(columns={'level_0': 'current_image', 'level_1': 'current_crop'}, inplace=True)\n",
    "track_results_df.drop(columns=['level_2'], inplace=True)\n",
    "track_results_df['crop_no'] = track_results_df['current_crop'].apply(lambda x: int(x.split('_')[-1]))\n",
    "\n",
    "track_results_df = track_results_df.sort_values(by=['current_image', 'crop_no']).reset_index(drop=True)\n",
    "\n",
    "track_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the tracking results with the crops dataframe\n",
    "all_crops_tracking = all_crops.merge(\n",
    "    track_results_df,\n",
    "    left_on=['base_image_path', 'crop_status'],\n",
    "    right_on=['image_path', 'current_crop'],\n",
    "    suffixes=('', '_tracking'),\n",
    "    how='left'\n",
    ")\n",
    "all_crops_tracking = all_crops_tracking.loc[:, ~all_crops_tracking.columns.str.endswith('_tracking')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "track_df = track_id_calc(all_crops_tracking, cost_threshold=1)\n",
    "print(\n",
    "    f\"Number of tracks: {track_df['track_id'].nunique()}\"\n",
    ")\n",
    "\n",
    "all_crops_tracked = all_crops_tracking.merge(\n",
    "    track_df,\n",
    "    how=\"left\",\n",
    "    left_on=[\"base_image_path\", \"crop_status\"],\n",
    "    right_on=[\"image_path\", \"crop_id\"],\n",
    "    suffixes=('', '_y')\n",
    ")\n",
    "\n",
    "all_crops_tracked = all_crops_tracked.reset_index(drop=True)\n",
    "all_crops_tracked = all_crops_tracked.loc[:, ~all_crops_tracked.columns.str.contains(\"_y\")]\n",
    "\n",
    "all_crops_tracked.loc[\n",
    "    all_crops_tracked['image_path'] == all_crops_tracked['image_path'].values[0],\n",
    "    ['previous_image', 'best_match_crop']\n",
    "] = 'first frame'\n",
    "\n",
    "\n",
    "all_crops_tracked.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where track_id is NaN the crop is not a moth, or the image does not have any crops. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check there are no duplicates in the crops\n",
    "duplicates = all_crops_tracked[all_crops_tracked.duplicated(subset=['base_image_path', 'crop_status'], keep=False)]\n",
    "duplicates = duplicates.sort_values(by=['base_image_path', 'crop_status']).reset_index(drop=True)\n",
    "\n",
    "if not duplicates.empty:\n",
    "    print(f\"Found {len(duplicates)} duplicates in the crops:\")\n",
    "    display(duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = './examples/images/dep000035/interesting_timelapse/annotated_boxes_tracking/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "files = os.listdir(output_dir)\n",
    "if len(files) > 0:\n",
    "    for f in files:\n",
    "        os.remove(os.path.join(output_dir, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "for i, image_path in enumerate(tqdm(image_paths_raw)):\n",
    "    imge = Image.open(image_path).convert(\"RGB\")\n",
    "    original_image = imge.copy()\n",
    "    original_width, original_height = imge.size\n",
    "\n",
    "    crops_df = all_crops_tracked.loc[all_crops_tracked['image_path'] == image_path, ]\n",
    "    crops_df = crops_df.loc[crops_df['crop_status'] != 'No detections for image.',]\n",
    "\n",
    "    if crops_df.shape[0] > 0:\n",
    "        boxes = []\n",
    "        for j, row in crops_df.iterrows():\n",
    "            label = ''\n",
    "            ann_col = 'grey'\n",
    "\n",
    "            # if not '' and not nan\n",
    "            if row['track_id'] != '' and pd.notna(row['track_id']):\n",
    "                label = row['track_id']\n",
    "                ann_col = 'green'\n",
    "\n",
    "            boxes.append({\n",
    "                'x_min': row['x_min'],\n",
    "                'y_min': row['y_min'],\n",
    "                'x_max': row['x_max'],\n",
    "                'y_max': row['y_max'],\n",
    "                'label': label,\n",
    "                'ann_col': ann_col\n",
    "            })\n",
    "        im = image_annotation(image_path, boxes=boxes, scale=False)\n",
    "        im.save(f'{output_dir}/{os.path.basename(image_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as GIF\n",
    "gif_path = \"./examples/images/dep000035/interesting_timelapse/gifs/tracking_images.gif\"\n",
    "gif_creater(output_dir, gif_path)\n",
    "\n",
    "# show the gif\n",
    "md(\"![trackingGif](\" + os.path.abspath(gif_path) + \" 'tracking')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dep_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
